
Kinectのトラッキング原理「部位認識に基づく3D姿勢推定」
Tweet
はじめに
今から1週間前から、私のTwitterのフォロワー周辺で、Kinectの原理とその発展的使用方法についての議論が始まりました。
参考:Kinectの仕組みにまつわるつぶやき一覧
ところが、Kinectはその原理が詳細に書かれているところがWebには少なく、少し検索したくらいではなかなかわかりよいまとめ記事がなく、私も含めてみなさん憶測で議論せざるを得ないところがありました。
そこで、この記事では開発者の資料・インタビューなどをもとに、Kinectの原理についてまとめようと思います。これをベースとして、皆様がKinectが用いている技術について発展的な考察ができるようになれば幸いです。
Kinectとは
原理を説明する前にKinectがどういうものだったかをまずおさらいしておきましょう。
MicrosoftがXBOX360用に発売したコントローラがKinectです。Kinectは非接触型の専用のカメラを設置することでプレイヤーの動き・姿勢をリアルタイムに認識し、その動き・姿勢をゲーム中のキャラクターの動きとリンクすることができるという、世界初の非接触型ゲームコントローラーです。
私は80年代生まれなので、かつてファミリートレーナー(今の若いコはわかりますかね 笑)で風雲たけし城をやってた世代なので、まさかこんなとんでもない技術が製品化されるとは夢にも思っていませんでした。
Kinectの原理(概要)
Kinectの原理をWebで調べていると、Wall Street JournalのTECH EUROPEというブログで、以下のようなKinectの記事を見つけました。
Key Kinect Technology Devised in Cambridge Lab (Kinectのキーとなる技術はMicrosoft Cabridge研究所で発明された)
この記事によると、Microsoft Research CambridgeのJamie Shottonという研究者たちのチームがKinectの最も重要な技術である「プレーヤー姿勢推定」の部分を開発したとのことです。
Kinectの基本的なハードウェア(距離画像センサ)はPrimeSenseと   いうジェスチャー認識会社のOEM製品です。このPrimeSenseのカメラを使って取得した距離画像をもとに、このカメラの中のチップに組み込まれた  Jamie   Shottonらが作ったプレイヤーの姿勢推定のソフトウェアを用いることで、Kinectはプレイヤーの姿勢を推定(というかほぼ正確な認識)している  というわけです
Kinectのもたらした2つのブレイクスルー
Kinectにはハードウェア側(距離画像センサ側)、ソフトウェア側(姿勢推定側)に、それぞれ以下のようなブレイクスルーがあります。
距離画像センサのブレイクスルー:従来の距離画像センサはTOF形式の非常に効果なものでないとリア ルタイム の距離画像取得は難しかしいものです。PrimeSense社は”Light   Coding”という赤外線パターン照射を普通のCMOSカメラで解析することで実現可能な独自の方式で距離画像センサを実装しています。これにより  Kinectのようなコンシューマ向けの比較的安価な距離画像センサを提供できるようになりました。
姿勢推定側のブレイクスルー:前述のWSJの記事でもあるように、動いている人間の体をマーカーレス(※1)でトラッキングすることは以前は不可能であった。これが後述の手法で(私は力業と感じるが)ついに可能となり、ユーザーが何も身につけなくても体の動きをとらえられるようになった。
Kinectのコア技術「人体部位の機械学習(Machine Learning)」
Kinectのプレイヤー姿勢推定の技術のコアは「超大規模な機械学習(Machine Learning)」にあります。これにより、リアルタイムかつロバストなプレイヤー姿勢の認識が可能となります。
これに関して、上記WSJの記事中後半のDr.Shottonの話を以下に翻訳しておきます。
The Xbox team had built a tracking system that sort-of worked, but    nothing like as well as it needed to for a commercial product. It had    the problem that after about a minute or so it would lose track, and    once it lost track, it had no way of recovering itself.
(XBoxチームはひとまず動く程度のトラッキングシステムを構築したのだが、商用製品として必要とされているレベルでは全くなかった。そのシステ  ムには、1分くらい立てばトラッキングが外れてしまうという問題があり、一度トラッキングが外れると自力では復活できなかった。)
It also had the rather serious draw back that it could only track    people who were about the same size and shape as the systems developer.    It was not ready for commercial operation.
(またそのシステムにはさらに深刻な欠点があった。当時のそのシステムは開発者と同じようなサイズや形の人間しかトラッキングできなかったのである。これでは商用で出せるものではなかった。)
We were tasked with taking this system but improving it so that it    worked for all different people at all times and never lose track,  said   Dr. Shotton.
(「我々はこのシステムを担当することになったのですが、いつ何時でも全ての異なる(サイズや形の)ユーザーに対して動作し、絶対にトラッキングも失わないものに、改良を施しました。」とDr.Shottonは言う)
The secret was machine learning.
(その秘密は機械学習にあります。)
The idea was that we would teach the computer with lots of different    people of lots of different shapes and sizes in lots of different   poses  and the computer will learn how to distinguish one part of your   body  from another part, he said. Since the Kinect camera includes   depth  information, it can  distinguish between big people a long way   away and  small people up close.
(「コンピュータに対してそれぞれ異なった形やサイズでそれぞれ異なった姿勢を持った大量の人間のサンプル画像をコンピュータに学習させることで、   コンピュータはユーザーの体の一部分を別の一部分と区別できるようになります。」と彼は言う。Kinectのカメラは深度情報を含んで計測するので、遠く  にいる背の高いユーザーと近くにいる背の低いユーザーを区別することができる。)
つまり、これまでになかった程の大量のパターンを大量に機械学習させておくことで、どんな人のものでも識別できるような人体部位(Body Parts)ベースの識別器を用意し、これを用いてリアルタイムに、「ここは手。ここは頭。ここは足」と3D空間上で各パーツの3D位置を認識できるシステムを作ったというわけです。あとでも触れますが、相当な数のパターンを大量に学習したようで、力業的解決をしている要素があります。
Kinectの処理パイプライン
では、前処理としてあらかじめ機械学習で部位別のモデルを学習させているという点をおさえたところで、ここではKinectの処理のパイプラインを以下の4段階に分けて説明していきます。
距離画像の取得
体の各部位の推定(各フレームごと)
スケルトンの仮説作成
スケルトンのトラッキング(仮説から最終的な結果を推定)
Kinectは仕組みが全て公開されているわけではないので、あくまでWeb上で得られた情報から説明するのみで、実際の原理とは多少異なる可能性があることをご了承下さい。
ここではtwitterで@heigazenさんからいただいた、先月ケンブリッジ大で行われたJamie Shottonの講演の情報も用いて、各処理の内容をまとめています。あらためて情報ありがとうございました。
1.距離画像の取得
Kinectのカメラから、毎フレームごとの距離画像を取得します。
Kinectのカメラは以下の3つの素子が外界と光のやりとりを行います。
赤外線のStructured light照射素子
赤外線カメラ
RGB カメラ
Sturctured Lightとは、赤外線の点パターンを照射しそれを撮影することで、対象の形状変化をそのパターン変化から取得するというものです。これにより、深度(各点の奥行き)を含めた画像である「距離画像」を取得します。
KinectのStructured lightは以下の動画のように、赤外線カメラで撮影して確認することができます。
2.体の各部位の推定(各フレームごと)
1.で取得した距離画像を用いて、先に用意してある決定木により各部位のどこに相当するかの識別を行います。Kinectでは20の部 位に分けてその20の部位をトラッキングしているそうです。ここでも1.と同様1フレームごとの処理であり、時間情報はまだ一切使っていないことに注意で す。
窓ごとにその窓が体のどの部位に相当するかの計算を、予め用意した識別器(決定木とのこと。恐らくRandomised Decision Forest。)により高速で行います。
窓ごとの識別に使っている特徴量が何かというのはわかりません。ただし、先述のWSJの記事では以下のような記述がありました。
In order to simplify the amount of data, Dr. Shottons system looks  at  each pixel separately. It doesnt need to know about every possible   combination of every limb.
(データ量をシンプルなものにするため、Dr. Shottonのシステムは各ピクセルをそれぞれ見ている。各部位のあり得る全てのコンビネーションというものは知る必要がなくなっている)
窓の中での計算はどうあれ、決定木の学習はシンプルにピクセル毎にやっている模様です。いずれにせよ、詳細資料は現時点で手に入っていませんので、今説明しているこれらの処理の詳細はわかりません。
識別器が決定木なので、高速に識別することが可能なのですが、逆に言うと、衣服・体系・姿勢に大量のバリエーションがあるモーション キャプチャ画像で学習させている分、決定木の枝葉が深く、とんでもない数の学習サンプルが必要になってしまっています。およそ100万個(!)の画像を 使ってこの決定木を学習させたそうです。
(※当然これはDiscriminativeな手法なのでこのような超大量なサンプル画像を使って学習しているわけです。最近の国際学会ではGenerativeな手法も同時に用いて学習サンプル数を下げるためのHuman Pose Estimation手法が発表されはじめています。)
3.スケルトンの仮説作成
2.で得られた毎フレームごとに検出された各部位を用いて、運動力学的な拘束と、時間的な一貫性が保たれるように、3Dでの部位の配置(関節結合)を抽出します。
これはオクルージョンして見えていない部分は考慮されていないので完全な骨組み(スケルトン)ではなく、あくまで距離画像から識別できた表面に見えている部位のみ集めた仮説です。
また、複数プレイヤーの場合、この時点ではプレイヤーごとの区別はついていません。
4.スケルトンのトラッキング(仮説から最終的な結果を推定)
3.で得られた関節結合の仮説から、最後に実際の人間の骨組み(スケルトン)の動きを推定します。各仮説から確率的に一番もっともらしい3D配置を計算し(詳細は不明)、プレイヤーごとのスケルトンの推定がここで確定します。この確定したスケルトンの配置が、XBOX360のゲームに入力として入っていくわけです。
まとめ
以上、今まで距離画像カメラ自体の仕組みはなんとなく情報が手に入っていたKinectに関して、まだ日本語ではまとまった情報がなかった「距離画像取得後のプレイヤーの3Dスケルトンの取得の仕組み」について、Webで得られる情報をもとに、概要を説明しました。
3次元姿勢推定はまだまだ発展途上な技術であり、オフラインであとで時間をかけてできるものはあれど、リアルタイムにトラッキングでき るものはありませんでした。そんな中で、Microsoft  Researchが実用化レベルにまで持ってきて、実際にKinectを発売したのはすごいことです。つまり、Kinectの発売はHuman-Computer-Interactionの世界に起こった一大革命です。
この記事をきっかけに3次元姿勢推定という新しいテーマに少しでも多くの方が興味関心を持っていただけると幸いです。
関連する記事:
KinectのPrimeSense社、AsusとPC向けのKinect風ジェスチャー認識デバイスを開発中 
Microsoft、Kinect以外のデバイスでも非接触ジェスチャー認識を加速
Kinectの取得した3D骨格推定結果にアクセスできるライブラリ「OpenNI」が公開 
Kinectのトラッキング原理「部位認識に基づく3D姿勢推定」 | DERiVE コンピュータビジョン ブログ & メルマガ
