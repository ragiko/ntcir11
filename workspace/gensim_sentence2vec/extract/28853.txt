十～数百、それ以上のデータがある状態で分類器を構築する場合、私はRandom Forestをよく使います。Random Forestとは、標本データを復元ありの無作為抽出 (bootstrap)して作成した仮想データを多数生成して、それぞれのデータに対して毎回ランダムに選択した変数群を用いて決定木を構築、各々の決定木の多数決で予測を行うといった分類器です。bootstrapでデータの揺らぎを学習し (bagging)、多数の異なる決定木の多数決 (ensemble)でモデルの揺らぎを学習するイメージです。予測精度が高く、過学習 (overfit)しない点、bootstrapにより予測率が評価されるので、cross validation等が必要ない点など、扱いやすい方法です。 Rで計算できます。パッケージをロードして、データを読み込みます (データはここからDLできます)。library('randomForest')tr<-read.table("./lung_michigan.txt",header=T,row.names=1)Random Forestを実行します。パラメータは2つ。ntreeは何個の決定木を構築するか (=bootstrap数) 、mtryは決定木のサイズです 。※mtryはデフォルトでは変数の数の平方根です。sampsizeは省略できますが、群のサイズがアンバランスな場合、多い群に予測が引っ張られるため、小さい群のサイズを指定します。rf<-randomForest(class~.,data=tr,importance=T,proximity=T,ntree=1000,mtry=10,sampsize=c(24,24))このデータはA:alive=24, D:death=62で構成されています。予測精度は以下で確認できます。rf$confusion   A  D class.errorA 57  5  0.08064516D  9 15  0.37500000ntreeに対する予測精度の推移をプロットできます。plot(rf)
緑が全体の予測精度(error rate)、赤がA、黒がDの予測精度です。予測に対する各変数の寄与度を得る事ができます。impimp imp         MeanDecreaseAccuracy MeanDecreaseGiniSEC31L1          1.0843264667       0.90073387RAFTLIN          0.6662848410       0.54427994SLC2A1           0.4065791941       0.21934335・・・・寄与度のプロットも可能です。varImpPlot(rf)
寄与度の計算は決定木を構築する際、該当変数をモデルから除いた際の、予測精度の低下 (Mean Decrease Accuracy)、あるいはGini indexの減少 (Mean Decrease Gini)に基づいています。Random Forestでは、予測モデルの構築と同時にサンプル間の類似性も計算することが可能です。距離行列に変換してWard法でクラスタリングしてみます。dshcplot(hc)
まめトーーク! Random Forest
