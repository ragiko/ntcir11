特許請求の範囲
【請求項1】  所定の話者の発声音声文を書き下した学習用テキストデータに基づいて、処理対象の単語のユニグラムと、処理対象の単語とその単語から直前の距離1にある単語との間のバイグラムと、処理対象の単語とその単語から直前の距離2にある単語との間のバイグラムと、上記ユニグラム及び2つのバイグラムを正規化するための正規化パラメータとを含むモデルパラメータを有する統計的言語モデルを、処理対象の単語とその直前の2つの単語との観測値のトライグラムと、上記2つのバイグラムの推定値との誤差が最小となるように平滑化して学習することにより、上記モデルパラメータを有する統計的言語モデルを生成する生成手段を備えたことを特徴とする統計的言語モデル生成装置。【請求項2】  入力される発声音声文の音声信号に基づいて、所定の統計的言語モデルを用いて音声認識する音声認識手段を備えた音声認識装置において、上記音声認識手段は、請求項1記載の統計的言語モデル生成装置によって生成された統計的言語モデルを用いて音声認識することを特徴とする音声認識装置。
発明の詳細な説明
【0001】【発明の属する技術分野】本発明は、学習用テキストデータに基づいて統計的言語モデルを生成する統計的言語モデル生成装置、及び上記統計的言語モデルを用いて、入力される発声音声文の音声信号を音声認識する音声認識装置に関する。【0002】【従来の技術】近年、連続音声認識装置において、その性能を高めるために言語モデルを用いる方法が研究されている。これは、言語モデルを用いて、次単語を予測し探索空間を削減することにより、認識率の向上及び計算時間の削減の効果を狙ったものである。最近盛んに用いられている言語モデルとしてN−グラム(N−gram)がある。これは、大規模なテキストデータを学習し、直前のN−1個の単語から次の単語への遷移確率を統計的に与えるものである。複数L個の単語列w1L=w1,w2,…,wLの生成確率P(w1L)は次式で表される。【0003】【数1】【0004】ここで、wtは単語列w1Lのうちt番目の1つの単語を表し、wijはi番目からj番目の単語列を表わす。上記数1において、確率P(wt｜wt+1-Nt-1)は、N個の単語からなる単語列wt+1-Nt-1が発声された後に単語wtが発声される確率であり、以下同様に、確率P(A｜B)は単語又は単語列Bが発声された後に単語Aが発声される確率を意味する。また、数1における「Π」はt=1からLまでの確率P(wt｜wt+1-Nt-1)の積を意味し、以下同様である。【0005】N−グラムは極めて単純なものでありながら、構築の容易さ、統計的音響モデルとの相性の良さ、認識率向上や計算時間の短縮の効果が大きい等の理由で、連続音声認識には非常に有効である(例えば、従来技術文献1「L.R.Bahl et al.,A Maximum Likelihood Approach to Continuous Speech Recognition,IEEETransaction on Pattern Analysis and Machine Intelligence, pp.179-190,1983年」、従来技術文献2「P.C.Woodland et al.,THE 1994 HTK Large Vocabulary Speech Recognition System,Proceedings of ICASSP95,Vol.1,pp.73-76,1995年」、従来技術文献3「村上ほか,単語のtrigramを利用した文音声認識と自由発話認識への拡張,電子情報通信学会技術研究報告,SP93−127,pp71−78,平成6年」参照。)。【0006】一般に、N−グラムの言語モデルは、Nを大きくすると長い単語連鎖を取り扱うことにより次単語の精度は高くなるが、パラメータ数が多くなり、学習データ量が少ない場合は出現頻度の低い単語に信頼できる遷移確率を与えることはできない。例えば語彙数が5,000語のとき、トライグラム(trigram)(N=3)の全ての単語の遷移組は(5,000)3=1,250億であるから、信頼できる遷移確率を求めるためには、数千億単語以上からなる膨大なテキストデータが必要となる。これだけの膨大なテキストデータを集めるのは事実上不可能である。逆に、Nを小さくすると、遷移確率の信頼性は高くなるが、短い単語連鎖しか取り扱うことができず、次単語の予測精度は低くなる。【0007】この問題を解決するため、次のような方法が提案されている。(1)補間による未学習遷移確率の推定方法この方法は、例えば、削減補間法(Deleted Interpolation)(例えば、従来技術文献4「F.Jelinek et al.,Interpolated estimation of Markov SourceParameters from Sparse Data,Proceedings of Workshop Pattern Recognition in Practice,pp.381-37,1980年」参照。)や、バックオフ平滑化法(Back-offSmoothing)(例えば、従来技術文献5「S.M.Katz,Estimation of Probabilities from Sparse Data for the Language model Component of a Speech Recognizer,IEEE Transaction on Acoustics, Speech, and Signal Processing,Vol.ASSP-35,No.3,pp.400-401,1987年3月」参照。)等に代表される方法で、小さいNのN−グラム(N−gram)の値で遷移確率を補間することにより、学習用テキストデータには存在しない単語遷移に対しても、遷移確率を与えることができる。しかしながら、出現頻度の低い単語に関しては信頼できる遷移確率を与えられない恐れがある。【0008】(2)クラスN−グラムによるパラメータ数の削減方法この方法は、相互情報量に基づくクラスタリング(例えば、従来技術文献6「P.F.Brown et al.,Class-Based n-gram models of natural language,Computational Linguistics,Vol.18,No.4,pp467-479,1992年」参照。)や、品詞(従来技術文献7「周ほか,確率モデルによる日本語の大語彙連続音声認識,情報処理学会,第51回全国大会講演論文集,pp.119−120,平成7年」参照。)等によるクラス間のN−グラムを考えたもので、L個の単語の文生成確率P(w1L)は一般に次式で表される。【0009】【数2】【0010】ここで、ctは単語wtの属するクラスを表し、cijはi番目からj番目のクラス列を表わす。上記数2で、P(ct｜ct-N+1t+1)は、直前の(N−1)個の単語の属するクラスから次の単語の属するクラスへの遷移確率を表す。クラス数が50のとき、トライグラムの全てのクラス間の遷移の組は503=125,000であるから、数十万単語程度と単語N−グラムに比べてかなり小規模なテキストデータで遷移確率が求められると考えられる。しかしながら、単語間の特有な連接関係を表現することができないので、次単語の予測精度は悪くなると考えられる。【0011】これらの問題点を解決するために、従来例に比較して遷移確率の予測精度及び信頼性を改善することができる統計的言語モデルを生成することができる統計的言語モデル生成装置を、本特許出願人は、特開平9−134192号公報において提案している。この従来例の装置では、所定の話者の発声音声文を書き下した学習用テキストデータに基づいて、すべての語彙を品詞毎にクラスタリングされた品詞クラスに分類し、それらの品詞クラス間のバイグラムを初期状態の統計的言語モデルとして生成する生成手段と、上記生成手段によって生成された初期状態の統計的言語モデルに基づいて、単語の品詞クラスからの分離することができる第1の分離クラス候補と、1つの単語と1つの単語との結合、1つの単語と複数の単語の単語列との結合、複数の単語の単語列と1つの単語との結合、複数の単語の単語列と、複数の単語の単語列との結合とを含む連接単語又は連接単語列の結合によって単語の品詞クラスから分離することができる第2の分離クラス候補とを検索する検索手段と、上記検索手段によって検索された第1と第2の分離クラス候補に対して、次単語の予測の難易度を表わす所定のエントロピーを用いて、クラスを分離することによる当該エントロピーの減少量を計算する計算手段と、上記計算手段によって計算された上記第1と第2の分離クラス候補に対するエントロピーの減少量の中で最大のクラス分離を選択して、選択されたクラスの分離を実行することにより、品詞のバイグラムと可変長Nの単語のN−グラムとを含む統計的言語モデルを生成する分離手段と、上記分離手段によって生成された統計的言語モデルのクラス数が所定のクラス数になるまで、上記分離手段によって生成された統計的言語モデルを処理対象モデルとして、上記検索手段の処理と、上記計算手段の処理と、上記分離手段の処理とを繰り返すことにより、所定のクラス数を有する統計的言語モデルを生成する制御手段とを備えたことを特徴としている。【0012】【発明が解決しようとする課題】従来例の装置で生成したバイグラムの統計的言語モデルでは、それほど記憶すべきパラメータはそんなに多大にならないが、トライグラム以上の統計的言語モデルでは、記憶すべきパラメータが膨大になり、統計的言語モデルを記憶する記憶装置の容量が多大になるという問題点があった。【0013】本発明の目的は以上の問題点を解決し、トライグラム以上の統計的言語モデルであっても記憶すべきパラメータが少なくてすみ、しかも高い音声認識率を得ることができる統計的言語モデル生成装置及び、当該統計的言語モデル生成装置を備えた音声認識装置を提供することにある。【0014】【課題を解決するための手段】本発明に係る請求項1記載の統計的言語モデル装置は、所定の話者の発声音声文を書き下した学習用テキストデータに基づいて、処理対象の単語のユニグラムと、処理対象の単語とその単語から直前の距離1にある単語との間のバイグラムと、処理対象の単語とその単語から直前の距離2にある単語との間のバイグラムと、上記ユニグラム及び2つのバイグラムを正規化するための正規化パラメータとを含むモデルパラメータを有する統計的言語モデルを、処理対象の単語とその直前の2つの単語との観測値のトライグラムと、上記2つのバイグラムの推定値との誤差が最小となるように平滑化して学習することにより、上記モデルパラメータを有する統計的言語モデルを生成する生成手段を備えたことを特徴とする。【0015】また、本発明に係る請求項2記載の音声認識装置は、入力される発声音声文の音声信号に基づいて、所定の統計的言語モデルを用いて音声認識する音声認識手段を備えた音声認識装置において、上記音声認識手段は、請求項1記載の統計的言語モデル生成装置によって生成された統計的言語モデルを用いて音声認識することを特徴とする。【0016】【発明の実施の形態】以下、図面を参照して本発明に係る実施形態について説明する。【0017】図1に本発明に係る一実施形態の連続音声認識装置のブロック図を示す。本実施形態の連続音声認識装置は、言語モデル生成部20を備え、言語モデル生成部20は、学習用テキストデータメモリ21に格納された所定の話者の発声音声文を書き下した学習用テキストデータに基づいて、処理対象の単語のユニグラムと、処理対象の単語とその単語から直前の距離1にある単語との間のバイグラムと、処理対象の単語とその単語から直前の距離2にある単語との間のバイグラムと、上記ユニグラム及び2つのバイグラムを正規化するための正規化パラメータとを含むモデルパラメータを有する統計的言語モデルを、処理対象の単語とその直前の2つの単語との観測値のトライグラムと、上記2つのバイグラムの推定値との誤差が最小となるように平滑化して学習することにより、上記モデルパラメータを有する統計的言語モデルを生成することを特徴としている。【0018】本実施形態では、言語モデリングの新しい手法としてエントロピー最大手法による生起距離の異なる単語共起関係の結合(DUAME)による統計的言語モデル(以下、DUAMEモデルという。)を用いる。言語のモデル化は、音声認識及び翻訳における重要な要素である。近年、N−グラムが言語モデルの主流となっているが、これは、N−グラムが言語列を離散的確率過程と見なし、マルコフ(Markov)モデルとして効率的にモデル化することができるという理由による。N−グラムの利点は、モデル化が簡単なこと、そして実行時の計算が簡単な所にある。しかしながら、このモデルに使用できる言語制約は、連続した単語列に対してのみであるため、多様な言語制約の統合に対して柔軟性がなく、また長距離の言語制約表現することが難しいという問題点があった。この問題点を解決するために、本実施形態では、エントロピー最大手法を用いて統計的言語モデルを平滑化して学習する。【0019】ところで、本実施形態で用いるエントロピー最大手法(ME)は、多数の言語制約を組合せて用いる場合に効果的なモデリング手法である(例えば、従来技術文献8「R. Rosenfeld et al.,A  maximum entropy approach to adaptive statistical language modeling,Computer Speech and Language, pp.187-228,1996年」や従来技術文献9「S. A. Della Pietra et al.,Adaptive languagemodeling using minimum discriminant estimation,In ICASSP92, pp.I-633-635, 1992年」参照。)。エントロピー最大手法の下では、知識リソースを各々1つの特徴セットと見なし、それを確率関数のセットへと対応づける。エントロピー最大手法は、エントロピーが最大となるような確率関数のセットを求めるものである。従って、エントロピー最大手法を用いて、それぞれ異なった知識リソースが表現している特性を保ったままひとつの表現形式にまとめあげることが可能である。【0020】本実施形態では、生起距離の異なる単語共起関係という複数の特徴をエントロピー最大手法を用いて統合した統計的言語モデル(DUAMEモデル)を用いる。従来のN−グラムモデルに比べると、DUAMEモデルは、柔軟な特徴を用いることができ、かつ未出現の事象の平滑化も同じMEの枠組みの中で行えるというメリットがある。また、生起距離1から(n−1)までの共起関係により、N−グラムにおける共起関係であるn個単位の列を近似することができる。このため、以下に示すように、DUAMEモデルは大幅に少ないメモリでN−グラムの非常によい近似を行うことができる。【0021】まず、最大エントロピーの原理について説明する。【数3】S=<x1,x2,…,xz>を所定の自然言語の任意の単位列(本実施形態において、単位は単語であり、単位列は単語列である。)とした場合、イベント<Hi,xi>は、単位xiに先行して起こった単位列のうち、長さ(n−1)で窓がけした単位列である【数4】Hi=<xi-n+1,…,xi-1>と定義される。全てのイベントからなる空間を、イベント空間ε<H,x>とする。ある特定の自然言語のコーパスにおいて、コーパスに出現するイベントを観察可能なイベントと呼び、その他を未出現イベントセットとして分類することができる。【0022】観測可能なイベントセットからは、特徴セットGを抽出することができる。特徴g(hi,xi)はイベントのもつ単位列Hi全体又は単位列Hiの一部hiに着目することで決められ、属性セット【数5】<g(hi,xi),ai,mi,αi>を持つ。ここで、g(hi,xi)は特徴を示し、hiはイベント窓Hiの下の文脈の着目するサブセット、xiは列のうち着目する単位、aiはその目標期待値、miはその特徴期待値、αiは特徴の確率に関する特徴重み係数である。【0023】イベント及び特徴の上記定義に基づいて、ある指数分布で単位列を次式のように予測することができる。【0024】【数6】【0025】ここで、【数7】は指数分布に従った確率であり、【数8】はイベント<Hi,xi>において活性化された特徴重み係数の積であり、次式を用いて正規化される。【0026】【数9】【0027】各特徴g(hi,xi)には、次式で示される対応する期待値が存在する。【0028】【数10】【0029】ここで、ph(Hi)は学習データにおいて文脈Hiが観測された確率である。これにより、特徴期待値は、その目標期待値の近似(次式)となることが期待できる。【数11】m(hi,xi)≒E(p(hi,xi))=a(hi,xi)また、最尤となる指数分布モデルは、次式で示される最大エントロピーモデルと同一であることが証明されている(例えば、従来技術文献10「J. N. Darroch et al.,Generalized iterative scaling for log-linear models,In TheAnnals of Mathematical Statistics, Vol.43, pp.1470-1480, 1972年」参照。)。【0030】【数12】【0031】従って、エントロピーが最大となる指数分布を、尤度が最大のとなる指数分布m(S)で置換すれば、単位列の最大エントロピーを求めることができる。【0032】次いで、エントロピー最大手法による生起距離の異なる単語共起関係の結合言語モデルについて説明する。N−グラムは、一次のマルコフ(Markov)モデルと見なすことが可能であり、単位列の連続性に基づいている。これに対して、エントロピー最大モデルはイベントの記述に重きを置いたものであって、当該イベントにおける活性化されたさまざまな特徴を使用することができる。これは、その特徴がイベントによって表現できるものである限り、なんら制限なしに特徴を定義できることを意味している。事実、N−グラムの場合と同等の特徴も存在している。この1つとして、生起距離の異なる単語共起関係を使用するにする。この場合、使用する特徴は次のように定義することができる。【0033】定義:生起距離の異なる単語共起関係の特徴は、属性【数13】<gd(h,x),ad(h,x),md(h,x),αd(h,x)>を有する生起距離dの単位ペア(h,x)である。【0034】ここで、hは限定された窓内の1つの文脈上の単位であり、xは着目する単位(クラス、単語又は句)、dはhとx間の距離、ad(h,x)は特徴gd(h,x)の目標期待値、md(h,x)はその特徴期待値、αd(h,x)はgd(h,x)の重み係数である。【0035】従って、文脈窓長nのイベント<h1,…,hn,x>に対して、高次のN−グラムをn個の異生起距離の異なる単語共起関係の特徴を使用して次式のように近似することができる。【数14】αn(h1,x)αn-1(h2,x)…α1(hn,x)⇒αN-gram(h1,…,hn,x)距離属性を有するこれらの特徴は互いに重複部分を持たず、特徴重み係数αiのその共起を表している。従って、生起距離の異なる単語共起関係のエントロピー最大モデルの一般的表現は、以下のように記述することができる。【0036】イベント<h1,…,hn,x>のときに次式となる。【0037】【数15】【0038】特に、距離=2であるイベント<h1,h2,x>のときに次式となる。【0039】【数16】【0040】本実施形態のDUAMEモデルは、全ての生起距離の異なる単語共起関係(UA)の特徴の結合によってNグラム特徴を近似でき、また未観測のイベントの分布を自動的に平滑化することができるため、その機能はバックオフN−グラムに類似している。【0041】距離nのDUAMEモデルの場合、必要なメモリ容量はVn+n×V2+V未満であり、すなわちVnのオーダーのメモリ容量を必要とする。ここで、Vは語彙のサイズ(語彙数)、nは文脈上の窓の長さであって、Vnは合計の文脈Z(h1,…,hn)の組み合わせ数を表し、n×V2+Vは単位に関連する特徴重み係数を格納するために必要なメモリ容量である。距離2のDUAMEモデルの場合、必要なメモリ容量は3V2+Vのオーダー未満である。【0042】すなわち、同一次元のN−グラムは、VN+1のオーダーのメモリ容量を必要とする。従って、距離nのエントロピー最大モデルに必要なメモリ容量は、N−グラムモデルの場合より遥かに少ない。【0043】さらに、本実施形態に係る統計的言語モデル及びその生成処理について詳細に説明する。本実施形態の統計的言語モデルは、単語列wj,wk,wiに対して処理対象の単語wiのときに、次のモデルパラメータを有する。(a)α0(wi):wiのユニグラムパラメータ(観測値);(b)α1(wj,wi):wj,wiの距離1のバイグラムパラメータ(近似値又は推定値);(c)α2(wk,wi):wk,wiの距離2のバイグラムパラメータ(近似値又は推定値);及び(d)z(wk,wj):正規化パラメータ。ここで、正規化パラメータは、ユニグラム及び2つのバイグラムを正規化するためのパラメータである。また、モデル生成処理において次の中間パラメータを用いる。【0044】(e)g0(wi):単語wiが現われれば1、そうでなければ0である中間パラメータ;(f)g1(wj,wi):距離1でwj,wiが現われれば1、そうでなければ0である中間パラメータ;(g)g2(wk,wi):距離2でwk,wiが現われれば1、そうでなければ0である中間パラメータ;(h)T:学習データ中の総単語数;及び(i)a(wk,wj,wi):学習データ中での単語列wk,wj,wiの出現確率。【0045】エントロピー最大手法を用いた平滑化後のトライグラムの確率p(wi|wk,wj)は次式で表される。【0046】(i)g1(wj,wi)=1かつg2(wk,wi)=1のとき【数17】p(wi｜wk,wj)=α1(wj,wi)×α2(wk,wi)/z(wk,wj)(ii)g1(wj,wi)=1かつg2(wk,wi)=0のとき【数18】p(wi｜wk,wj)=α1(wj,wi)/z(wk,wj)(iii)g1(wj,wi)=0かつg2(wk,wi)=1のとき【数19】p(wi｜wk,wj)=α2(wk,wi)/z(wk,wj)(iv)g1(wj,wi)=0かつg2(wk,wi)=0のとき【数20】【0047】上記の数17乃至数20に示すように、平滑化後のトライグラムの確率p(wi|wk,wj)は、ユニグラムの観測値と、2つのバイグラムの推定値とにより、近似的に現れていることがわかる。本実施形態では、この式を用いて漸近計算して、ここで、トライグラムと、2つのバイグラムの推定値との誤差が最小となるように漸近計算して平滑化して学習することにより、本実施形態のモデルパラメータを有する統計的言語モデルを生成する。そして、本実施形態では、2つのバイグラムのパラメータにより、トライグラムのパラメータの代替として用いている。【0048】図3及び図4は、図1の言語モデル生成部20によって実行される言語モデル生成処理を示すフローチャートであり、図3及び図4を参照して、言語モデル生成処理について説明する。まず、図3のステップS1においてすべてのi,j,kに対して次式のように、【数21】α0(wi)1【数22】α1(wj,wi)1【数23】α2(wk,wi)1と初期化する。次いで、ステップS2においてすべてのj、kの組み合わせに対して次式を用いてそれぞれバイグラムの正規化パラメータz(wk,wj)及びユニグラムの正規化パラメータzuniを計算する【0049】【数24】【数25】【0050】さらに、ステップS3においてすべてのi,j,kに対して次式を用いて中間パラメータm(wk,wj,wi)を計算する。【0051】【数26】【0052】ここで、C(wk,wj,wi)は3つの単語にてなる単語列の出現回数であり、C(wk,wj,wi)/Tはトライグラムの確率、すなわち、トライグラムa(wk,wj,wi)となる。次いで、ステップS4においてすべてのi,j,kに対して次式を用いてそれぞれm0(wi),m1(wj,wi),m2(wk,wi)を計算する。【0053】【数27】【数28】【数29】【0054】さらに、図4のステップS5においてすべてのi,j,kに対して次式を用いてそれぞれα0(wi),α1(wi,wj),α2(wi,wk)を更新する。【0055】【数30】α0(wi)α0(wi)×a(*,*,wi)/m0(wi)【数31】α1(wi,wj)α1(wi,wj)×a(*,wj,wi)/m1(wi,wj)【数32】α2(wi,wj)α2(wi,wk)×a(wk,*,wi)/m2(wi,wk)【0056】ここで、*はワイルドカードであり、すなわち、任意の単語である。次いで、ステップS6において次式を用いてクルバック・レイグラーの発散値D(誤差に対応する。)を計算する。【0057】【数33】【0058】そして、ステップS7において発散値Dは所定のしきい値Dth(例えば、900)よりも小さいか否かが判断され、NOのときは収束していないと判断してステップS2に戻り、上記の処理を繰り返す。一方、ステップS7でYESのときは、ステップS8においてすべてのj,kの組み合わせに対して上位数24及び数25を用いてそれぞれバイグラムの正規化パラメータz(wk,wj)及びユニグラムの正規化パラメータzuniを再計算し、ステップS9において計算されたパラメータα0(wi),α1(wj,wi),α2(wk,wi),z(wk,wj),zuniを含む統計的言語モデルを統計的言語モデルメモリ22に格納して当該言語モデル生成処理を終了する。【0059】さらに、連続音声認識装置の構成及び動作について以下に説明する。図1において、単語照合部4に接続され、例えばハードディスクメモリである音素HMMメモリ11内の音素HMMは、各状態を含んで表され、各状態はそれぞれ以下の情報を有する。(a)状態番号(b)受理可能なコンテキストクラス(c)先行状態、及び後続状態のリスト(d)出力確率密度分布のパラメータ(e)自己遷移確率及び後続状態への遷移確率なお、本実施形態において用いる音素HMMは、各分布がどの話者に由来するかを特定する必要があるため、所定の話者混合HMMを変換して生成する。ここで、出力確率密度関数は34次元の対角共分散行列をもつ混合ガウス分布である。【0060】また、単語照合部4に接続され、例えばハードディスクなどの単語辞書メモリ12に格納される単語辞書は、音素HMMの各単語毎にシンボルで表した読みを示すシンボル列を格納する。【0061】図1において、話者の発声音声はマイクロホン1に入力されて音声信号に変換された後、特徴抽出部2に入力される。特徴抽出部2は、入力された音声信号をA/D変換した後、例えばLPC分析を実行し、対数パワー、16次ケプストラム係数、Δ対数パワー及び16次Δケプストラム係数を含む34次元の特徴パラメータを抽出する。抽出された特徴パラメータの時系列はバッファメモリ3を介して単語照合部4に入力される。【0062】単語照合部4は、例えばワン−パス・ビタビ復号化法を用いて、バッファメモリ3を介して入力される特徴パラメータのデータに基づいて、音素HMMと単語辞書とを用いて単語仮説を検出し尤度を計算して出力する。ここで、単語照合部4は、各時刻の各HMMの状態毎に、単語内の尤度と発声開始からの尤度を計算する。尤度は、単語の識別番号、単語の開始時刻、先行単語の違い毎に個別にもつ。また、計算処理量の削減のために、音素HMM及び単語辞書とに基づいて計算される総尤度のうちの低い尤度のグリッド仮説を削減する。単語照合部4は、その結果の単語仮説と尤度の情報を発声開始時刻からの時間情報(具体的には、例えばフレーム番号)とともにバッファメモリ5を介して単語仮説絞込部6に出力する。【0063】単語仮説絞込部6は、単語照合部4からバッファメモリ5を介して出力される単語仮説に基づいて、統計的言語モデルメモリ22内の統計的言語モデルを参照して、終了時刻が等しく開始時刻が異なる同一の単語の単語仮説に対して、当該単語の先頭音素環境毎に、発声開始時刻から当該単語の終了時刻に至る計算された総尤度のうちの最も高い尤度を有する1つの単語仮説で代表させるように単語仮説の絞り込みを行った後、絞り込み後のすべての単語仮説の単語列のうち、最大の総尤度を有する仮説の単語列を認識結果として出力する。本実施形態においては、好ましくは、処理すべき当該単語の先頭音素環境とは、当該単語より先行する単語仮説の最終音素と、当該単語の単語仮説の最初の2つの音素とを含む3つの音素並びをいう。【0064】例えば、図2に示すように、(i−1)番目の単語Wi-1の次に、音素列a1,a2,…,anからなるi番目の単語Wiがくるときに、単語Wi-1の単語仮説として6つの仮説Wa,Wb,Wc,Wd,We,Wfが存在している。ここで、前者3つの単語仮説Wa,Wb,Wcの最終音素は/x/であるとし、後者3つの単語仮説Wd,We,Wfの最終音素は/y/であるとする。終了時刻teと先頭音素環境が等しい仮説(図2では先頭音素環境がx/a1/a2である上から3つの単語仮説)のうち総尤度が最も高い仮説(例えば、図2において1番上の仮説)以外を削除する。なお、上から4番めの仮説は先頭音素環境が違うため、すなわち、先行する単語仮説の最終音素がxではなくyであるので、上から4番めの仮説を削除しない。すなわち、先行する単語仮説の最終音素毎に1つのみ仮説を残す。図2の例では、最終音素/x/に対して1つの仮説を残し、最終音素/y/に対して1つの仮説を残す。【0065】以上の実施形態においては、当該単語の先頭音素環境とは、当該単語より先行する単語仮説の最終音素と、当該単語の単語仮説の最初の2つの音素とを含む3つの音素並びとして定義されているが、本発明はこれに限らず、先行する単語仮説の最終音素と、最終音素と連続する先行する単語仮説の少なくとも1つの音素とを含む先行単語仮説の音素列と、当該単語の単語仮説の最初の音素を含む音素列とを含む音素並びとしてもよい。【0066】以上の実施形態において、特徴抽出部2と、単語照合部4と、単語仮説絞込部6と、言語モデル生成部20とは、例えば、デジタル電子計算機で構成され、バッファメモリ3,5は例えばハードディスクメモリで構成され、音素HMMメモリ11と単語辞書メモリ12と学習用テキストデータ21と統計的言語モデル22とは、例えばハードディスクメモリなどの記憶装置に記憶される。【0067】以上実施形態においては、単語照合部4と単語仮説絞込部6とを用いて音声認識を行っているが、本発明はこれに限らず、例えば、音素HMMを参照する音素照合部と、例えばOne  Pass  DPアルゴリズムを用いて統計的言語モデル22を参照して単語の音声認識を行う音声認識部とで構成してもよい。【0068】【実施例】さらに、本発明者は、本実施形態の言語モデル生成部20を用いて行った実験及びその実験結果について説明する。本特許出願人が所有する英語旅行対話タスク・コーパスに基づいて、距離2のDUAMEモデルについて実験を行った。距離2の結合されたエントロピー最大モデルのパープレキシティを、最尤(ML)バイグラム及びトライグラムモデルの場合と比較した。また、学習モデルの基本単位は、可変N−グラム言語モデルで学習された拡張された単語クラスとして定義した。可変N−グラム言語モデルは、エントロピーを最小化に基づいて、品詞クラス(POS)からクラスを分離した(例えば,従来技術文献11「H. Masataki et al.,Variable order n-gram generation by word-class splitting and consecutive word grouping, In ICASSP96, pp.I-188-191, 1996年」や従来例の特開平9−134192号公報参照。)。ここで、拡張されたクラスの最終的な数は1069である。【0069】ここで、次式の標準的パープレキシティPP及び次式のクルバック・レイブラー(Kullback-Leibler)の発散値Dをそれぞれ用いた2つの方法を使用して、DUAMEモデリングの収束状況を確認した。【0070】【数34】PP=exp((−1/n)×lnP(X))【数35】【0071】図5及び図6は、この2方法の収束過程の一例を示したものである。ここで、各N−グラムモデルのテストセットにおけるパープレキシティは、公知のCMU−ケンブリッジ言語モデリングツールキットを使用して計算した。その結果を表1に示す。【0072】【表1】英語旅行対話タスクにおける各種モデルのパープレキシティPP(学習セット:471,632、テストセット:23,337)——————————————————————————————————                                制  約  数              モデル      ユニグラム  バイグラム  トライグラム  d2 UA      PP——————————————————————————————————GTトライク゛ラム     1069    3672  207057    なし  49.5LNトライク゛ラム     1069  63672  207057    なし  52.2DUAME    1069  63672      なし  98798  52.1GTハ゛イク゛ラム     1069  63672      なし        なし  56.9LNハ゛イク゛ラム     1069  63672      なし        なし  58.8——————————————————————————————————(注)GT:グッド・チューリング;LN:線形;DUAMEにおけるd2  UAはバイグラムである。【0073】  表1から明らかなように、パープレキシティPPに関しては距離2のエントロピー最大モデルの方がバイグラムモデルより低く、また、線形ディスカウントバックオフ法によるトライグラムに匹敵するものであることを示している。【0074】以上説明したように、本実施形態によれば、言語モデル生成部20は、学習用テキストデータメモリ21に格納された学習用テキストデータに基づいて、処理対象の単語のユニグラムと、処理対象の単語とその単語から直前の距離1にある単語との間のバイグラムと、処理対象の単語とその単語から直前の距離2にある単語との間のバイグラムと、上記ユニグラム及び2つのバイグラムを正規化するための正規化パラメータとを含むモデルパラメータを有する統計的言語モデルを、処理対象の単語のトライグラムと、2つのバイグラムの推定値との誤差が最小となるように平滑化して学習することにより統計的言語モデルを生成する。従って、本実施形態によれば、N−グラムモデルの統計的言語モデルに比較してV倍少ないメモリ容量で実現することができ(ここで、Vは語彙数である。)、学習データ中に現われなかった事象に対する予測を事前知識なしに人手を介さず自動的に平滑化して学習することができる。また、きわめて良好な近似的な高次のN−グラムの統計的言語モデルを得ることができる。さらに、従来例のN−グラムの統計的言語モデルを使用した音声認識に比較してより高い音声認識率で音声認識することができる。【0075】【発明の効果】以上詳述したように本発明に係る請求項1記載の統計的言語モデル装置によれば、所定の話者の発声音声文を書き下した学習用テキストデータに基づいて、処理対象の単語のユニグラムと、処理対象の単語とその単語から直前の距離1にある単語との間のバイグラムと、処理対象の単語とその単語から直前の距離2にある単語との間のバイグラムと、上記ユニグラム及び2つのバイグラムを正規化するための正規化パラメータとを含むモデルパラメータを有する統計的言語モデルを、処理対象の単語とその直前の2つの単語との観測値のトライグラムと、上記2つのバイグラムの推定値との誤差が最小となるように平滑化して学習することにより、上記モデルパラメータを有する統計的言語モデルを生成する生成手段を備える。従って、本発明によれば、N−グラムモデルの統計的言語モデルに比較してV倍少ないメモリ容量で実現することができ(ここで、Vは語彙数である。)、学習データ中に現われなかった事象に対する予測を事前知識なしに人手を介さず自動的に平滑化して学習することができる。また、きわめて良好な近似的な高次のN−グラムの統計的言語モデルを得ることができる。さらに、従来例のN−グラムの統計的言語モデルを使用した音声認識に比較してより高い音声認識率で音声認識することができる。【0076】また、本発明に係る請求項2記載の音声認識装置によれば、入力される発声音声文の音声信号に基づいて、所定の統計的言語モデルを用いて音声認識する音声認識手段を備えた音声認識装置において、上記音声認識手段は、請求項1記載の統計的言語モデル生成装置によって生成された統計的言語モデルを用いて音声認識する。従って、本発明によれば、N−グラムモデルの統計的言語モデルに比較してV倍少ないメモリ容量で実現することができ(ここで、Vは語彙数である。)、学習データ中に現われなかった事象に対する予測を事前知識なしに人手を介さず自動的に平滑化して学習することができる。また、きわめて良好な近似的な高次のN−グラムの統計的言語モデルを得ることができる。さらに、従来例のN−グラムの統計的言語モデルを使用した音声認識に比較してより高い音声認識率で音声認識することができる。
統計的言語モデル生成装置及び音声認識装置 - 株式会社エイ・ティ・アール音声翻訳通信研究所
