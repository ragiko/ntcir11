Front-End Factor Analysis for Speaker Verificationを読んだメモ
論文メモ
IEEEのAudio, Speech and Languageから2011年5月発表の論文について読んだメモ.(Author: Najim Dehak, 他)メモどころか下手くそな訳みたいになってしまったので,今後はもっと簡潔にメモを取りたい(ついでに途中で大変になってきたので,省略が多い).メモ中に(#...)と書いているのは,自分の解釈です.間違っている可能性高いです.Baum-Welch statisticsというのが出てきたが,とりあえずBaum-Welchの統計量?とした.レイリー係数もなんのことか分からない.[用語]WCCN: Within-Class Covariance NormalizationLDA: Linear Discriminate AnalysisNAP: Nuisance Attribute ProjectionEER: Equal Error RateMinDCF: minimum Detection Cost FunctionJFA: Joint Factor AnalysisUBM: Universal Background Model[ざっくりとした概要]話者照合に使う特徴ベクトル(i-vector)についての論文.GMMスーパベクトルからi-vectorを取り出すよう因子分析する.i-vectorだと低次元の話者とチャネル依存の特徴ベクトルとして扱えて,チャネル依存の因子の影響を抑えるために,後でチャネル補正の手法(WCCN,LDA,NAP)を適用するらしい.コサインカーネルを使ったSVMでの識別と,i-vectorのcos類似度をとって閾値で照合を決定する方法を比較したら,後者の方が精度良いし高速という結果.最も精度が良かったのは,NIST2008評価セットの男性英語話者に対して,i-vectorにWCCNを適用後,LDAを適用してcos類似度を計算した場合で,EERが1.12%で,MinDCFが0.0094になった.ちなみに,JFAよりもi-vectorでcos類似度計算して比較した結果の方がEERが4ポイント男女ともに改善したとのこと.[モデルについて][既存手法(JFA)]JFAだと1つの発話のGMMスーパベクトルが次のようにモデル化されると仮定している. : UBMからの一般的なスーパベクトル : 話者部分空間( : 固有声行列(発話ごとに変化しない要因), : 残差行列(対角行列)) : セッション空間(固有チャネル行列(発話(セッション)ごとに変化してしまう要因)このモデルの中で, と は話者依存の因子で, はセッション依存の因子と見なせ,それぞれの部分空間でで正規分布するランダムな値.このJFAを話者認識に適用するには,まず開発コーパス(学習データ)から の部分空間を推定して,それから話者とセッション因子の を評価したいターゲットの話者に対して推定するっぽい.話者依存のスーパベクトルは,(セッション空間のものを除いて)(セッション補正は  ということ)ただし,このJFAだとチャネル因子にも話者情報が含まれていることが実験的に分かっているらしい(N. Dehak, "Discriminative and generative approches for long- and short-term speaker characteristics modeling: Application to speaker verification").この話者依存スーパベクトルに対して、(学習で得た発話のスーパベクトルと)テスト発話の類似度を計算して,スコアリングして照合を決定するらしい.スコアリング手法もいくつか種類がある模様.[提案手法(i-vector)]JFAだと固有声行列Vと固有チャネル行列Uの2つの空間で定義していたのを,i-vectorだと1つの空間で再定義するらしい.その1つの空間というのが,総変動空間であって,話者変動もチャネル変動も同時に含んだ空間になって,総変動行列で最大の固有値を持つ固有ベクトルを含むように定義されるらしい.(#総変動行列の,最大の固有値の固有ベクトルで空間を定義する?)このモデルだと,GMMスーパベクトルに対して話者もチャネルも分離されないので,ある1つの発話に対してのモデルとしては以下の式のようになる.M: GMMスーパベクトルm: 話者とチャネルに独立なスーパベクトルT: 低ランクの矩形行列w: N(0, 1)の正規分布をするランダムベクトルの要素は総変動因子(the total factor)?で,これらのベクトルを"i-vector"(個人を特定するベクトル)と呼ぶ.このモデルの特徴は、を平均ベクトルと共分散行列で正規分布すると仮定していることで,総変動行列 の学習過程は固有声行列 の学習とほぼ全く同じで,唯一違うのは,固有声学習で,所与の話者の全ての録音音声が同一の人のものだという過程をしているのに対し,総変動行列の学習には,ある所与の話者の発話セットの全体が異なる話者によって生成された(所与の話者からの全ての発話が異なる話者によって生成された)と見なしていることである.つまり,この提案手法のモデルでは,発話を低次元の総変動空間に写像するシンプルな因子分析のように見なせる.ただ,総変動因子のは,所与の発話に対しBaum-Welchの統計量?(#Baum-Welchアルゴリズムのこと?)での条件付き事後分布で定義されうる隠れ変数で,その事後分布は正規分布であり,平均はi-vectorと一致する.Baum-Welchの統計量?はUBMを使って抽出される(UBMからBaum-Welchアルゴリズムで事後分布を導出する?).例えば,Lフレーム と特徴空間の次元数 で混合数 のUBM  とすると,Baum-Welchの統計量では,所与の発話 に対してi-vectorを推定するのに, と を得る.ただし, は正規分布の添字(index)で,はベクトル を生成する混合要素 の事後確率に一致する.つまり,i-vectorを推定するには,UBMの混合要素の平均(UBMの混合平均?)に基づいて集約化した1次のBaum-Welchの統計量を計算する必要があって以下の式のように計算される?.このは混合要素 に対するUBMの平均であり,所与の発話に対するi-vectorは,以下の公式から得られる.N(u)は次元の対角行列として定義してあり,対角成分はN_c I (c = 1, ..., C).は次元のスーパベクトルで,所与の発話uに対してBaum-Welch統計量の第1次元を全て連結したもの. は因子分析の学習中に推定される 次元の対角共分散行列であり,総変動行列Tでは捉えられないようなものの残差分散をモデル化している.[SVMを使って識別する場合]cosカーネルを使ったSVMで識別する場合について述べているところ.どうやらcos類似度使って閾値用意して、決定した方が精度も処理も高速らしいので,飛ばす.[cos類似度スコアリング]cosカーネルの値を直接使ってスコアリングしよう,という話.target話者のi-vector  とtestのi-vector とのcosカーネルの値を決定値 と比較することで,照合するか否かを決定する.注目すべきは,targetとtestのi-vectorの両方が,全く同じように推定されるため,i-vectorは話者認識の新しい特徴と見なせるということ.enrollmentステップは要らない.(#話者のラベル付けが要らないってこと?)この提案手法なら因子分析をしているが,話者とチェネルの影響をモデル化しているというよりも,特徴抽出器のような役割を果たして,cosカーネルの値を決定値と比較するだけだからJFAと比べて複雑じゃなく,凄い速いらしい.[セッション間補正]チャネル補正をGMMスーパベクトル空間ではなく,低次元の総変動因子空間で行うことで計算が少なくなるらしい.邪魔な影響(nuisance effect)の補正手法は,大体3つあるらしく,WCCN,LDA,NAPを試して実験.i) WCCN話者因子空間に対して適用.クラス内共分散の逆行列を使って,cosカーネルを正規化する手法.Hatchにより導入された手法.1対他クラス分類を使って,対象話者と詐称者の間に線形分離するSVMで使用された.WCCNの基本的な考えは,SVMの学習段階での誤受理率と誤棄却率を最小化することである.そのエラー率を最小化するために,クラス分類のエラー尺度の上界を定義している.最適解は,これらの上界を最小化するのと同様にクラス分類エラーを最小化するものを見つけることで,結果的に得られる解は,線形カーネルの式で与えられる.R: 半正定値の対称行列最適に正規化されたカーネル行列は、 R = W^{-1} で与えられ,W は学習時にバックグラウンドで,全ての詐称者に対して計算されるクラス内共分散行列である.これは,1クラスに属する1人の所与の話者から全ての発話がなされたと仮定しているため,そのクラス内共分散行列は,以下のように計算される.ただし,  は各話者のi-vectorの平均であり, は話者数, は話者  の発話数である.(発話ごとにi-vectorが存在する.)cosカーネルでの内積を保存するために,特徴写像関数  は以下のように定義される.ただし,B は [ tex:W^{-1} = B B^t ] のようにコレスキー分解によって得られる.この論文では,WCCNアルゴリズムはcosカーネルに適用したらしく,その新しく得られたカーネルの式は以下のようになる.WCCNアルゴリズムは,セッション間の分散を補正するために,クラス内共分散行列を使って,cosカーネル関数を正規化しているが,NAPやLDAといった手法と違い,(#元の?)空間での(#データの分布の?ベクトルの向きの?)方向性の保存を保証している.ii) LDA1人の所与の話者から全ての発話が1つのクラスの表現をしていると仮定する場合,LDAはチャネルの影響によるクラス内分散を最小にするように、新しい空間軸を定義することに当たる.LDAの手法は,いらないものを除くように識別基準を元にして,話者間の分散に関するものを最小化するために行う.パターン認識の分野で広く使われている,次元削減の手法.この手法にある考えは,異なるクラス間をより識別しやすくなるように新しく直交軸を見つけることである.その軸はクラス間分散を最大化して,クラス内分散を最小化するように求める.この論文のモデルでは,各クラスは1人の話者からの全ての発話によって構成されるものとし,LDAの最適問題は,以下の比率に沿って定義される.この比率は,レイリー(#減衰?)係数と言われたりする.これは,所与の空間方向での,クラス間分散 とクラス内分散 (式(13)と同じ) の情報率の量を表しており,以下のように計算される.ただし, は各話者のi-vectorの平均で, は話者数, は話者 の発話数.i-vectorの場合,話者の母平均ベクトル はnullベクトルと同じで,理由は因子分析において,これらのi-vectorがの標準正規分布(0の平均ベクトル)を持つためである.つまるところLDAの目的は,レイリー係数を最大化することであり,この最大化は,一般的な固有値公式から得られる,最大の固有値を持つ固有ベクトルによって構成される写像行列 を定義するのに使われ, は固有値の対角行列.LDAによって得られた写像行列 をi-vectorは通るため,2つのi-vector  と  間のcosカーネル式は以下のように書き換えられる.iii) NAPi-vectorのバックグラウンド?で計算されたクラス内共分散の最大の固有値をもつ固有ベクトルに基づくチャネル空間を定義する[5].その後,直交的に補正されたチャネル空間(話者空間)に,新しくi-vectorを写像する手法.邪魔な方向性(#影響)を除くように適切な写像行列を見つける手法.その写像行列は,特定の話者にのみ依存するような,チャネルの補空間で直行に写像してくれるもので,以下のように定式化される.Rは式(13)でのクラス内共分散(もしくはチャネル共分散)の最大の固有値を持つ 個の固有ベクトル(kベスト固有ベクトル)を列に持つ,低次元の矩形行列である.これらの固有ベクトルはチャネル空間を定義し,NAP行列でのcosカーネルは以下のように定式化される.ここでの,  は2つの全てのi-vectorである.[実験][結果]
データセット手法EER(English)DCF(English)EER(All)DCF(All)
Front-End Factor Analysis for Speaker Verificationを読んだメモ - 無shockな人生歩みたい日記
