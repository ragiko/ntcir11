Ngram言語モデルメモ
自然言語処理
はじめに
現在よく使われていると思われる確率的言語モデルについて簡単に調べてみたのでメモ。
Ngram言語モデルとは
例えば、「お酒が飲みたい」と「バリウムが飲みたい」という文章があった時に、前者の方がよく聞く文章で、後者はほとんど聞かない文章
上記のような「文章の出やすさ」を数学的モデルで表現したい
特に確率を使って表現したい(確率的言語モデル)
単語列が与えられたとき、その単語列の生起確率は
例えば「お酒/が/飲みたい」は、P(お酒が飲みたい)=P(お酒)*P(が|お酒)*P(飲みたい|お酒が)
しかし、P(単語|ながーい文章)を求めるのは実際には難しい
単語の種類がmで単語列の長さがnならば、m^n通りをすべて計算して値を推定しなければならない無理
Ngram言語モデルは、「各単語の生起確率は、直前の(N-1)単語までのみに依存する」モデル(Markovモデル)
2gramモデルなら、「今日/の/お昼/は/パスタ/です」はP(今日のお昼はパスタです)=P(今日)*P(の|今日)*P(お昼|今日の)*P(は|のお昼)*P(パスタ|お昼は)*P(です|はパスタ)
0gramモデルは、直前の単語に依存しない=全ての単語が等確率で生起するモデル
1gramモデルは、その単語が現れる確率のみを考慮するモデル(順番は考えない)
文頭文字$を考えて、P(今日|$$)*P(の|$今日)*P(お昼|今日の)*...とすれば統一的に扱える
Ngram確率の推定
学習データとしていくつかの文章が与えられたとして、を推定したい
最尤推定
単語列が学習データに現れる回数をとする
P(単語|直前の単語列)=(「直前の単語列+単語」の出現回数)/(「直前の単語列」の出現回数)
単語のN個組と(N-1)個組の相対頻度
学習データ中に単語列が出てこなければ確率が0になってしまう
たまたま学習データにでてこなかっただけかもしれない、、、(実際は0ではない)
学習データに出てこない単語列に対しても適切な推定値を与えたい(スムージング、平滑化)
加算スムージング
出現回数に一定の値を加えたものを使う
: 定数(0<δ<=1)、δ=1のときラプラス法と呼ばれるみたい
V : 単語列の異なり総数(Ngramのuniqな総数?)
簡単だけど、他の手法に比べて精度が悪い
全部同じオフセットじゃいろいろおかしい
線形補間法(Interpolation)
Ngramの確率値をMgram(N>M)で線形補間する
Interpolationでは、低次のngramは常に考慮される
補間係数λの推定の手順
"学習データ"と、学習データとは異なる"評価データ"を用意する
学習データからNグラム確率を求める
評価データでの生成確率が最大になるようにEMアルゴリズムで各補間係数λiを最適化する
補間係数の推定手法
ヘルドアウト補間法
学習データを2つに分けて、片方をNグラム推定用、もう片方を補間係数推定用に使う
削除補間法
学習データをm個にわけて、j個目以外でNグラム推定して、j個目で補間係数を推定する(1個だけ取り出す方法はleaving-one-out法)
各jについて求めた補間係数の平均値が収束するまで推定を繰り返す
バックオフ・スムージング(Back-off)
学習データで出現するときはグッドチューリングの推定値を使って、出現しないときは(1-全ての出現する場合の推定値の和)を出現しない単語に均等に確率を分配する
Back-offでは必要に応じて低次のngramを使用する
のとき、
は、ディスカウント係数
のとき、
αは、ディスカウント係数を掛けた確率の和は1にならないので、その差分をC=0の単語列の生成確率に分配する関数
カッツ・スムージング
出現回数の多い単語列は最尤推定、k個以下のときグッド・チューリング推定値を用いる手法
チャーチ・ゲイル・スムージング
改良グッド・チューリング推定法を用いる手法
ウィトン・ベル・スムージング
のとき、
のとき、
は、単語列の後に出現した異なり単語数
「"I like pen"、"I like apple"、"I like pen"」が出現したならr(I like)=2
http://www.ee.columbia.edu/~stanchen/e6884/labs/lab3/x207.html
ワン・カウント法
単語の生起にディレクレ分布を仮定
性能がよいらしい(特にtrigram)
は、定数
は、であるような単語列の数
Kneser-Neyスムージング
かなり高い性能を持つスムージング手法
性能的に一番は「改良Kneser-Neyスムージング」らしい
原論文のpdfが見れないので式がよくわからない、、、
原論文pdf
http://www.ece.umassd.edu/FACULTY/acosta/ICASSP/Icassp_1995/pdf/ic950181.pdf
参考
http://aclweb.org/anthology/P/P08/P08-2036.pdf
http://d.hatena.ne.jp/tsubosaka/20100414
その他のNグラムモデルの改良版など
Nクラスモデル
単語をクラス(カテゴリやクラスタ)にわけて、クラスの生起確率とクラスからの文字の生起確率を考える
キャッシュモデル
直前に使われた単語は再び使われやすいという性質を取り入れたモデル
トリガーモデル
距離のはなれた単語間の共起関係を取り入れたモデル
可変長Ngramモデル
Nを可変にすること(単語に応じて直前の何単語を見るかを変えること)で、モデルの信頼性を失わず、精度を向上させることができるモデル
階層的ベイズ言語モデル(ベイズ階層言語モデル)
http://acl.ldc.upenn.edu/P/P06/P06-1124.pdf
階層的ベイズモデルを使った言語モデル
階層的ディレクレ言語モデル
ディレクレ分布を仮定した階層的ベイズ言語モデル
単語のpower-lowな性質を考慮できてない
階層的Pitman-Yor言語モデル(Kneser-Neyスムージングの近似)
power-lowを考慮するためにPitman-Yorプロセスを用いた階層的ベイズ言語モデル
Nested Pitman-Yor言語モデル
階層的Pitman-Yor言語モデルを入れ子にした言語モデル
参考文献
計算と言語4「確率的言語モデル」
http://www.denizyuret.com/ref/goodman/chen-goodman-99.pdf
http://www.r.dl.itc.u-tokyo.ac.jp/~nakagawa/SML1/smoothing1.pdf
http://plata.ar.media.kyoto-u.ac.jp/mori/research/topics/LM/LM.pdf
http://chasen.org/~daiti-m/paper/naist-dlec2004-lmodel.pdf
http://mastarpj.nict.go.jp/~mutiyama/corpmt/6.pdf
http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf
Permalink | コメント(0) | トラックバック(0) | 01:20 
Ngram言語モデルメモ - Negative/Positive Thinking
