年越ししてから既に2ヶ月が過ぎ、2月も終わりが見えかけてきた今日この頃です。生存報告をかねて、少しだけ最近やっていた事を書いておきます。去年の年末くらいに、面白そうな論文を見つけたのでそれを読みつつ、実装していました。NIPS2009 で発表された論文です。その名も Conditional Neural Fields 。
http://books.nips.cc/papers/files/nips22/NIPS2009_0935.pdf名前から何か感じるところの有る人もいそうですが、これはCRFに隠れ層を加えて、非線形にした物になります。自分のメモ用に、先に簡単に CRF についておきます。--CRF の説明はNLP2006のチュートリアル資料が割と分かりやすいです。http://nlp.dse.ibaraki.ac.jp/~shinnou/lecture/nl/NLP2006.pdf私はオンライン学習器でしかCRFを作った事が無いので、ここではそのつもりで書きます。CRF では素性関数に対応するパラメータを学習するわけですが、その式は大雑把に書くと次のようになります。Λ_t+1 = Λ_t + η * (正解の素性関数ベクトル - 期待素性関数ベクトル)η は学習率、Λ は素性関数に対応するパラメータベクトルです。正解の素性関数ベクトルは入力系列が与えられれば素性と正解のラベルのペアを取り出すだけで作れます。要はどの素性とラベルのペアが何個有ったか、どのラベルとラベルの遷移が何回あったか、をカウントしてベクトルにしています。期待素性関数ベクトルはちょっと計算が面倒ですが、現在のパラメータΛのもとで、入力系列から生起しうる全てのラベル系列を考えます。各ラベル系列が生起した際の、各素性関数の期待値を計算し足し合わせた物が期待素性関数ベクトルです。まともに全てのラベル系列を生成して試すともの凄い計算量になるので、これは Forward-Backward アルゴリズムを用いて計算するのが一般的です。--前置きが長くなりましたが、 CNF について書きます。
CRF との違いは、観測素性の扱いになります。CRF では、素性関数 f(x,y) は素性xとラベルy のペアを観測した際に1になるような関数です。これが CNF では、f(h(X,t),y) になります。数式は元論文のままだと比較しにくいので変えてます。関数h はロジスティック関数で、X に対して非線形な値を取ります。t は系列のカレントの位置を表しています。f(h(X,t),y) は組み合わせ素性xとラベルy のペアを観測した際に、h(X,t)を返す関数と思ってください。X が大文字になっている理由は、CNF では入力系列に対して隠れ層で組み合わせ素性を取り出すためです。素性関数 f(h(X),y) は入力系列 X に対して、位置tに置ける組み合わせ素性 x とラベル y のペアに対して、h(X,t) を返します。h(X,t) は中で何をしているのかと言うと、入力系列 X とパラメータθベクトルの内積を計算して、その値をロジスティック関数に乗せて非線形な値にしています。こうする事で、単純に組み合わせ素性が観測されたというだけではなく、その素性に対して非線形な重みを与えています。次に、パラメータの最適化ですが、実は素性関数が非線形になっているという事を無視すると、観測素性についてのパラメータと、遷移素性についてのパラメータの更新の式は一緒になります。ただし、正解の素性関数ベクトルはどの素性とラベルのペアが何個有ったか、どのラベルとラベルの遷移が何回あったか、ではなく、素性に対して非線形な重みを計算し足し合わせたものになります。ラベルとラベルの遷移は相変わらずただのカウントです。要は、カウントの仕方が1ずつ足していたのが、実数値ずつ足すようになった物です。期待素性関数ベクトルも同じ要領です。 入力系列から生成されうる全てのラベル系列を考えて、各ラベル系列が生起した際の各素性関数の期待値を計算して足し合わせます。 ただし期待値を計算する際には素性に対して、これまで有った/無かったで済ませていたところにh関数の値が入ります。パラメータ推定での1つ大きな違いは、これに加えて隠れ層のパラメータθについても更新することです。
uchiumi log: Conditional Neural Fields
