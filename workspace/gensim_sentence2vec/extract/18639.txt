
次へ: 多層パーセプトロン
上へ: 多層パーセプトロン
戻る: 単純パーセプトロン
単純パーセプトロンの学習
単純パーセプトロンの結合荷重(パラメータ)を推定するための学習アルゴリズ
ムとしていくつかの方法が提案されているが、Rosenblattらの方法は、ネット
ワークにあるパターンを分類させてみて間違っていたら結合荷重を修正する誤
り訂正型の方法であった。しかし、この学習規則は、線形分離可能でない場合、
すなわち、誤識別 0 にする線形識別関数が存在しない場合には、誤り訂正の
手続きを無限に繰り返しても解に到達できない可能性がある。また、学習を途
中で打ち切った場合に得られるパラメータが最適であるという保証がない。
これに対して、出力ユニットの入出力関数として線形関数を用い、ネッ
トワークの出力と教師信号と平均2乗誤差を最小にするような結合荷重を推定
する場合には、平均2乗誤差の意味で最適なパラメータを求めることができる。
今、 個の学習用のデータを 
とする。
ここで、
が入力ベクトルで、その入力ベクトルに対する望みの出
力(教師信号)が  である。この時、この学習用のデータに対する2乗誤
差は、
(63)
となる。最適なパラメータを求めるために、パラメータ(結合荷重) 
を逐次更新することにより次第に最適なパラメータに近似させる最急降下法を
用いることにすると、2乗誤差 
のパラメータに関す
る偏微分を計算する必要がある。2乗誤差 
のパラメー
タに関する偏微分は、
(64)
となる。ただし、 とする。また、
であ
る。従って、最急降下法によるパラメータの更新式は、
(65)
のようになる。これは、Widrow-Hoffの学習規則(Widrow-Hoff learning rule)
と呼ばれている。また、教師信号とネットワークの出力の誤差 
に応じてパラメータを修正するため、デルタルール(delta rule)
と呼ばれることもある。
Widrow-Hoffの学習規則では、最急降下法を用いて逐次近似によりパラメータ
を推定するが、最適な解を行列計算により陽に求めることも可能である。
今、学習用データの入力ベクトルを並べた
次元の行列を 
とし、教師信号を並べた
次元のベクトルを 
とする。これらを用い
ると2乗誤差は、
(66)
のように書ける。これをパラメータ  で偏微分し、0 と置くと、
(67)
となる。従って、が正則ならば、最適なパラメータは、
(68)
となる。これは、重回帰分析(multiple regression analysis)と呼ばれる最も
基本的な多変量データ解析と等価である。重回帰分析では、
を説明
変数 (explanatory variable)、  を目的変数(criterion variable)と呼ぶ。
入出力関数としてロジスティック関数を用い、最尤法によりパラメータを推定
する場合には、ロジスティック回帰と呼ばれる手法と等価となる。この場合に
は、ロジスティック回帰のためのパラメータ推定アルゴリズムとして知られて
いるフィッシャーのスコアリングアルゴリズムを学習に利用することも可能で
ある。
平成14年7月19日
単純パーセプトロンの学習
