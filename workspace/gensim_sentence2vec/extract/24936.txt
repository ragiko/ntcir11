
画像認識技術はここ数年で長足の進歩を遂げ、中でもGoogleはその進歩の成果の一部をエンドユーザーにも提供している。どれぐらい進歩したかを知るためには、たとえばGoogle Photosで自分の画像を検索してみるとよいだろう。でも、物や情景を認識することは、最初の一歩にすぎない。
9月にGoogleは、今や人気のディープラーニング(deep learning, 深い学習)手法を使った同社のやり方が、単一の物の画像を認識するだけでなく、一枚の画像中のさまざまな物(果物籠にいろんな種類の果物がある、など)を分類できることを、みんなに見せた。
それができたら次は、画像を自然言語で説明することに挑戦したくなるだろう。Googleはそれを、今トライしている。Google Researchのペーパー(小論文)によると、写真に下の例のようなかなり長い説明文をつけられるように、自分自身を教えるシステムを開発した。今すでにそれは、相当正確だそうだ。
Googleの研究者たちが述べているところによると、この問題への典型的なアプローチはまずコンピュータヴィジョンのアルゴリズムに仕事をさせ、その結果を自然言語処理に渡して説明文を作らせる。それで十分なようだが、しかし研究者たちは、最新のコンピュータヴィジョン技術と言語モデルを一体化した単一のシステムを訓練して、画像を与えると人間可読な説明文を直接作り出す方がよい、と言っている。Googleによると、このやり方は二つの再帰型ニューラルネットワーク(recurrent neural network, RNN)を組み合わせた機械翻訳で有効だった。翻訳と写真のキャプション付けはちょっと違うが、基本的なやり方は同じだ。
Googleのやり方が完璧、という意味ではない。機械翻訳のクォリティを人間による翻訳と比較する指数BLEUスコアでは、コンピュータが作ったキャプションは27〜59点ぐらいのあいだだ。人間は69点ぐらいになる。でも、25点に達しないほかのやり方に比べると、大きな進歩だけど。
[原文へ]
(翻訳:iwatani(a.k.a. hiwa))
複雑な画像のキャプション(説明文)を自動生成するシステムをGoogleが研究開発中 - TechCrunch
