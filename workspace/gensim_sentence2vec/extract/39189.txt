スペシャルセッション音声A/音声B/電気音響［音によるシーン理解とその進展 I］・あらゆる音の検出・識別を目指して—音響イベント検出研究の現在と未来—(NTT CS研・大石康智)さまざまな録音に対して、これまでは音声や音楽などの特別な信号を検出・識別することが多かったが、それ以外の雑音も含めてすべての音を認識する「音によるシーン理解」。ICASSPでの発表件数も徐々に増えている。これまでの研究課題を「音の特定度」と「時間的参照範囲」を軸として整理する。特定度は、データベースの音との同一性基準(完全同一波形かどうか)で、時間的参照範囲はフレーム単位か長時間統計を使うか。これらの分類は音楽検索からの借用。大分類として「同一音の検出・識別」「音響イベントの検出・識別」「音声・音楽の区間検出・識別」「音環境の検出・識別」の4つを認定。同一音の検出は完全に同じ波形の検出。研究は少ないが、今後の需要は高い。音響イベント検出では、MFCC-HMMやNMFなどが利用されている。音声/非音声/音楽区間の検出・識別では音声区間検出技術などが使われ、汎用信号区間検出技術(GSAD)もITU-Tで標準化されている。音環境の検出・識別は、映像クリップへのインデクシングや推薦などに使われる。BoWをベースにSVMなどの識別が一般的。関連する競争型ワークショップ。CLEAR(2006-2007)は12種類の音響イベントの検出と9種類の音環境の識別を行っている。D-CASE(2012-2013)はオフィス環境での16種類の音響イベント検出と10種類の音環境の識別を行っている。Albayzinはスペインの大学機関でのワークショップ。87時間分のTVニュース番組の音響信号の時間区分化タスクがあった。TRECVID MED(2010-2014)はTRECVIDの中のMultimedia Event Detectionのタスク。TRECVID MER(2012-2014)はMultimedia Event Recountingで、イベントが検出された証拠を列挙して説明するというタスク。全体的な傾向。シンプルな特徴量や識別器で大規模データを評価している。音響イベント検出は難しく、特にイベントが重なると困難。最先端の機械学習を駆使した研究の紹介。DNN、ベイズ学習。特にベイズ学習の研究例の紹介。・Acoustic word learning from temporal-frequency patches for sound event detection (NICT・Xugang Lu)音のイベント検出(例としてTED Talkの音響イベントによるセグメンテーションが出てきた)。古典的にはMFCCやPLPなどの特徴量によって特徴抽出し、HMMなどでモデル化するか、統計量をSVMなどで分類する。ここで音声の場合には信号の「構造」がよく定義されているが、音響イベント(拍手、笑いなど)ではその中にどんな構造があるのかよくわかっていない。ここで、繰り返し観測される時間周波数パターン上のパターン(Acoustic Word)を教師なし学習によって抽出し、それに基づいて識別を行う。学習の際にはwhiteningを行う。Acoustic wordを抽出した後、入力信号のセグメントから各Acoustic wordへのユークリッド距離のベクトルを作る(posteriogramみたいなものか)。係数の小さいところは0に置換してベクトルをスパース化する。従来のGMM-HMMでは、十分な性能が出ない。Whiteningは効果がある。スパース化を行うとちょっと性能が上がる。Acoustic wordの数は多い方が性能が上がる。・画像認識技術に基づく映像中のイベント検出(NII・佐藤真一)画像・映像認識タスク。何が写っているか、何をしているか、場所はどこか、何のシーン化などを計算機により解析する。従来のタスクでは、さらにこれをブレークダウンして「画像・映像意味分類」「物体検出」「画像・映像検索」「テキスト変換」などが行われる。画像・映像分類では、100～10000程度のあらかじめ決められたカテゴリに画像・映像を分類。物体検出では、あらかじめ定められた物体を画像・映像中から検出して場所を特定する。画像映像検索はクエリから対応する画像・映像を見つける。テキスト変換は、画像や映像を説明する自然言語を生成する。画像の特性。前の物体は後ろの物体を隠すので、本質的に非線形の扱いが必要。画素・部品の絶対位置はあまり意味をなさない。照明条件、物体の姿勢、背景などを無視しなければならない。画像認識処理の主な流れ。局所特徴量(SIFT,DenseTrajectoriesなど)を抽出し、Bag of Wordsを作ってSVMなどで識別。画像認識におけるコンテキスト。車の認識器を学習させると、道路を検出していたりする。また家を識別しようとすると周囲の植え込みを認識する識別器ができることがある。これらの情報はコンテキストとよばれ、物体認識を助けると考えられている。データ収集。音声の場合は収録ができるが、画像の場合には自分でデータを作成することが難しいので、既存の画像・映像から収集して利用する。「タスク指向データ収集」はタスクを決めて(識別クラスなど)からデータを収集する。COIL-100, Caltech 101/256 データセットなど。「無背景」や「姿勢正規化」などの制約を与えて問題を簡略化している。PASCAL VOCやImageNetはより難しいタスク。全体としては問題が簡単になりすぎる傾向がある。「データ指向データ収集」ではタスクと独立にデータが選ばれている(TRECVIDなど)。こちらは逆に問題が難しくなりすぎ、何の問題を解けばよいのかわかりにくくなりがち。正解の付与を行う場合に、タスクがないので0から正解を付与する必要があり、判断に迷うこともある。厳正な評価のために。テストデータを開発者からいかに隔離するか。PASCAL VOCでは評価データの正解は公開されず、サーバに評価結果を送ると正誤判定だけが返ってくる。TRECVIDでは評価データを公開正解は参加チームの結果提出後に公開。スペシャルセッション音声A/音声B/電気音響［音によるシーン理解とその進展 II］・Detection and classification of acoustic events using multiple resolution spectrogram patch models (NTT)音響イベント検出において、音響イベントごとに時間・周波数解像度がどのように影響するかを調べ、複数の時間周波数解像度によるスペクトログラムパッチを使う方法を提案。スペクトログラムのセグメントを使ったDNN-HMM(オートエンコーダによるプリトレーニング)を識別器として利用。複数の時間周波数解像度によるスペクトログラムを計算して、イベントごとに精度の高い解像度の特徴だけを選んでそれぞれ識別し、その結果を統合して最終結果を得る。・イベント遷移を考慮した音響トピックモデルによる欠損を含む観測からの音響シーン推定(NTT)井本さん。不特定多数のユーザによって適当に収録された音響コンテンツを対象に音響イベント検出をしようとすると、タッピングノイズなどの雑音によりデータに欠損が生じるので、それへの対処。音響イベントから音響シーンを推定する手法として音響トピックモデルを利用。欠損の補間方法として、音響イベント系列の生成をHMMでモデル化し、観測されなかったイベントはHMMから生成する。さらにイベント生成をHMMとした音響トビックモデルの導出。従来法では欠損があると性能が大きく下がるのに対し、提案法では欠損があっても性能が下がらない。・クラウドセンシングにより収集された環境音のシンボル表現を用いた音地図構築手法(岡山大)原先生。環境音の感じ方は物理量だけでなく時間帯や場所、人による影響もある。将来的にそのような研究に役立てるため、今回は時間・場所の情報を持つ大規模な音環境データベースを作成した。環境音収録アプリをAndroidで収録。音そのものと8帯域BPFの出力値を収録。騒音レベルは常に収録し続け、音そのものの収録はデバイス保持者が手動で行う。実際に音を収録する実験を行い、騒音レベルを地図に重ねてマップを作った。また、人鳥虫車風の音の識別実験を行った。識別はGMM-UBM。F値で0.5くらい。・車両の自動検出と分類—都市センシングを目指して—(TOA)音と振動から車両の検出と分類を行う。状況は車が1台だけ走行している状況を仮定。識別タスクは乗用車vs.トラック。特徴量にはTDSC(フレーム内の波形の局所最大値と局所最小値、ゼロクロス間隔などから計算)を使う。分類では音よりも振動を使ったほうが性能がよく、両方を組み合わせることでさらに性能が上がる。最後に都市センシングの構想について。・Speaking-face detection for multimodal person recognition in TV shows (岐阜大)田村先生。TV映像中の人の認識。音声・映像中の顔や文字などの情報を統合して人の認識を行っていく。そのために、発話中の顔の認識を行う。映像中から顔を検出してそのシーケンスを作成し、シーケンス内の各画像について高次局所相関特徴量を計算する。音声については通常の39次元特徴量を用いる。識別器はGMM。正解精度76%ぐらい。スペシャルセッション音声A/音声B/電気音響［音によるシーン理解とその進展 III］・運筆音を利用した手書き数字認識(三重大)表題の通り。対象は数字0～9で、手本をなぞり書き。認識方法はHMM(状態数は書いている時間に比例)。特徴量はMFCC+Δ。ΔΔは使っても性能が上がらない。認識率85%ぐらい。0と6、4と5などの誤認識が多い。・ロボットのための音シーン理解技術の実装例(大阪産大)マイクロホンアレイで複数の音源の内容を収録し、方向推定によって音源方向を特定。音源にビームを向けて特定の音源の音を収録し、また音源方向にNULLを作ることで背景音を収録する。収録後、言語音だったた書き起こす。プラットフォームはRobovie。マイクロホンアレイにはMEMSマイクを使っている。・多重衝突音のパワー包絡を用いた危険音検出の検討(立命館大)監視カメラで音を取って危険な音を検出する。従来はMFCC+HMMによる識別だったが、今回は危険音、特に衝突音が短時間に繰り返し発生する音(多重衝突音)に有効な特徴量を検討。これらの音は「椅子が倒れる」とか「物が落ちる」に対応する。研修に有効な特徴量として、時間波形のパワーの包絡を利用した。識別はパワー包絡を特徴量としたGMM。GMMをフレームごとに構成するので、認識対象は固定長である必要がある気がする。パワー包絡だけだと単発音(コップを置く音など)との識別が難しいので、最初にヒューリスティックに単発音かどうかを判定し、その後識別を行う。・反復スペクトル減算のための連検定に基づく雑音環境識別(立命館大)音声強調のために重み付き反復SS(L-SS)を用いてミュージカルノイズを抑えながら雑音を抑圧する。その性能を上げるため、雑音環境ごとに異なるSSパラメータを利用する。Kurtosis比からミュージカルノイズが出ているかどうかを判定し、ミュージカルノイズが出ない程度の反復回数にとどめる。実験の結果、定常雑音と非定常雑音ではパラメータの値が大きく違う。そこで、雑音が定常的なのかどうかを判定するために「連の検定」を利用する。この方法で高精度に定常・非定常判定ができる。・ユビキタスセンシングに基づく日常生活行動データベースの構築(名古屋大)西田先生。COI STREAMのプロジェクト。高齢者の生活行動のセンシングと見守り。生活行動の認識を行うための基礎データとして、実環境を対象にした72時間連続での日常生活行動データベースを構築した。収録したのは映像、音(小型カメラによる)、加速度、角加速度、位置、地磁気(スマホによる)。集めたデータをタグ付けした。各行動での音声対数パワー分布が示されたが、行動によって結構違う。また、13の行動パターンの認識を行った。処理単位は1分、音のMFCCと加速度の平均+標準偏差。識別はGMM。評価データ以外をすべて学習に使うと、約90%の精度。・うっかり者を手助けする環境音認識アプリの開発について(和歌山大)環境音認識システムROCKON。歩きスマホ中に周囲の環境音を認識してユーザに知らせるというアイデア。実際の消防車の音は認識できたそうだ。認識システムの評価として、AdaBoostとHMMの性能比較を行った結果、HMMよりもAdaBoostの方が性能が高かった。データはAndroidアプリを自作してクラウドソーシングで集めているそうだ。
09/05 音響学会3日目まとめ | aitoの日記 | スラッシュドット・ジャパン
