
第3章:検索エンジン
3-1:仕組み
3-1-0.それぞれの特徴
3-1-1:検索手法の仕組み
3-1-2:ロボット型の情報収集の仕組み
3-1-3:ロボット型の索引作成の仕組み
3-2:性能評価の意義
3-3:評価
目次へ
第3章:検索エンジン
WWWは極めて素晴らしい技術であるが,1つの大きな欠点があった。ウェブで何かを見つけようと思ったら,探し回るしかない。これでは,何千もの通り沿いに商品がぎっしり並んだ巨大な市場を歩き回るようなもので,何時間もかかる。その通りに並んでいるものはほとんどがつまらないものか,全く必要の無いものだが,貴重な探し物を見つけるためには,それらの中も歩き回らねばならない。この蚤の市というか、はっきり言ってしまえばジャングルのようなところを切り分けて進むために、登場したのが検索エンジンと言うシステムである。このサービスは,昼夜ウェブの中を自動的に歩き回り,強調された単語を見つけると,それらの場所を示す膨大な索引の中に保存しておく。ユーザが探したい語句を入力すると、その語句を含むホームページを索引の中から全て探し出し、一覧にして表示する。
検索エンジン、サーチエンジンの定義は実際の所あいまいであるが、一般には次のような要件を持つ大規模な検索ツールだといえる。
分野を特定せずに多量の情報を収集している。
収集した情報をデータベース化している。
それらの情報を検索できる
WWWの情報検索システムは大きく2つに分けることができ、その一つはディレクトリ型といわれる主題分類表を用いたシステムで、もう一つはロボット型である。次項で、その具体的な仕組みをディレクトリ型とロボット型それぞれについて説明する。
3-1.仕組み
検索エンジンの仕組みをそれぞれの項で説明する。
3-1-0.それぞれの特徴
ディレクトリ型
ディレクトリ型は、インターネットのWebページの情報をカテゴリー別にして検索を行うサービスである。ディレクトリ型の特徴としては以下のようなものがある。
選別した情報を収録している。
収録情報を階層構造で分類している。
分類の階層をたどる検索を基本としている。
トップレベルのディレクトリから興味のある分野へのディレクトリと順に絞り込んで検索する事ができるため、直感的でわかりやすい。。一番の特徴は、分類分けなどの作業を全て人の手によって行っている事である。収録するURLが人手によって的確に分類されるため、検索時にノイズが比較的少ないなどの長所がある。が、ロボット型に比べデータ量は必然的に少なくなる。
検索を行う者の検索要求があいまいであったり、幅広い検索を行う時に有効である。
ロボット型
ロボット型は高性能なマシンと高度なソフトウェア技術によって、世界中に広がる膨大なWWW上の情報を高速に検索できるサービスである。これはWebページの内容をそのまま検索対象とすることから、全文検索サービスともいわれる。現在の主要な検索エンジンはYahoo!など一部のものを除けばほとんどがこのタイプである。ディレクトリ型が手作業で登録データの整備を行っているのに対して、ロボット型は情報の収集から索引ファイルの作成まで、データベース作成に関する一連の処理をプログラムによって自動的に行うため、収録情報量がディレクトリ型に比べ桁違いに多いことが特徴である。また、各ページの文字列から索引ファイルを作るため、ページ内の詳細な情報を検索することができる。つまり、広い範囲から情報を探すには最適のエンジンである。その反面、人間の判断が入っていないため、情報の質や分野を整理できないのが欠点である。また、エンジンごとに索引の仕方やコマンド体系が異なり、求める情報にいかに早くたどり着くかという意味で使いこなしには慣れと習熟が必要である。
検索者が比較的はっきりとした情報(検索)要求を持っていたり、より限定度の高い検索を行う時に有効である。
3-1-1.検索手法の仕組み
ディレクトリ型
Yahoo!においては、情報の収集は自薦・他薦で登録依頼のあったサイトだけを収録し、エンジン側で実際にそのサイトの情報を見て内容を確認し、サイト単位で分類する。このサイトの分類やカテゴリの作成などは、すべてサーファーと呼ばれる専門のスタッフが行っている。そして、サーファーは実際に個々のサイトの全てのページに目を通し、その内容を簡潔に表す説明文を付与し、データベースに登録する。Yahoo!などディレクトリ型のサーチエンジンの場合、検索はこの説明文の内容から行われる。
サイト単位で登録されていることがディレクトリ型の大きな特徴である。このサイトの登録の方法についてYahoo!が用いている手法に「インディビジュアル・リンク」と「カテゴリ・リンク」というものがある。まずは前者について。1つのサイトでもいくつか複数の内容をもっている場合や、単一の場合でも複数の側面を持っているようなサイトは、一箇所だけに登録されていたのでは探しにくくなってしまう。そこで登録時に内容を確認した上で、複数のカテゴリに登録するということがある。これが「インディビジュアル・リンク」である。次に後者の「カテゴリ・リンク」であるが、これはカテゴリごと別のカテゴリにリンクしてしまうという手法である。このカテゴリ・リンクは実際前者よりも多用されている。それはYahoo!の検索はページを検索するというより、カテゴリを検索するということに主眼をおいているからである。
現在Yahoo!はGoogleとの提携により、Yahoo!ディレクトリの検索結果がなければ自動的にロボット型であるGoogleの検索結果を返す。
ロボット型
ロボット型サーチエンジンの特徴は、情報の網羅性、情報の新鮮さ、検索の速さなどである。これらが実現できるのも、全ての処理をコンピュータによって行っているからである。ロボット型サーチエンジンを構成するシステムは大きく分けて、ウェブロボット、インデクサー、検索サーバー、ウェブサーバーの4つの部品からなる。基本的な仕組みは、まずウェブロボットが定期的に世界中のホームページを巡回してデータを取り込んできて、インデクサーがそれらのホームページ上の文字情報から単語インデックスと呼ばれるデータベースを作る。その単語インデックスデータベースをもとに検索サーバーは高速な全文検索サービスを実現する。ウェブサーバーは、ユーザーからの検索要求を受け付け、それを検索サーバーに送り、返ってきた検索結果をユーザーのウェブブラウザーに送信する。
3-1-2.ロボット型の情報収集の仕組み
情報収集を受け持つウェブロボットは、スパイダーやクローラとも呼ばれ、ウェブ上のホームページを構成するHTMLテキストファイルを自動的に収集するプログラムである。この時、ホームページがハイパーリンクでリンクされているという特徴を利用し、すべてのリンクをたどりながら世界中のウェブサーバーを訪れ、膨大なホームページデータをくまなく収集するのである。
このロボットプログラムは、Webサーバに対し通常のHTTP GETを行いページを得る。次にGETしたページを解析してURLをたどり、取り出したURLに対しても同様の処理を繰り返す。
今日の十数億ともいわれるホームページを全て収集するには、ウェブロボットは高速で効率的にデータを取得しなければならない。大規模なロボット型エンジンであるgooでは、複数の汎用コンピュータを高速ネットワーク接続することで、あたかも1つのスーパーコンピュータのように動かし、1日で1000万ページデータを収集可能とする高速性が実現されている。こうしたgooの技術のベースにあるのが、米国の NOWグループが開発しInktomi社が商用化したエンジンである。Inktomi社のエンジンは米国でも商用検索エンジンとして利用されており、 WWWを代表する検索エンジンである。
また、ニュースページから個人のホームページまで存在しているウェブでは、更新される周期もそれぞれで大きく異なる。こうしたホームページの更新周期に合わせてウェブロボットを走らせ、常に最新のデータをそろえておくことが必要となる。gooのロボットプログラムでは、現時点でのWWWのサイズでは、1ヶ月足らずで全WWWデータにアクセスできるという。
しかしオンラインメディアのためにデータの監査を請け負っている米ABC Interactive社によると、すべてのWebサイトに寄せられるページ要求のうち平均して約7%がロボットによるものだという。サイトによってはこの数字が30%を上回ることがあると指摘しており、Webサイトの統計上全く無視できない数字であることがわかる。(WebSideStory社:2001年9月19日付、http://www.websidestory.com/cgi-bin/wss.cgi?corporate&news&press_3_145、日本文掲載:INTERNET Watch)[1]
3-1-3.ロボット型の索引作成の仕組み
ロボットによるページデータ収集後は、ページ上に存在する言葉が瞬時に検索できるように、データの処理が必要となる。この処理を行うプログラムがインデクサーである。これは、ウェブロボットが収集したホームページ全てを対象に単語インデックス、いわば索引を作成する。これにより、全ホームページに含まれる全ての単語について、どの単語がどのファイルに含まれるかを示すのである。この単語インデックスを使うことで、サーチエンジンは何千万というウェブホームページに対する高速検索を実現する。
単語インデックスを作成するには、テキスト中のすべての単語を切り出す必要がある。日本語は英文のように単語間がスペースで区切られていないため、単語の切り出しには複雑な処理が必要となる。ここで、テキストを単語ごとに区切るだけの処理を分かち書き処理といい、それぞれの単語の品詞と活用形を求めることも含めた処理を日本語形態素解析処理という。(形態素とは、意味を持つ単語の最少単位のことである。)例えば、「インターネット上に分散する情報の中から有益な情報を探している」というテキストを分かち書きすると、「インターネット 上 に 分散 する 情報 の 中 から 有益な 情報 を 探し て いる」となり、切り出した各単語からインデックスを生成できるようにする。この時、切り出した単語の中には「探す」がないため、このテキストでは「探し」では検索できるが、「探す」では検索できないことになる。そこで、動詞ならば終止形が標準というように、切り出した単語の品詞と活用形を形態素解析によって求め、各単語を標準形に変換して、この標準形から単語インデックスを生成する。形態素解析で主なものに、Gooで使われているNTTのInfoBee日本語形態素技術がある。
単語インデックスはサーチエンジンにおけるデータベースであり、これは単語とその単語が出てくるページのURLを組にした表によって表されている。検索時には、ユーザーが入力した検索単語とインデックス中の単語との照合をし、一致した行からその語を含むページのURLのリストを得る。例えば、単語インデックスに対して、「京都」という入力が与えられると、単語の欄が「京都」である行との照合が行われ、一致したらそのページのURLが検索結果として得られるわけである。
3-2.性能評価の意義
ほん研究ではインターネットを使用する多くのユーザが利用する、各種の検索エンジンの性能評価を行う。
最近の検索エンジンは、ポータル化を進め検索ヒット数を増やし、情報に重みをつけるといった、検索結果をいかに処理するかという部分に改良が移ってきている。また、検索スピードと検索ヒット数を売りにしているサイトもある。
しかし、いくら検索スピードを上げたとしても、ユーザが実際に直接感じるネットワークのサービスは、アプリケーション利用時のアクセス速度、応答速度、データ転送時間を含めたものである。
つまり、検索エンジン側のサービス性能の向上と、実際にユーザが感じるサービスにはギャップがある。
そこで本研究では透過的クライアント監視層を実装し計測する。これによりネットワークの末端であるユーザの感じるギャップを客観的に評価する事ができる。
プロキシによる透過的なネットワークシステムの性能評価システムの開発
