特徴選択とは機械学習と統計学の用語であり、頑健な学習モデルの構築のため、特徴集合のうち意味のある部分集合だけを選択する手法のことを指す。特徴量選択、変数選択、特徴削減、属性選択、素性選択、変数部分集合選択などとも呼ばれる。生物学の文脈では、DNAマイクロアレイの実験に基づいて影響力のある遺伝子を検出する手法を指す場合もある。不要で冗長な特徴量をデータから除去することによって、特徴選択は学習モデルを次の点で改善する:
次元の呪いの効果を緩和する。
汎化性能を向上させる。
学習を高速化する。
モデルの可読性を改善する。
特徴選択を行うと、データのうちどの特徴量が重要でありどのようにそれらが関係しているかなどといった点について、人間が理解しやすくなるという効果もある。
導入[編集]
単純な特徴選択アルゴリズムは場当たり的なものだが、より系統だったアプローチも存在する。理論的観点からは、教師あり学習問題において最適な特徴選択を行うには、選ばれた大きさのすべての部分集合を特徴集合から取り出し、総当たりで試す必要があるということが証明できる。特徴の数が多くなれば、このやり方は実用的でなくなる。実用的な教師あり学習アルゴリズムの特徴選択では、最適な集合ではなく満足できる集合を求めることになる。
特徴選択アルゴリズムは典型的には、特徴ランキングと部分集合選択という二つのカテゴリに分類される。特徴ランキングでは、ある指標によって特徴をランクづけし、一定のスコアに達しなかった特徴を除去する。部分集合選択では、最適な部分集合を目指して特徴の組み合わせを探索する。
統計学では、ステップワイズ回帰(英語版)がもっともよく用いられる特徴選択の形態である。この手法は、各ステップにおいてもっとも良い特徴を追加する(もしくはもっとも悪い特徴を除去する)貪欲アルゴリズムである。機械学習では交差検証によって特徴の良さを評価することが多く、統計学ではなんらかの規準を最適化することが多い。このやり方には入れ子型の特徴量に関する問題が内在しているため、分枝限定法や区分線形ネットワークなど、より頑健な手法が研究されている。
部分集合選択[編集]
部分集合選択では、特徴集合の部分集合がまとまりとして適切かどうかを評価する。部分集合選択のアルゴリズムは、ラッパー、フィルター、埋め込みの三種に分類できる。ラッパーは探索アルゴリズムを用いて可能な特徴の空間を探索し、それぞれの部分集合でモデルを走らせて評価を行う。ラッパーは計算量的にコストが高く、モデルの過剰適合を起こす危険性がある。フィルターは探索を行う点でラッパーに似ているが、モデルを走らせるかわりにより単純なフィルターを用いて評価を行う。埋め込み型の方法はモデルごとに特化したものであり、モデルに埋め込まれている。
よく用いられる探索のアプローチは貪欲な山登り法である。山登り法では、候補となる特徴部分集合を評価し、部分集合の一部を書き換えてそれが古い部分集合を改善している限り手続きを繰り返す。部分集合の評価では、特徴部分集合をスコアづけする指標が必要となる。総当たり探索は通常実用的でないため、実装者が停止点を定め、その停止点までに見つかったうち最高のスコアを持つ特徴部分集合を満足できる特徴部分集合として採用する。停止の規準は、アルゴリズムによって異なるが、部分集合のスコアがしきい値を超える、プログラムの実行時間が規定値を超える、などである。
探索のアプローチには、
総当たり
最良優先
焼きなまし法
遺伝的アルゴリズム
貪欲前向き選択
貪欲後ろ向き選択
などがある。
フィルターの規準として、分類問題では相関と相互情報量の二つがよく用いられる。これらのスコアは候補となる特徴(もしくは特徴部分集合)と求める出力カテゴリの間で計算される。
フィルターの規準としてはほかに、次のものがある:
クラスの分離性
誤分類確率
クラス内距離
確率分布の距離
エントロピー
一貫性に基づく特徴選択
相関に基づく特徴選択
最適性規準[編集]
特徴選択を制御する最適性規準には様々なものがある。もっとも古いものとしては マローのCp(英語版)統計量や赤池情報量規準がある。これらの手法では t統計量(英語版)が  を超えた変数を採用する。
その他の規準としては、  を用いるベイズ情報量規準 (BIC) 、  を近似的に用いる最小記述長(この近似の計算は正しくないとする議論もある[要出典])、  を用いる Bonnferroni 法や RIC 、偽発見率(英語版)に基づいて 付近のしきい値を用いる様々な規準がある。
特徴選択が埋め込まれている手法[編集]
Random forest (RF)
Random multinomial logit(英語版) (RMNL)
リッジ回帰(英語版)
決定木
Memetic algorithm(英語版)
ボトルネック層を持つ自己記述ネットワーク
プルーニング(英語版)ステップのあるその他の多数の機械学習手法
特徴選択のためのソフトウェア[編集]
MATLAB, Scilab, NumPy, R言語などの多くの標準的なデータ解析ソフトウェア(参考:en:Category:Data analysis software)では、特徴選択の機能が提供されている。特徴選択に特化したソフトウェアとしては次のものがある。
RapidMiner – 無料で公開されているオープンソースソフトウェア。
Weka – 無料で公開されているオープンソースソフトウェア。
Orange (ソフトウェア)(英語版) – 無料で公開されているオープンソースソフトウェア。(orngFSSモジュール)。
TOOLDIAG Pattern recognition toolbox – 無料で公開されている C のツールボックス。
minimum redundancy feature selection tool – 無料で公開されている、最小冗長性による特徴選択を行う C/Matlab のソースコード。
関連項目[編集]
クラスター分析
次元削減(英語版)
特徴抽出(英語版)
データマイニング
参考文献[編集]
JMLR Special Issue on Variable and Feature Selection
Feature Selection for Knowledge Discovery and Data Mining (本)
An Introduction to Variable and Feature Selection (サーベイ)
Toward integrating feature selection algorithms for classification and clustering (サーベイ)
Searching for Interacting Features
Feature Subset Selection Bias for Classification Learning
M. Hall 1999, Correlation-based Feature Selection for Machine Learning
外部リンク[編集]
NIPS challenge 2003 (NIPS(英語版)も参照)
Naive Bayes implementation with feature selection in Visual Basic (実行ファイルとソースコード)
特徴選択 - 機械学習の「朱鷺の杜Wiki」
特徴選択 - Wikipedia
