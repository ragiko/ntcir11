
[English][TSC Home]
NTCIR−2 自動要約タスク(automatic text summarization task)
(Last updated on July 5, 2000.)
以下に
NTCIR-2で行われる自動要約タスクの内容を説明します。
但し、本ホームページの内容は、最終版ではなく、今後必要に応じて
更新される場合があります。更新が行われる場合は、ホームページ上で
行い、案内をしますので、ご了承下さい。
(最新のtask description)
(1)タスクの説明
自動要約タスクでは,データを蓄積するとともに,システムの評価を行なう。
1 日本語テキストに対する要約データを蓄積
これまで人手でテキストを要約した言語データは,日本語に対してはごくわずかしか
作成されておらず,また,研究に利用可能なものが十分存在するという状況とは言え
ない.このタスクでは,新聞記事を対象に,人手で作成した要約データを大規模に蓄
積し,研究目的で利用に供したいと考えている.
今回の要約データ作成では,2種類の要約を作成する予定をしている.
一つは,いわゆる重要文抽出に基づく要約であり,要約作成者に,重要な文を選択し
てもらい,また,それらの重要文中の重要個所を選択してもらい,それらの重要個所
をつないだものを要約とする.
二つ目は,自由作成要約である.要約作成者に,原文にとらわれずに,自由に要約を
作成してもらう.要約作成者には,編集者,国語教師,記者など,ある程度要約とい
う作業に熟練している方々を依頼する.どちらの種類の要約も数百テキストの規模で
作成する予定である.
2 自動要約システムの評価
これまで要約の評価方法にはさまざまな議論があり,また我々も実行委員会やメイリ
ングリストでの議論を重ねてきたが,今回は,intrinsic(内的)な評価とextrinsic
(外的)な評価の2種類の評価を予定している.参加者は,どちらか一つ,あるいは
両方の評価に参加できる.
・intrinsicな評価
intrinsicな評価には,作成した要約データを用いたシステムの評価を予定している.
システムの評価は,作成した要約の種類に対応して次の2つを候補としている.重要
文抽出に基づいて作成された要約を評価に利用する場合,伝統的な評価方法同様,人
間の要約との一致度により評価を行なう.人間の自由作成要約を評価に利用する場
合,人間の要約との間の文字列上の一致度で評価を行なうのは困難であることなどか
ら,人間に主観的評価を行なってもらい,システムの要約がどの程度人間の要約に近
いかで評価する.評価基準としては,原文の重要な内容をどの程度カバーしている
か,読み易さ,およびその組合せである受容可能性を予定している.
・extrinsicな評価
一方,extrinsicな評価方法としては,システムの出力である要約を情報検索におけ
る適合性判定タスクに利用することで評価する方法を採用する予定である.これは,
1998年に開催されたTipsterのSUMMACでも評価方法の一つとして採用されたものであ
る.
情報検索タスクに基づく要約の評価は基本的に次のように行なわれる.まず,人間の
被験者に,検索要求とその検索結果としてテキストの要約を提示する.被験者は各要
約を読むことによって,そのテキストが検索要求に合っているかどうか(適合性)の判
断を行う.この適合性の判断をどの程度うまく行なえたか,判断にかかった時間など
を基に,提示された要約が良いかどうかを間接的に評価する.このことから,評価し
ている要約の種類は, `query-biased'で,指示的(indicative)な要約と言うことが
できる.
情報検索用テストコレクション(検索要求,テキスト集合,適合性判定の正解) とし
てどのようなものを用いるのか等未定の部分もあるが,原則的にSUMMACと同じ方法で
評価を行ないたいと考えている.評価基準としては,タスクに要した時間および,タ
スクをどの程度うまく行なえたかを示す指標として,再現率, 精度, F-measure(F値)
を用いる.
再現率 = 
被験者が正しく適合と判断したテキスト数/ 実際に適合するテキストの総数
精度  = 
被験者が正しく適合と判断したテキスト数/ 被験者が適合と判断したテキス
トの総数
F-measure = 2 * 再現率 * 精度/(再現率+精度)
(2)データ
1 要約データの題材
毎日新聞(有料)の記事を用いる.これまで作成されてきた要約データは,新聞記事,
特に報道記事に限定されてきた傾向が強い.今回の要約データ作成においては,その
ような現状を鑑み,報道記事だけでなく,社説などの論説記事も対象に要約データ作
成を試みる.
2 情報検索用テストコレクション
毎日新聞記事データ集を用いたテストコレクションを使う.
(3)参加の方法
第2回NTCIRワークショップ参加者募集のページ(http://www.rd.nacsis.ac.jp/~ntcadm/workshop/cfp2-ja.html)より
指示に従って行って下さい。申込みの期限は、2000年7月20日
(木曜日)です。
(4)スケジュール
2000年7月20日まで: 参加申込
2000年9月: Dryrun
2000年11月または12月:Evaluation (要約システム結果の提出)
2001年2月1日: 成果報告会のワーキングノート用仮論文の提出
2001年2月21-23日: 成果報告会(於: 東京。国立情報~学研究所一ツ橋記念大ホー
ル)
1日目: 一般公開、 2-3日目: 結果提出者のみ
2001年3月1日: 会議録用のカメラ・レディ原稿の提出
(担当:タスクの座長: 奥村学 (oku@pi.titech.ac.jp) または、福島孝博
fukusima@res.otemon.ac.jp )
TSC Home
TSC CFP
