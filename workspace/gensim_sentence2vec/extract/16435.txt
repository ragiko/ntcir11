これもある意味Deep Learning,Recurrent Neural Network Language Modelの話 [MLAC2013_9日目]
この投稿は Machine Learning Advent Calendar 2013 の9日目の記事です.
本日は,Neural Network(NN)を利用したLanguage Model(LM),つまりNeural Network Language Model(NNLM)の一種であり,
Recurrent Neural Network(RNN)を使ったRecurrent Neural Network Language Model(RNNLM)について説明します.
RNNLMに行く前置きとしてざっくりいくつか用意したので,知ってそうな人は先へ進んでください.
前置きを入れているうちに長くなりすぎた....
しかも,そもそもNNの説明し忘れたことに気づいて戦慄しました.
とりあえず手前味噌関連リンクを貼っておきます.A yet another brief introduction to neural networks
はじめに
RNNLMを提案したのはThomas Mikolovと言うひとです.
最近話題になったword2vecの人でもあります.
ただしword2vecはRNNLMを実装したものでは無いので今日は本題ではありません.
今日の本題は,彼らがword2vecの前に公開している rnnlmというツール の中身の話になります.
“ある意味deep learning”と言うのは,
後にでてくるBPTTで \(\tau\) を大きな値をとると,時間的に深く層を見ている形になるためです,
そのため多層ニューラルネットの問題にあったように, vanishing gradient problemが起きてしまうことがわかっています.
が,今回はその辺の学習の難しさや対策は踏み込みません,あんまり調べてないので.
前置き1: Recurrent Neural Network
RNNはNNで時系列解析を行うためのもので,80年代に提案されました
ある時刻tにおける隠れ層の状態を,次の時刻t+1の入力に使う.そのため,時刻t+1では,その時刻における入力+前回の履歴を時間的文脈として利用する.
RNNを言語モデルとして応用したものがRNNLMです
最初にRNNついて図を書いてみました,今後は右のシンプルなやつっぽい図のみを使います.
赤い部分が普通のNNと違うところで,前回の隠れ層の状態を保持して次回の入力に加える事で,
過去の履歴を利用します.
このRNNの特徴である赤と緑の部分を取り除くと,
モデルの形はシンプルなNNと同じになり,
同じようにBackpropagationによってパラメータ( \(CONTEXT(t)\) の前後の矢印)を学習することができます.
しかし,やはりRNNに特化した学習アルゴリズムであるBackpropagation Through Time (BPTT)という
ものもあり,後に説明します.
前置き2: Distributed Word RepresentationあるいはWord Embeddings
計算機で単語を扱う際に,単語をどう表現するかという話(表現方法はいくつかありますが,ニューラル言語モデル界隈だでの話です)
単語に連続値ベクトルを割り当てます.
で,ベクトルの中身がその単語の特徴をよしなに表現し,似た単語同士は似たベクトルになるよう学習します.
どんなベクトル空間になっているかの分析は後の Analysis で紹介する論文とかでされてます.
えんどう豆やカエルの卵やらに見えるものはベクトルです.
学習は,次のNeural Language Modelによって行われます.
その結果得られたDistributedなWordのRepresentationはWord Embeddingsと言われたりします.
言語モデル,n-gram言語モデルの説明は長くなるので,以下の二つの目的があることだけ示して進めます
過去の文脈(n-1個の単語)から,次に辞書中の各単語がどのくらいの確率で出てくるかを出力するモデル
単語の特徴を表現した単語ベクトルの学習
上の論文の中に,以下の図が出てきます.
この図を,先に作ってしまった 前置き2 の図にあわせて書き換えて説明すると,以下のようになります.
対応がわかりづらかったので,双方の対応する箇所にa.-e.という記号を付けました.
上の画像を右に90度回転させて,緑の四角をそれぞれの単語として見ます.
上の図は文脈が3単語(n=4)で,下の図は2単語(n=3)です.
showという単語のベクトルと,meという単語のベクトルを辞書から取り出して(b),
ふつうのNNと同じように,隠れ層(c),出力層(d)と計算していきます.
(d)は後述のsoftmax関数により辞書と同じサイズの確率分布になっていて,各次元が,そこに対応する単語が次に出てくる確率になります.
今回の場合,正解はyourなので,yourの所の確率を取り出してみます(e)
これが,この節の冒頭でいったふたつの目的の一つ目の処理です.
学習の際は,
NNのパラメータ(太い三つの矢印)
単語辞書中のベクトルの値(カエルの卵みたいな行列,厳密にはその時出てきている赤と緑の部分)
を同時に修正し,正しい予測ができるよう訓練します.
単語ベクトル(の辞書)も修正対象なので,正しい予測ができるようになった暁には,単語ベクトルも文脈が予測できるようななんか表現が得られていることが期待されます.
後半の2層Neural Networkから見ると,Backpropagationにおいて,
入力データにまで誤差が伝播していき修正されるイメージです.
これもある意味Deep Learning,Recurrent Neural Network Language Modelの話 [MLAC2013_9日目] &mdash; KiyuHub
