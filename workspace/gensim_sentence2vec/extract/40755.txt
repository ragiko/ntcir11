連続音声認識システムにおける単語挿入ペナルティーが, 真の言語確率とモデルの言語確率との差を補正する働きを持つことに着目し, 言語モデルのテストセットパープレキシティーと, 実験的に得られる最適な単語挿入ペナルティーから言語の真のパープレキシティーを推定する方法を検討する.品詞バイグラムと品詞トライグラムの2つの言語モデルを利用した認識実験の結果から真のエントロピーを推定し, 両者がほぼ一致することを示す.次に, 単語位置に応じて単語挿入ペナルティーの値を変化させる方法を検討する.本手法により, 固定的な単語挿入ペナルティーを用いる場合に比べて高い認識性能が得られること, 文を構成する単語数を無限大とした極限値が, 固定的な単語挿入ペナルティーと一致することを示す.
Optimization of Word Insertion Penalty (WIP) is discussed from the view point of entropy of a language model by 1) relating optimal value of WIP with the difference between true and model entropies, and 2) confirming the relation through recognition experiments. Difference of 'true' entropies estimated from bigram and trigram, i. e. 0.04 [bit], is much smaller than that of test set entropies of them, i. e. 0.2 [bit]. From this result, the function of WIP for compensating the entropy of the model is confirmed.

