スペクトラルクラスタリングの話
Machine Learning Advent Calendar 2012に参加させていただきました,@yonetaniryo と申します.現在,博士後期課程2年で,コンピュータビジョン・パターン認識に興味があります.最近,クラスタリング手法の一つであるスペクトラルクラスタリングについて勉強する機会があったので,今回はそれを紹介しようと思います.
2013 1.24 いただいたコメントをもとに,図を一部更新しました.
はじめに
本記事のモチベーション
本記事では,「スペクトラルクラスタリングについて何も知らない」人を「スペクトラルクラスタリングとは何かを大雑把には知っている」状態に持っていくことを目標にしています.具体的には,文献[1]の最初の方を紹介します.
本記事で扱う範囲
データのグラフ表現〜スペクトラルクラスタリングのアルゴリズム を扱います.
本記事で扱わない範囲
Normalized cutsとの関連: 文献[1][2]が詳しいです.
カーネル法との関連: 文献[1][3]が詳しいです.
スペクトラルクラスタリングのざっくりした説明
グラフの特性を表現するような行列としてgraph Laplacian matrixがあり,graph Laplacian matrixの固有値集合はグラフのスペクトル(スペクトラム)と呼ばれます[4].スペクトラルクラスタリング(spectral clustering)は,データをサンプル間の類似度に基づいてグラフ表現し,そのスペクトル(固有値)の計算を通してクラスタリングを解く手法です.[3][5][6]などで指摘されている通り,データをその局所性(元の特徴空間におけるペアワイズな類似性)を保存しつつ低次元空間に飛ばし(Locality preserving projection; LPP),その低次元空間の中で通常のクラスタリング(たとえばk-means)を行うことと同じです(良いグラフ表現をしていればk-meansなどせずにクラスタ分割ができます.詳しくは後述).
Similarity graph([1] Sec. 2)
スペクトラルクラスタリングにあたって,サンプル のペアワイズな類似性をグラフ表現する必要があります.このグラフをsimilarity graphと呼びます.
: similarity graph.
: サンプル  に対応.
: サンプル間の類似度に対応.類似度に応じて重みが与えられる(似てる/近いほど大きい).であり,のときはエッジ無し.ちなみにです.
重みの与え方はいくつかの種類があり,
-neighbor: 距離以下のサンプルのみ重みを与えます(他はゼロ).
-nearest neighbor: 各サンプルについて最も類似している個のデータ点にのみ重みを与えます.
Fully-connected: 重み(たとえばガウシアン重み)を全サンプル同士について与えます.
Graph Laplacian matrixとその諸性質([1] Sec. 3)
スペクトラルクラスタリングでは,similarity graphの性質をよく表現するような行列,graph Laplacian matrixを導入します.具体的には,
データ点の重み(近さ)を要素に持つの行列(はサンプル数)を weighted adjacency matrix として定義します.
サンプルに与えられる重みの総和 を要素に持つようなの行列をdegree matrix として定義します.
これらの行列を用いて,graph Laplacian matrix が得られます.
Graph Laplacian matrixは別名Kirchhoff matrixと呼ばれており[7],グラフを回路に見立てて,各頂点の電流入出量を表した行列と捉えることもできると思います.Graph Laplaciansには幾つかの重要な性質があります.以下ではそれを簡単に説明します(証明は付録).
性質1: 次元ベクトルについて,
性質2: は半正値対称行列(なので,固有値はすべて)
性質3: の最小固有値は0であり,対応する固有ベクトルは全要素1のn次元ベクトル.
性質4: 固有値0の重複度(multiplicity)は中の連結部分グラフの数に対応する(すなわち,similarity graph がに分解できる).また,固有値0に関する固有空間はによって張られる(は連結部分グラフの頂点数だけ1を並べたベクトル).
特に性質4が重要で,固有値0に対応する固有ベクトルが連結部分グラフの頂点集合を示しているということで,固有値問題を解くことでグラフの分割ができる=データのクラスタリングができる,というのが直感的に分かるかと思います.以上で準備は完了です.ちなみに,冒頭に定義したgraph Laplacian matrix は,正確にはun-normalized graph Laplacian matrixと呼ばれ,normalized graph Laplacian matrixというものもあります.2種類あります.
Normalized graph Laplacian matrixも,これまで紹介した諸性質に非常に似た性質を持っています.本記事では省略しますので,詳しくは[1]を参照してください.
スペクトラルクラスタリングのアルゴリズム([1] Sec. 4)
ここでは,unnormalized graph Laplacian matrix を用いたun-normalized spectral clusteringのみ紹介します.
Un-normalized spectral clustering
Input: データ,クラスタ数 Output: 各サンプルについてのクラスタID.
Similarity graph~ graph Laplacian matrix をつくる.
の固有ベクトルを,固有値が小さいものから順に個()求める.
固有ベクトルを列目に持つような行列 を作る.
が理想的に個の部分グラフに分割されている場合…は番目の連結部分グラフのindicatorであるはずなので,について 番目の行はサンプルの所属するクラスタを示しているはず.おしまい.
そうでない場合…番目の行をサンプルの新たな特徴ベクトルとして,通常のクラスタリング(たとえばk-means)を行う.
4. は先の性質4に由来する部分であり,スペクトラルクラスタリングのキモの部分になります.また,5. を考えると,スペクトラルクラスタリングが実質のところ局所性を保存した次元削減通常のクラスタリング,となっていることが分かります.が理想的に個の部分グラフに分割されている場合,と書きましたが,実際はそんな綺麗な分割になっていることはなく(similarity graphの計算の仕方に大きく依存します),出てくる固有ベクトルがindicator vectorsになることはまれです(多分まれだと思います).そこで,問題をちょっと読み替えて,部分グラフ間のエッジ重みが小さく(クラスタ間の距離が大きく)なるようなグラフの分割を探そうということを考えます.すなわちgraph cutsの問題であり,ここでようやくnormalized cutsが出てきます(つづきは付録).
まとめ
データのグラフ表現〜スペクトラルクラスタリングのアルゴリズムを紹介しました.基本的には,[1]の前半を踏襲した内容になっています.スペクトラルクラスタリングは非常に奥の深いテーマで,まだ理解できていないところは多いです… 誤り,補足等ありましたら随時修正していきますので,ぜひぜひよろしくお願いします.
参考文献
[1] A Tutorial on Spectral Clustering - Ulrike von Luxburg (url)[2] Normalized Cuts and Image Segmentation - Jianbo Shi and Jitendra Malik (url)[3] 機械学習概論 次元削減(2) - 東工大 杉山先生 (url)[4] Spectral graph theory - Wikipedia (url)[5] スペクトラルクラスタリングは次元圧縮しながらKmeansする手法 - 観月橋日記 (続生駒日記) (url)[6] スペクトラルクラスタリングの基本的な解説、および高速化手法のざっくりとした説明 - The beautiful mind (url)[7] Laplacian matrix - Wikipedia (url)[8] Spectral partitioning works: planar graphs and finite element meshes - Daniel A. Spielman and Shang-Hua Teng (url)[9] Algebraic connectivity of graphs - Miroslav Fiedler (url)
付録1: graph Laplaciansの諸性質とその証明
性質1: 次元ベクトルについて,
証明(ここが一番トリッキーですね!)
性質2: は半正値対称行列(なので,固有値はすべて)
証明性質1 のところで,なので,任意のベクトル について.
性質3: の最小固有値は0であり,対応する固有ベクトルは全要素1のn次元ベクトル.
証明: の番目の行をとおくと,より.したがって,.
性質4: 固有値0の重複度(multiplicity)は中の連結部分グラフの数に対応する(すなわち,similarity graph がに分解できる).また,固有値0に関する固有空間はによって張られる(は連結部分グラフの頂点数だけ1を並べたベクトル).
証明: 
のとき
固有値0に対応する固有ベクトルは性質3より.
のとき
個の連結部分グラフからなるgraph Laplacian matrix を考える.たとえばのとき.これまでの議論よりの最小固有値は0,対応する固有ベクトルは.このようなblock diagonal matrixについて,したがって,連結部分グラフ のgraph Laplacian matrix の固有値はの固有値であり,その固有ベクトルはのindicator vectorとなる.
付録2: Normalized cuts とスペクトラルクラスタリング
Normalized cutsは,最小カットに基づくグラフの分割を求める方法の一つで,分割された2つの部分グラフA, Bに関して,A, B間のエッジコストが小さく(最小カット)なり,かつA, Bそれぞれについて「頂点集合から全頂点へのエッジコストの総和」がバランスよくなるような目的関数を入れることが特徴になっています.また,この目的関数の最小化はレイリー商の最小化に帰着でき([2]に詳しく式変形が書かれています),結果としてグラフ分割を固有値問題で解くことになります.Normalized cutsによるグラフ分割は,先のnormalized graph Laplacian matrixを用いたスペクトラルクラスタリングに対応しており,
クラスタ内サンプル間の距離が小さくなるような分割
クラスタ間距離が大きくなるような分割
の両方を考慮することができます(un-normalized spectral clusteringは前者のみ考慮します.この辺の詳細は[1] Sec. 8.5を参照してください).
付録3: 固有ベクトルの選び方についての補足
が理想的に個の部分グラフに分割されていない場合においても,最小固有値は0かつ対応する固有ベクトルはとなります.したがって,実際のスペクトラルクラスタリングでは2番目に小さい固有値から順に個の固有ベクトルを用いる場合が多いようです.この2番目に小さい固有値に対応する固有ベクトルをFiedler vectorといい,Fiedler vectorのみを用いたグラフ分割はFiedler cut([8] Sec 2.3に概要があります)と呼ばれます.Fiedlerの論文は[9]にあります.
Permalink | コメント(7) | トラックバック(1) | 00:33
スペクトラルクラスタリングの話 - おいしいお米の話
