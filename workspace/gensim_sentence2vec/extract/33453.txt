
多変量解析
多変量解析
多変量解析とは
アンケートの調査やスーパーでの顧客購買情報など多くの場で様々なデータが収集できるようになってきた.これらは,年齢や性別,年収,最近の購買店など,複数の質問項目や属性項目を含めるため,ひとつの測定項目ではなく,複数の測定項目からなるデータになっている.ここで,このひとつひとつの測定項目のことを変数または変量という.ふたつの違いは変数が数学的な概念で,数値的な測定値を考えていることに比べ,変量は個体が担っている数量を表し,性別(男性,女性)のように名義的な測定値も含めている点が異なる.なお最近では,特にその違いを明確にせずにふたつを同じものとして扱っていることもあり,本稿では同様のものとして扱う.また,ひとつの変量(または変数)のデータのことを一変量データといい,ふたつ以上の変量(または変数)のデータのことを多変量データと呼ぶ.
変数が増えた分,多変量データはより大きな情報をもつことになり,これらから如何に有益な知見を得られるかが重要であろう.もちろん多変量を一変量の集まりとみなし,それぞれを変数ごとに解析することも可能だが,変量間の関係も考慮した多変量に対応するデータ解析法も利用したいところである.例えば,顧客の購買情報から今後の顧客購買傾向を予測する手法や患者をいくつかのグループに分類したいために複数の測定項目を用いたりする分類手法などがある.これらの多変量データに対応した統計手法の総称を多変量解析と呼ぶ.一般に,多変量データは表1のようなひとつの項目がひとつの列を,一人分のデータが ひとつの行を形成される.なお,この行列のことをデータ行列と呼びことがある.行列のそれぞれの列が変数で,各変数の取る値は,対象ごとに異なっている.この異なり具合をバラツキと呼び,統計解析では,このバラツキの中の規則性を発見することを解析の目的とする.
ひとつの変数だけを対象とした分析では,バラツキの中心やバラツキの大きさ,バラツキの偏りなどを調べるが,多変量データの場合,変数が複数あるため,これらを分析する場合は,各変数の上記のようなバラツキの特徴に加え,変数間の関係も調べることで,データからより多くの知見や変数間の相互影響を考慮した情報を得ることが可能である.
また,データの観測値にもさまざまなタイプが含まれており,大きく分けると変数の値が文字のものと数字のものがある.文字のデータと数字のデータでは含んでいる情報に違いがあるため,それぞれに適応した手法を選択する必要がある.
多変量解析は,表2のように目的変数がある手法とない手法のふたつに大別できる.目的変数とは,予測や変動の原因を調べたいと思っている変数のことで,外的規準や被説明変数と呼ばれることもある.
目的変数がある場合の分析としては,予測や判別の手法などがあり,目的変数の型が量的であれば重回帰分析,質的であれば判別分析,また質的データ(カテゴリデータ)に対応した数量化 I 類, II 類が提案されている.さらに回帰分析でもモデル式の違いより,ロジスティック回帰やプロビット解析,非線形解析など,様々な手法が提案されている.
目的変数がない場合の分析は,分析する変数間やサンプル間の関係を明確化させること目的で,手法としては,変数の合成値(総合指標)を作成する主成分分析や共通因子で各変数の変動を説明する因子分析,それらの質的データに対応した手法である数量化 III 類,IV 類がある.また,質的データの連関を線形モデルとして表現する対数線形モデル分析などが挙げられる.さらに対象や変数をそれらの関係からいくつかのグループに分類するクラスター分析や次元の縮約を考える多次元尺度構成法などデータのタイプや解析の目的によって多数提案されている.
様々な多変量解析手法
多変量解析手法はデータのタイプや解析目的に合わせ,様々な手法が提案されている.以下に主な多変量解析手法を述べる.
(1) 重回帰分析
重回帰分析は,観測される変数間の相関を利用することで,説明変数(予測する際に使用する変数.独立変数とも言う)の情報から目的変数(予測する変数.従属変数とも言う)の値を予測する手法である.目的変数の値を予測するための式としては,目的変数の一次式が用いられ,これを線形回帰と呼ぶことがある.対数関数や指数関数など,一次式以外の予測式を考えることもあり,その予測のあてはまりの良さを測る指標や多変数から予測に必要な変数のみを選択する変数選択法も提案されている.
以下では線形回帰について述べるが,他の方法でも同様な方法で考えることが可能である.
まず,説明変数がひとつの場合を述べる(この場合,単回帰モデルという).表3のようなふたつの変数にX とY に関するデータが与えられているとする.なお,ここではすべての変数が量的な数値データであるとする.
このとき,X (説明変数)の情報を使いY (目的変数)の値を予測するための式として通常,
yi = a + bxi
を用いる.この式を回帰式と呼び,a とb がデータから推定するパラメータで,それぞれ切片(または定数項)を回帰係数という.ここで,パラメータa ,b の推定法が問題となるが,これらは最小2乗法と呼ばれる推定法により推定される.これは,観測値と予測値との差(残差)の2乗和を最小とするようにパラメータの値を決める方法である.
ここで,説明変数の値xi が与えられたときに,回帰式による目的変数の予測値がa + bx iとなるので,予測値と観測値の差である残差 eiは,
で定義される.この残差の平方和をできるだけ小さくしようとするのが,最小2乗推定の原理である.
図1: 最小 2 乗推定
すなわち,
を最小にすることを考える.最小化のための具体的な計算は,残差平方和の式をa とb で偏微分し,それを0とおいた連立方程式
を解けばよく,
という結果が得られる.ここで,とはそれぞれ変数X とY の平均を表す.
各パラメータの値が確定し,モデル式に実測値を代入することにより各予測値が求められ,実測値と予測値の関係を考えることが出来る.
実測値と予測値の相関係数を重相関係数という.重相関係数は実測値と予測値の直線的な関係を測り,1に近いほどふたつの関係が強く,モデル式の当てはまりが良いことを示す.
また,y の実測値の平均周りの変動St のうち回帰式によって説明される変動(予測値の平均周りの変動)Srの割合r 2を考える.具体的な式は以下の通りである.
r 2のことを決定係数(寄与率ともいう)という.決定係数は 0 〜 1 の値をとり,1に近づくほど回帰式への当てはまりが良いことを意味し,すべての実測値がモデル式上に乗るとき,1の値をとる.なお,決定係数の値は重相関係数の2乗と一致する.
複数の説明変数を用いるときの回帰モデルは
となる.このモデルのパラメータ推定においても最小2乗法を利用できる.また,変数同士で高い相関がある場合,うまく予測できない問題(多重共線性(multicollinearity)の問題)がある.回帰分析の目的はよりよく目的変数を予測することにあるので,できるだけその目的に合うような説明変数の選択を行う必要がある.普通に考えれば,寄与率ができるだけ高くなるようにすればよいが,回帰モデルの適合度を表す寄与率は説明変数を増やせば増やすほど大きくなる.これは,みかけのあてはまりがよくなっているだけで,必ずしもモデルの説明力が高く,予測がうまくいくというわけではない.標本の大きさに比べ,説明変数の数がかなり小さくなければ,信頼できる結果とはいえない(誤差の自由度が大きいほど信頼できるモデルであるといえる).そこで,説明変数の取捨選択が重要な課題となる.この問題に対してまたモデルの善し悪しを測る尺度として,自由度調整済みの寄与率や AIC と呼ばれる統計量を利用する.
それぞれの統計量の定義式とモデル選択の基準は次の通りである.
自由度調整済みの寄与率
(値が大きいほど良いモデル)
AIC(赤池の情報量規準)
(値が小さいほど良いモデル)
複数の説明変数がある場合には,さまざまな説明変数の組み合わせの回帰モデルについて推定と上記統計量の計算を行い,最適と思われる回帰モデルを選べばよい.
回帰分析を行う場合,回帰係数の推定値やモデルの適合度などの統計量を求めただけで,分析を終わりにせず,回帰分析の最小2乗推定が最適な推定であるためには,残差に等分散性(説明変数の値や目的変数の値にかかわらず,残差()のバラツキは一定)と正規性(残差の分布は正規分布である)の成立条件があり,実際の分析においては,それらのチェックを行う必要がある.
これらの確認は,予測値と観測値の差である残差を求め,その分布を調べることで行う.通常,チェックする項目は,[1] 残差の分布が正規分布に近いものになっているか,[2] 残差の散らばり具合(分散)が説明変数や目的変数の値によらず一定になっているか,の2項目である.なお,時系列データの回帰分析を行った場合は,この 2 項目に加え,[3] 残差の系列相関の有無,についても調べる必要がある.以下にチェック項目の詳細を述べる.
[1] 正規性のチェック
残差のヒストグラムや正規確率プロットを作成して,正規性のチェックを行う.最小2乗法による推定の場合,必ずしも誤差項の分布が正規分布である必要はないが,残差の分布が正規分布から大きくかけ離れていることは,回帰モデルに何らかの問題があると考えることが自然で,また,回帰係数などの検定には正規分布の仮定が必要であるため,検定結果は信頼できるものではないことを意味する.
[2] 等分散(均一分散)性
前提条件のひとつに誤差項の等分散(均一分散)性がある.実際にデータ分析を行う場合,この前提が支持されないことが少なからずある.たとえば,売上高のようなデータを考えた場合,企業規模により誤差項のバラツキの大きさが変わると考える方が自然である.つまり,大企業と中小企業を比べた場合,大企業の売上高の方がかなり大きいので,その分売上高の変動幅が大きくなると言える.このような変数を含むデータの回帰分析を行う場合に,誤差項の分散が必ずしも均一ではなくなる.
分散の均一性のチェックを簡単に行う方法は,各観測変数と残差との散布図を作成する.この際,残差を縦軸側にとる方がチェックには便利である.均一であれば散布図内の点がほぼ同じ幅で散らばるが,均一でないと横軸の変数の値によりバラツキの幅が変わる.
なお,分散が一定でないような場合には,重み付き最小 2 乗法を用いることもある.
[3] 系列相関
回帰分析において,通常誤差項間は無相関であることを前提とするが,時系列データのように各観測単位間に相関があるデータの回帰分析を行う場合には,誤差項間に相関が現れる場合がある.そこで,時系列データについて回帰分析を行う場合には,誤差項間の相関の有無をチェックしなければならない.
ダービン・ワトソン統計量は,誤差項間に系列相関が存在するかどうかをチェックするための道具である.ダービン・ワトソン統計量(DW )は
で定義される.ここで,はi 番目のケースに対する誤差項を表す.
DW は0から4までの値をとるが,系列相関(正確にいえば1次の系列相関)がない場合には,2に近い値をとり,正の系列相関の場合には0に近い値に,負の系列相関の場合には4に近い値をとる.一般には,表4のような目安で系列相関の有無について判断を行う.
ここで,dl とdu は標本の大きさと説明変数の数で決まる定数である(数表についてはDurbin and Watsonの原著論文Testing for Serial Correlation in Least Squares Regression, Biometrika, vol 38 (1951)を参照).
(2) 判別分析
出身県や性別のように明確な判別基準があれば,その基準をもとにサンプルを判別可能であるが,そのような基準はいつもあるとは限らない.判別分析では目的変数を説明する変数(説明変数)を用いて,判別基準を作り,その基準を元に所属グループを予測(判別)する手法である.判別分析には基準に線形式を用いた線形判別分式やグループの分布も考慮したマハラノビスの距離を用いた判別法などが提案されている.
例えば,ふたつのグループにおける説明変数の同時分布が分かっているとする.新たなデータが与えられたとき,そのデータがどちらのグループから来たのかその所属グループをすることを考えたいが,各グループでの分布は分かっているので,1番目のグループから来た場合の可能性と 2 番目のグループから来た場合の可能性を計算し,両方の可能性を比較し,可能性が高い方のグループから来ていると判断するのが合理的と言える.
しかし実際には,分布が分かっていることはまずないため,その分布をデータから推定し,推定された分布を基に,可能性の比較を行うことになる.
分布が多変量正規分布であり,ふたつのグループの共分散行列が等しければ,可能性が等しくなる境界を変数の線形関数で表すことができる.
ここで,判別関数の値が正であるか負であるかで所属するグループを判別する.なお,判別関数のパラメータはなるべくグループ同士が分離するように求められる.
2群の判別を行う場合は,ひとつの判別式だけでよいが,3群以上になると複数の判別式が必要となる.例えば,グループが3つある場合,まずグループ1とそれ以外(グループ 2と3)を判別する判別関数を求め,次にグループ2とグループ3を判別する判別関数を求めればよいだろう.
また,別の方法としてグループの中心からのマハラノビス距離と呼ばれる距離を用いて,もっとも近いグループに判別する方法もある.なお,マハラノビス距離とは,バラツキの大きさや相関を考慮した距離である.
回帰分析の場合と同様に,判別のためのモデルである判別関数を評価する必要がある.回帰分析の場合は,目的変数の予測が目的であったので寄与率が重要な指標だった.判別分析においても寄与率を求められるが,目的が判別であるので通常は判別がうまくいった割合(または,その逆の間違って判別された割合)で評価される.
誤判別率は,与えられたデータを判別した場合に誤って判別されたケースの割合である.もちろん,誤判別率はできるだけ小さい方がよい判別と言える.例えば,実際のグループがわかっているデータに判別分析を用いて,その結果のグループと実際のグループに属する対象の数をまとめると次のような表が得られる.
データに基づいて求められた判別率は,判別式を作成したデータに対するものであり,新たなデータに対してのものではない.すなわち,実際の判別力は判別率よりも小さいと思った方がよい.そこで,本来の判別能力を調べるためには,推定に使用していないデータを用意する必要がある.非常に大量のデータが存在する場合は,データセットを推定に使用する部分と評価に使用する部分とに分けて分析することで誤判別率を評価することもよいだろう.
(3) 主成分分析
主成分分析は,多変数からなるデータをできるだけ情報を減らすことなく少ない次元でデータを表せるようにデータの縮約を行い,そのデータの裏にある構造の見つける手法である.
具体的には,元の変数の線形結合を考え,その分散が最大になるように線形式の係数を決める.もちろん,係数を大きくすれば分散はいくらでも大きくできるので,係数の2乗和が1であるという制約の下で分散が最大になるように求めることを考える(図3参照).すなわち,
線形結合:
ただし,
このように,分散が最大になるようにして決められた係数を基に求められる量を主成分という.主成分はデータがもっとも散らばっている方向を探していることになり,データにもっとも近い直線(平面)を求めていることと同じである.
例えば,データが与えられたとき,データの中心から各データへの距離は固定されるが,ここでデータの中心を通る1本の直線を引いたとする.この直線をできるだけ各データ点に近くなるようにすることは,各データ点から直線に対し垂線を降ろした点とデータの中心との距離が最大になることと同じであると考えられる.すなわち,この直線を新たな座標軸と考えれば,この座標上でのデータの分散を最大にするということと同じになる.つまり,主成分分析ではできるだけ各点から直線への距離の2乗和を最小にすることを考える.
図3: 主成分の意味
回帰分析の場合にも同様のことを行ったが,縦軸方向の距離を問題にしており,主成分分析では,すべての変数を平等に扱うため,通常の点と直線の距離を考える点が異なる.
主成分はひとつだけ求めるのではなく,最初の主成分が求まると,それと無相関になる2番目の主成分を求めることができる.2番目の主成分を求めるときは,1番目の主成分と無相関であるという条件の下で,その分散が最大になるようにする.以下,3番目以降も同じで,それまでに求まっている主成分とはすべて無相関となるように求められる.このように求めた主成分を順に,第1主成分,第2主成分,… などという.
主成分は変数の数だけ求めることができるが,データの数が変数の数より小さい場合は,データの数から1引いた数の主成分しか求めることはできない.
主成分を具体的に求める問題は,共分散行列(相関行列)の固有値問題に帰着される.共分散行列(相関行列)の固有値を大きい順に並べたとき,その固有値が主成分の分散になり,固有ベクトルが主成分を作成するときの係数になる.
主成分分析を行う場合,共分散行列を分析するか,相関行列を分析するかが問題になる.一般的には,すべての変数の単位が同じで各変数のバラツキの大きさも考慮した分析を行いたい場合は,共分散行列を分析し,そうでない場合は相関行列を分析する.相関行列を分析するという意味は,すべてのデータを基準化(平均を0,分散を1に変換すること)し,その基準化されたデータについて定義どおりの主成分分析を行うことを意味する.
また主成分は元の変数の線形結合であるが,この新たに作成された主成分が持つ意味を解釈することは重要である.主成分を解釈するときは,その主成分を定義する線形結合の係数に注目する.
主成分の計算が終わった後は,主成分をいくつまで採用するかについて決定しなければならないが,主成分数の決定は,固有値が1以上(カイザー基準)となる場合や累積寄与率など,複数の基準を総合的に判断し,採用する主成分の数を決定する.
得られた主成分のための係数を使って,元のデータから各個体について主成分の値を求めることができ,これを主成分得点と呼ぶ.各個体の主成分得点を利用して各個体の散布図を作成することにより,それぞれ個体の主成分に対する解釈を考えることができる.
(4) クラスター分析
クラスター分析という言葉は数多くの分類手法の総称である.クラスター分析では,各個体間の差を定義できるデータを基に,個体または変数間の似ている度合い(類似度)または似ていない度合い(非類似度,または距離)を考え,似ているものは同じグループ(このまとまりのことをクラスターと呼ぶ)に似ていないものは異なるグループに分類する.
クラスター分析法はひとつの結合の段階で随時クラスターを結合する(もしくは分岐する)階層的クラスター分析法と 1 回ですべての個体をいくつかのクラスターに分け,その後ある意味でよりよい分類結果になるように調整を行う非階層的クラスター分析法がある.
一般的に,階層的クラスター分析法は結合の過程が樹形図と呼ばれる二分木で表せるため解釈がしやすいが,データ数が増えるとその計算量も膨大になる欠点がある.非階層的クラスター分析法は1回で分類されるのでその結合の途中過程は表示されないが,階層的クラスター分析法に比べ計算量が押さえられるため,データマイニングで使われるような大規模データを解析する場ではよく使われている.
階層的,非階層的を問わず,クラスター分析では,まずデータからすべての個体間の組み合わせについて距離を定義する.代表的な個体間の距離にはユークリッド距離(多次元空間の単なる幾何的な距離:d (x ,y ) = (Σ(x i-y i)2)1/2),ユークリッド距離の 2 乗(より離れている個体に大きな重みを与えたい場合の距離:距離d (x ,y ) =Σ(x i-y i)2),市街地(マンハッタン)距離(単純な各次元の差で表された距離:距離 d (x ,y ) =Σ|xi-yi|),チェビシェフの距離(ひとつの次元でも違っているものは違っていると定義したい場合の距離:距離d (x ,y ) = max|x i-y i|),不一致割合(分析に含められる次元に対するデータがカテゴリ型であるときに特に利用:距離d (x ,y ) = (x i≠y iの数)/(変数の数),などがあげられる.
具体的な結合の手順は,最初の段階では,各個体がひとつの個体を含む各クラスターを表わしていると考え,個体間の距離が選択された距離によって定義される.次に,いくつかの個体が一緒に結合されるとクラスター同士(単一の個体もしくは複数の個体からなる)の距離を求めることになるが,このクラスター間距離の決め方によりそれぞれクラスター化法が定義される.以下に代表的なクラスター化法を述べる.
[1] 最近隣法(最短距離法ともいう):ふたつのクラスター間の距離はそれぞれのクラスター内のもっとも近い個体間の距離として定義される.結果として得られるクラスターは一方の方向に長いチェーンになりやすくなる傾向がある.
[2] 最遠隣法(最長距離法ともいう:クラスター間離は,それぞれ異なるクラスターに属する任意の個体間の最大距離として定義される.
[3] 群平均法(UPGMA):ふたつのクラスター間の距離は,異なるクラスターに属するすべての個体の組み合わせを考え,その個体間の距離の平均として定義される.
[4] 重み付き群平均法(加重平均法,WPGMA):計算において各クラスターの大きさを重みとして使用する点を除けば,群平均法と同じ.この方法は,クラスターサイズが非常にアンバランスであると思われる場合に使用される.
[5] 重心法(UPGMC):クラスターの重心は次元によって定義される多次元空間内の平均的点である.この方法では,ふたつのクラスター間の距離を重心間の距離として定義される.この手法はある意味直感的なクラスター間の距離をとるが,クラスター同士が結合する際の距離が前の段階よりも小さい距離をとる距離の逆転と呼ばれる現象が起きることがある.
[6] ウォード法:この方法は,クラスター間の距離を分散分析的アプローチで評価しているため,他の方法とはかなり異なり,各ステップで形成される任意のふたつのクラスターにおけるグループ内平方和を最小にするように距離が定義される.
これらの手法を用いることで,全ての対象を含むひとつのクラスターにするまで結合を繰り返す.この結果を図4のような樹形図と呼ばれる図を用いて結合過程を表示する.縦に樹形図を書いた場合,下(横に書いた場合は左)の方で結合しているほど近い個体であることを意味し,樹形図内で各個体(クラスター)の結合を示す際の棒の高さは,その結合した際の距離を表す.すなわち,低い位置で結合している個体同士は,それより高い位置で結合している個体同士より近い距離にあることを意味するとともに,短い縦棒で結合されているクラスター同士は長い縦棒で結合されているものより近い距離にあることになる.
指定された数のクラスターを構成したい場合には,樹形図をある高さで切断して,クラスターの確定を行う.ある高さで切断する際に,クラスターの数と同じだけの縦棒の数になるように切断し,各縦棒の下にある個体を各クラスター内の個体として分類する.ただし,重心法等を用いた際に生じる距離の逆転が起きた場合には樹形図が複雑になるため,注意が必要である.
図4: 階層的クラスター分析法の樹形図
図4は九州8県における県間居住移動世帯のデータを階層的クラスター化法 (群平均法) を用いた結果である.このデータでは,県間の移動が多い(頻度が高い)と県同士がある意味近い(似ている),移動が少ない(頻度が低い)と遠い(似ていない)と考えているため,地理的に近隣の県が早い時期で結合し,唯一他県と陸で繋がっていない沖縄が最後に結合することが樹形図からも読み取れる.またクラスター数を3と考え,図4のAの部分に線を加えることにより,北九州(福岡,大分,熊本,佐賀,長崎)と南九州(宮崎,鹿児島)及び沖縄に分けられることが一見してわかる.
個体数が多いとき,階層的クラスター法は計算量が膨大になることからあまり適さないため,このような場合にはk -means 法,ISODATA 法等の非階層的手法を用いられる.
(5) その他の多変量解析
これらの他にも共通因子と呼ばれる潜在変数でデータを表すことにより個体間の関係を考える因子分析や対称間の関係を元になるべく持っている情報を失わないように多次元のデータを視覚的に解釈可能になるまで次元縮約する多次元尺度構成法や変数間の相関を解析する正準相関分析等,様々な用途に合わせ提案されている.また,質的データに対応した多変量解析として数量化理論が提案されている.数量化理論はI類からIV類まで提案され,それぞれ量的データにおける重回帰分析,判別分析,主成分分析,因子分析に対応している.
またこれらの手法は,多くの市販の統計ソフトウェア (SPSS や S-PLUS など)に含まれており,その他にも個別の開発者が提供しているオンラインソフトウェアでも利用可能である.
多変量解析
