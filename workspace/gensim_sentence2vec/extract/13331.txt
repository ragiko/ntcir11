
互換用文字は、存在するはずのなかった文字
前回までは互換漢字が追加提案しにくくなっている現状について述べた。規格の上から互換用文字/互換漢字といった文字がどのように考えられているかは、次のUnicode規格書の一文に明らかだ。
Conceptually, compatibility characters are those that would not have been encoded except for compatibility and round-trip convertibility with other standards.(概念上からは、互換用文字とは他の規格との互換性及び往復の保全性の目的以外には、符号化されるはずのなかった文字である。)
『Unicode Standard 5.0』2.3 Compatibility Characters(http://www.unicode.org/versions/Unicode5.0.0/ch02.pdf#G11062)
「本来は存在するはずがなかった文字」、それが互換用文字だ。そうした危うい文字に対し、日本は人名用漢字という政令に根拠を持つ文字の一部を対応させているのが現状だ。この先、常用漢字表に追加される字体が新字体だった場合、文字によっては第2部第1回/第2回で説明したような漢字政策の玉突き現象が発生、JIS X 0213を包摂分離し、これを互換漢字としてUCSに追加提案しなければならなくなる可能性がある。では政令文字の符号化を互換漢字に依存し続けた場合にどのような不都合が生じるのか、今回はそれを考えていきたい。
互換漢字を使用する上で、最も障害になるのは、Unicode正規化(Unicode Normalization。または単にNormalizationとも)を適用すると必ず対応するCJK統合漢字に置き換わってしまうことだ。後述するさまざまな制限も、すべてこのUnicode正規化に起因して発生する。
Unicode正規化については、2004年に本誌連載『文字の海、ビットの舟』で2回にわたって述べたことがある[*1]。発表当時いくつか好意的な反応をいただいたが、読み返すと内容の古さと多くの間違いに気付かされることになった[*2]。そこでなるべく多くの人にわかるよう、あらためて説明し直したいと思うが、限られた紙数ではすべてを書くことは難しい。より正確には原典である『Unicode Standard Annex #15 Unicode Normalization Forms』(UAX#15)や注釈に示したWebページ等を参照していただきたい[*3]。
2種類の「発音記号付きラテン文字」を符号化する方法
1987年の秋、カルフォルニアのシリコンバレーで3人のソフトウェア技術者がミーティングを行った。Xeroxのジョー・ベッカー、リー・コリンズ、そしてApple Computer(当時)のマーク・デイビス。ベッカーとコリンズは1981年に発売されたワークステーション『スター』に搭載され、最も早い多言語文字コードとされている「XCCS」(Xerox Character Code Set)を開発していたし、デイビスは札幌に滞在してMacintoshの日本語版OS『漢字Talk V1.0』を開発した経験があった[*4]。彼等はこのミーティングで世界中の言語を取り扱う単一の文字コードが必要であることに合意、協力してその開発を進めることになった。その年の12月、ベッカーによってこの文字コードは、unique(唯一の)、universal(普遍的な)、uniform(不変の)から「Unicode」と命名される。翌1988年9月まで、最初期のUnicodeは次のような特徴を持つようになっていた[*5]。
16ビット固定長(2進数で16桁、最大文字数6万5536文字)で世界中の文字が符号化可能[*6]
ASCII(ISO 646)と上位互換で、容易に変換可能[*7]
日中韓の漢字を統合したCJK統合漢字[*8]
合成ずみ文字を排除し、合成列により符号化する[*9]
このうち最後の「合成ずみ文字」と「合成列」が今回のテーマとなる技術だ。これは主にヨーロッパ諸語に見られる発音記号付きラテン文字を符号化するための方式のことなのだが、詳しいことはすぐ後で説明するとして、今はもう少し文字コードの歴史をたどってみよう。
誕生間もないUnicodeが選んだ「妥協」
この頃、文字コード標準化の世界は昏迷していた。1984年に国際的な文字コード審議機関であるISO/TC97/SC2[*10]において、漢字を含む各国語を統一して符号化できる16ビット文字コードの枠組みを開発することが合意され、作業部会として今に続くWG2が発足する。その後1987年3月に最初の原案を可決したのはよいが、3つの国内規格により漢字2万1000字以上をすでに符号化していた中国が、16ビットの範囲ではそれらすべてが収容できないことによりこの原案に強く反対していた。そこでSC2は枠組みを32ビットに拡張することに方針変更、その原案を作成し直していた[*11]。これが後にUCSと呼ばれるものにつながる。
このような状況の中でUnicodeは産声を上げることになった。多国籍IT企業とは言え、それまで国際標準化活動とは無縁なソフトウェア技術者の私案だったUnicodeは、この後IBMやMicrosoftなど急速にメンバーを拡大、Unicodeコンソーシアムとして団体の形を整えていく。1991年1月にはUnicode社としてカルフォルニア州に法人登記される(社長はマーク・デイビス)。
やがてアメリカの代表機関L2の支持を獲得するまでになり[*12]、彼等が主張するCJK統合漢字と同じようなアイデア「Han Character Collection」を打ち出した中国と連繋しながら[*13]、1991年8月にジュネーブで開催されたWG2において、ついにUCSの一部をUnicodeが乗っ取るような形での一本化が承認される(詳細は第2部第3回の注釈1を参照)。そして1993年にISO/IEC 10646-1として制定されたのである。しかし、こうして激しい競争に勝ち残るまでには(そして勝ち残った後も)、Unicodeは当初の構想と矛盾する無数の妥協を続けなければならなかった。
そうした妥協の中でも最初の、そして後々まで大きな影響を残したものが、1990年3月に合成ずみ文字も収録するよう方針を変更したことだ[*14]。最初期のUnicodeが合成列方式を採用していたことはすでに述べた。これが彼等なりの信念から採用されたことは明らかだ。ところがヨーロッパ各国の言語を符号化する有力な規格、ラテン1として知られるISO 8859-1(1987年制定)は合成ずみ文字方式を採用していた(図1)。この規格はイギリス英語、ドイツ語、フランス語など25の言語を符号化可能にしており、すでに実装実績があった。この状況の中で、UicodeはISO 8859-1と互換性を確保しなければ生き残ることは難しいと思われたのである。
図1 ISO/IEC 8859-1の文字表。右半分を見ると発音記号付きの文字は合成ずみの形で収録されていることがわかる。この部分がUnicodeとの互換性でネックになった(『ISO/IEC 8859-1:1998』ANSI、1999年、P.5)
ここで合成ずみ文字(precomposed character)と合成列(composite sequence)について説明しよう。前者はあらかじめ発音記号と合成した形を規格に収録して使う方法だ。一方、後者は専用の結合文字(combining character)を組み合わせて表現する(図2)。
図2 発音記号付きのラテン文字を表現する2つの方法/合成ずみ文字と合成列
合成ずみ文字は何といっても実装が容易というメリットがある。1つの符号位置を1文字として扱えばよいから、見た目から簡単に文字列のバイト数(記憶容量)を割り出すことができる。いささか退屈だが手堅く現実的な方式だ。反面で必要な文字の形だけ符号位置を用意しなければならないという無視できないデメリットがある。
一方、合成列は2つ以上の符号を1つの文字として扱うことになり実装が難しい。文字の形に合わせて微妙に発音記号の位置を調整しないといけないし、複数の符号位置が並んでいるもかかわらず、単一の文字と同様にカーソルの移動、削除、行末処理などをしなければならない。もちろん見た目からバイト数は割り出せない。反面で合成ずみ文字より収録文字数が抑えられ、いったん実装してしまえば既収録の符号と組み合わせて使える文字を増やせたりと、融通がききやすいメリットがある。いかにも気鋭のエンジニアが好みそうなチャレンジングな方式なのである。
大事なのはどちらの方式にもメリットとデメリットがあることだ。最近のBlu-ray Disc規格とHD DVD規格の対立を思い出させるような話だが、それと違うのはUCSとの一本化に成功する前のUnicodeにとって、すでに実装実績のあるISO 8859-1に対し往復の保全性を保証すれば、ヨーロッパ各国にも支持を広げることが期待できたということだ[*15]。こうしてUnicodeには当初からの合成列用の結合文字に加え、合成ずみ文字を収録することにしたのである。
ではこの変更が、後にどのような影響を与えることになったのか、それは次回考えることにしよう。

