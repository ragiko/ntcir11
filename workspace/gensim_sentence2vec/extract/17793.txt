 Class N-gramモデルとHMM、もしくはどこが隠れてるねん
Class N-gramモデルというのは、各単語はあるクラスに所属するものとし、クラスの連なりによって次の単語の確率分布を予測する言語モデルである。各単語がどのクラスに所属するかは、コーパスの対数尤度を最大化するように決定される。
このモデルはHMMと非常に似ている。HMMとの違いは、HMMでは単語があるクラスへと所属する確率が存在するのに対し、Class N-gramモデルでは単語はあるクラスへと確実に所属するところだ。つまり、クラスAに所属する単語は、クラスBやCには絶対に所属しない。(HMMだと、クラスAへの所属確率0.8, クラスBへの所属確率0.1、みたいになる。)比べてみるとつまり、HMMの方がちょっと柔らかい印象を受ける。
今、ちょっと嘘を書いた。もしかするとClass N-gramモデルという言葉で、各単語のクラスへの所属確率をイメージする人もいるかもしれない。ただ、この日記は論文ではないので、あんまり詳しく調べずに書いてしまうことにする。
Class N-gramモデルの対数尤度を最大化するアルゴリズムは、"Algorithms For Bigram And Trigram Word Clustering" (S. Martin, J. Liermann, H. Ney, 1998) が詳しい。もしくは、それを高速化した"ClassModelを用いた単語分類の拡張及び高速化" (岡野原, 2004) もある。どちらにせよ、試行錯誤をしつつ対数尤度を高めていくというやり方になる。そして最適解にたどり着く保証はなく、実際にも最適な解にはたどり着かない場合の方が多い。
一方、HMMでは、最適なパラメータは閉じた形で求めることが出来る。最尤推定の場合は、という条件はつくけれど。さて、閉じた形で最適解が求められるHMMと、局所解を求めるためにすら収束計算が必要なClass N-gramモデル。この違いはどこからくるのか?
結論を先にかいてしまうと、タグつきコーパスの有無が原因である。HMMを形態素解析やPOS taggingなどに使う場合、品詞を状態とみなして学習用のコーパスを作るのだが、タグつきコーパスがあれば、HMMの最尤推定は簡単である。(余談だが、CRFはタグつきコーパスがあってもパラメータの推定に数十時間とかかかる。)タグつきコーパスがない場合、HMMであっても学習にはEMアルゴリズムや変分ベイズを使った収束計算になり、これらは最適解にたどり着く保証はない。つまり、タグつきコーパスのあるなしが、閉じた形で解を求められるかどうかという差を産んでいるという訳だ。
そして、Class N-gramモデルでは、タグつきコーパスを用意することは出来ない。形態素解析に用いられるHMMは品詞を状態と同一視することが多く、それゆえタグつきコーパスを作ることが出来る。しかし、Class N-gramモデルにおけるクラスには文法上の明確な定義はなく、従って人が手作業で正解を決めることは出来ない。そもそも、最適クラス数をAICやMDLで求める、とか言ってる時点で人手で正解が作れないことは明らかである。
音声処理の人に言わせると、形態素解析のHMMはHMMじゃないそうだ。どこがHiddenやねん、コーパスに状態が書いてあるやないか、というわけだ。まぁ、そう突っ込まれるとまったくその通りとしか言いようがないのだが、形態素解析の場合はタグ(というか、品詞)が出したいので、タグつきコーパスを使うのは仕方がないよねー、という言い訳は出来ると思う。
もちろん、世の中にはいろんな研究があるわけで、形態素解析でも、タグなしコーパスから学習させたHMMを使っている場合もある。例えば、風間らの"教師なし隠れマルコフモデルを利用した最大エントロピータグ付けモデル"とか。これはMEMMの素性の一つとしてHMMの状態を使っている。
実はここまでは前振りだったのだが、前振りが長くなりすぎて本文を書く気力が残ってないので、本文はまた明日。
ツイートする
Permalink | コメント(7) | トラックバック(0) | 17:20 
Class N-gramモデルとHMM、もしくはどこが隠れてるねん - 射撃しつつ前転
