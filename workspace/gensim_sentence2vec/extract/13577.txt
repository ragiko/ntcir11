 音声認識は、50年以上前に研究が始まった。今では、音声を文字に変換するソフトや対話型ロボット、音声で検索できる携帯電話用のアプリなど、用途が広がっている。
しかし、どれくらいの人が実際に使っているだろうか。文字変換ソフトの場合、文章を入力するためにマイクの前で長々としゃべり続けると疲れるし、特に日本では、オフィスの多くが相部屋なので、全員がしゃべり出したら仕事にならない。
携帯電話やスマートフォンにはキーボードがないので、普及とともに音声認識も広く使われるようになると予想していたが、多くの人は頑張って指で入力している。これは技術的な問題ではなく、人前で機械に向かって話しかけるのが恥ずかしいという、社会的な面が大きいと考えられる。
音声で入力する際には、きちんとした文法で明瞭に発声するのが大前提だ。そうしないとうまく働いてくれない。また、基本的には一問一答型で、こちらから何か言わないと反応もない。
目標はツアーガイド
我々は、もっと人間らしい対話ができるシステムが作れないかと考えて研究を進めている。目標は、ホテルのコンシェルジュやツアーのガイド。現状のシステムのように一問一答ではなく、対話をしながら相手の要望や好みを探り、お勧めの食事や観光地を紹介してくれるものだ。
ガイドさんたちは客が質問しなくても、状況に応じて興味深い話をしてくれる。これも重要なポイントと考えている。大抵の人は、恥ずかしさもあってシステムに向かって話すのをためらうため、話が続かない。
そうした観点から、システム側から気を利かして話しかけてもいいのではないかと考え、5年前に、京都の観光地を音声で案内するロボットを作った。
例えば、金閣寺について問われたら、インターネット上の情報をもとに答えた後、「誰が建てたか知っていますか」と話をつなぐ。そうすると、話し相手も興味を持って対話を継続してくれるようになった。
ただし、本格的に対話できるようなシステムの実現には、まだ多くの課題がある。鉄腕アトムやドラえもんのような、しゃべるロボットはいつできるの、とよく聞かれるが、答えるのは難しい。実現するのは30年後なのか100年後なのか、全く分からない。
iPS細胞シンポのネット配信にも字幕
人間が日常的に話している言葉を音声認識する研究も進めている。だが、この場合は機械との対話とは別の難しさがある。
機械と対話する時は、言葉を正確に認識させようと、できるだけ明瞭に話しかける。ところが、人間同士の会話の場合はそうではない。語彙(ごい)も文体も発音もまちまちで、文法的ではない言い回しやくだけた表現、言いよどみもある。
今のところ、人間同士の自然な会話をすべて正確に認識し、自動的に記録や字幕を作るような万能の音声認識システムはない。対象ごとにシステムを構築する必要がある。
中でも、研究開発が進んでいるのは、放送番組に字幕を付ける技術だ。アナウンサーは発声が訓練されているので、音声を正しく認識できる割合は95%にも達する。
衆議院の会議録作成も今世紀に入り、速記に変わる手段として、世界で初めて音声認識が導入された。システムは我々が開発したものだが、国会特有の語彙や言い回しに対応するため、過去2000時間分の審議音声と、12年間分の会議録テキストといった大規模なデータを用いて、音声認識システムを学習させた。まだ完璧なものではなく、音声を認識させた後、速記者による編集や校閲の作業を経て、会議録が完成する。現在の音声認識率は約90%となっている。
最近では、大学の講義や講演もネット配信されることが多いが、コストがかかるため、字幕はほとんど付けられていない。音声認識で字幕を付けようとしても、専門用語が多く、内容や話し方も幅広いので、議会などに比べて音声認識率が悪い。せいぜい60～80%だ。このため、我々は講義や講演の動画に字幕を付ける研究にも取り組んでいる。
京都大では、1000本以上の講義や講演を配信しているが、視聴回数が特に多いiPS細胞(人工多能性幹細胞)に関するシンポジウムでは、我々の技術を用いてすべてに字幕を付けた。「幹細胞」「分化多能性」といった専門用語が多く出てくるため、音声認識システムをiPS細胞の講演用に適応させた結果、85%の認識率を達成できるようになった。
変化する言葉の重み
口述記録は、昔はその場で紙に筆記していた。その後、録音機器が開発され、書き起こして記録するようになった。現在は、音声もデジタルデータで保存する時代だ。音声の録音も、昔は政治家ら特別な人に限られていたが、今ではこの講演も含めて日常的に録音・録画されるようになった。また、コールセンターや音声検索サービスなどの通話も記録されている。
発言がすぐに消えていた昔と比べて、現代は我々が発する言葉の一つ一つの重みが変わっていると言えるのではないだろうか。
かわはら・たつや1987年京都大工学部情報工学科卒。同大学助教授などを経て、2003年から現職。米ベル研究所客員研究員、国立国語研究所非常勤研究員なども務めた。12年、音声言語処理の研究で科学技術分野の文部科学大臣表彰。
Q&A
Q:日本語は同音異義語が多いため、英語などに比べて音声認識の精度が低いのではないか。
A:確かに日本語は英語に比べて音素の数が少ないため、同音異義語が多い。ただし、音声認識の精度という観点からは、少なくとも主要言語に関しては大きな違いはない。精度に最も影響するのは、システムに学習させるデータがどれぐらい整備されているかという点。主要言語では既に膨大なデータベースが作られており、どれも80～90%の精度がある。
Q:衆議院の会議録を作成する音声認識システムでは、首相が交代した場合、システムをどう改良するのか。
A:このシステムは常に運用しながら、審議データが一定量蓄積されたら更新するようになっている。その間の首相の交代も結果として反映される。ただし、総選挙で議員が半分近く入れ替わった政権交代時には、さすがに支障が出るかもしれないと思った。過去の国会審議のデータベースにない多くの新人議員の声を認識しにくいのではないか、と。しかし、実際には思ったほど問題になっていない。
Q:国会審議でヤジが入った時はどう処理するのか。
A:聞き取りにくくなるほどの大きな声のヤジが出ると、認識率はかなり落ちる。雑音として認識され、文字として変換されない部分も出てくる。ただし、かなりうるさい場合でも、7割ぐらいは何をしゃべっているのか認識できる。
「話がわかるコンピュータ—音声認識と対話システムの最前線—」 学術情報メディアセンター 河原達也教授 : 地域 : 読売新聞(YOMIURI ONLINE)
