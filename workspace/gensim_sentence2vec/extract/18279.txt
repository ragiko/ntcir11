世界中の人々の間に大きく立ちはだかる言葉の壁は、この10年の技術革新で低くなりつつあります。その技術の名は「機械翻訳」で、人間の言葉を翻訳し、異なる言語で綴られた言葉でも理解できるようにしてくれます。今回の記事は機械翻訳の中で特に音声を入力とする「音声翻訳」についての話です。
音声翻訳を正確に行うために、3つの技術が必要になります。話された内容を正確に読み取り、コンピュータが理解できるテキストに変換してくれる「音声認識」、その内容を異なる言語へ翻訳する「機械翻訳」、そしてテキストを再び音声へと変換する「音声合成」です。この全ての技術は計算機が開発されてすぐにコンピュータの有用な応用先として取り上げられてきましたが、人間の言葉は複雑で、なかなか現実的な精度に及びませんでした。しかし、インターネットの普及によるデータの大規模化や、統計的な処理法の発展により、この10年で精度が劇的に改善され、ようやくある程度使えるようになってきました。
NAISTの知能コミュニケーション研究室では、音声認識、機械翻訳、音声合成の基礎技術開発に取り組んでいます。例えば、音声認識では単語の発音を正確に推定して発音辞書の正確性を図る研究、機械翻訳では文の構造を考慮して日英翻訳のような語順の異なる言語の間の翻訳精度向上を図る研究、音声合成では様々な声質を生成する柔軟性を保ちながら合成音声の質向上を図る研究などが行われています。しかし、今回の記事では、精度だけでなく、音声翻訳を違う観点から見つめた研究を2つ紹介します。
同時性の高い音声翻訳
以下の動画は自動音声翻訳の一例です。
言葉は正確に伝えられていますが、話し始めてから実際の翻訳結果が出てくるまでに多くの時間がかかることも分かります。これを実際の人間の通訳者の様子を写した以下の動画と比べてみましょう。
ここで、顕著な違いとして見受けられるのは、実際の人間の通訳者は発話の終わりを待たずにすぐに通訳を開始していることです。しかし、これをするために高度な技術が必要となります。特に、日本語と英語のような語順の大きく異なる言語の間の翻訳なら、翻訳を開始するのが早すぎると、正確な翻訳を行うための情報を得ないうちに翻訳の精度が低下する恐れがあります。逆に、開始が遅すぎると聞いている人に取って余分な待ち時間が発生します。
そこで、我々が注目したのは、いかにこの訳出するタイミングを判定するかです。実際の翻訳データや通訳データから、どの単語が現れたら翻訳が開始できるか、どの単語が現れたら次の入力を待った方が高い精度が実現できるかを判断する仕組みを作成しました。そして、その結果を実際にシステム上に実装し、以下のように適切なタイミングを判断して翻訳を進めることのできる同時音声翻訳システムを構築しました。以下は提案してきたシステムのデモです。
仕組みの詳細について、日本音響学会の論文や音声研究で最大の国際会議InterSpeechの論文などで発表しています。また、この研究の続きで、実際の通訳者に習って翻訳システムを作成する研究も行っており、更に高性能かつ素早い訳出を極めていこうと思っています。
声質の翻訳
海外から日本へと輸入された映画を考えてみよう。その映画の内容を日本語へ翻訳する方法として、「字幕」と「吹き替え」があります。どちらを好むかは個人差がありますが、今回の話は吹き替えを考えます。吹き替えの映画を聞いた際、声優の声は映画のイメージに合わせて選ばれ、更に声優は場面に合わせて感情のこもった声で話します。しかし、吹き替えの声優の代わりに、俳優の声を従来の音声翻訳システムにかけてみたとしましょう。仮に100%の翻訳精度が実現できたとしても、出てくるのは元の俳優とは程遠い、無味乾燥な声質になります。
そこで、我々が研究で着目したのは、声の強調、感情、イントネーションなどの非言語情報を翻訳することです。手法として採用したのは、音声認識の段階で、発生された言葉自体とともに、声のさまざまな特徴量を認識し、線形回帰やニューラルネットという機械学習の技術を用いて相手言語に翻訳することです。研究はまだ初期段階ですが、以下の例のように、入力された声の強調を音声翻訳の出力に反映させるのに成功しています。
仕組みの詳細について、日本音響学会の論文や音声翻訳に関する国際会議IWSLTの論文などで発表しています。これからは、声の強調だけでなく、イントネーションや韻律、個人性まで反映して行こうと思っています。
言葉の壁を取り払う自動音声翻訳 | NAIST Edge
