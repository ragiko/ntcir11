      
世の中には、既に分かっている過去のデータがあります。このデータを利用しない手はありません。過去のデータを利用すれば、もし分からない未来のデータが出現した場合、過去のパターンから有効な知識として活用させることができます。 今回、ご紹介するのはそんな過去を知れば未来が見えてくる手法です。概して『パターン認識』と呼ばれる手法とその類です。      
「パターン認識」、難しい言葉に聞こえるかもしれませんが、我々は常にパターン認識をしております。      
例えば、ある人の顔を見たときに瞬時に記憶の中から誰なのか識別してますし、初めて見る場合でも似たような人物を探しどんな人間なのか当てはめたりすることもできます。 
楽しいときはどんな表情をするか、苦しいときはどんな表情をするかという「パターンクラス」を私たちは持っています。初めて会う人の表情でさえ、感情をよみとる能力を持ち合わせています。それがパターン認識です。
他にもぐちゃぐちゃに書かれた字に出くわした場合、まず自分の記憶から検索をかけて当てはめてみて、それでも分からなければ前後の文字からの意味がつながるかで検証したりして識別しています。      
未知のデータは、既知のデータに当てはめて知識発見をする。我々は常にパターン認識の世界で生きており、人間はパターン認識をするうえで最高傑作なのです。(人間とコンピュータの違いは別章で記述)      
さらにはコンピュータで行うパターン認識(=予測)を考えていきましょう。 
それでは、さっそくパターン認識手法のイメージから入りましょう。      
ここは政治家たちのパーティ会場です。           
政治家(政党)は、下図のような配置でいます。       
あれ?中央に1人、政党の分からない人がいます。与党(自民・公明)か野党(民主・自由・社会)でいいから、この人の政党が知りたいです。       
あなたなら、どういった根拠で推測しますか?       
しばらく考えたら、下のボタンを押してください。各々の手法の概念を説明します。       
※イメージで、もちろん正確ではありません。InternetExplorer5以上でご覧下さい。       
さて、各手法のイメージはつかめたでしょうか?        
ずいぶん簡単にできそうな感じでイメージを作成してみたのですが、実際は複雑なデータの中からどうすれば認識率の高いモデルが作成できるか色々大変なのです。       
詳しい計算方法を説明しても、ご理解いただけそうにないので割愛させていただき概要だけ説明します。        
代表サンプルを定める方法       
k近傍法(k-NNルール)・最短距離法
k近傍法(k - Nearest Neighbor)は目的のわからないデータから、全既知データまでの距離を測ります。         
そして距離の短い順k個で多数決し判断します。       
パターンが2種類のときは、kの値を奇数にします。そうすれば同点とかいうことがなくなりますからね。
最短距離法とは1-NNルールと呼ばれるほどで、k近傍法でk=1のときと全く理論は同じです。
距離の計算方法は、距離関数(ユークリッド距離が一般的)や重み付けなどで変わってきます。
例えば著者は最短距離では信頼性が足りないと思うし、単純に多数決してもサンプルの類似度が異なりますので、距離を考慮した方法で識別してみたりもしますが。
まぁイメージほど単純ではありませんが、利用しやすいパターン認識法です。
データ量が大きくなってもそれなりに対応できる手法ではありますが、データが更新するたびに完全に最初からはじめないといけないので更新の激しいデータには向きません。
また、データの中に目的のパターンを識別するのに関係ないファクター(政治家パーティ図でいう年齢データとか出身地とか)が多数あれば、既知データが交じり合い、誤って判断してしますので元データのクレンジング(補完・正規化・特徴選択)をしっかりしていないと有効とはなりません。 また、既知データのパターンサンプル数が偏っていれば影響も大きいのです。
分割線によって区切る方法
判別分析・ベイズの決定境界・マハラノビスの汎距離
これらの手法は、何かの基準線(基準面)を作って判別する手法です。
判別式は、グループの違いが最も顕著に表現できるように作成されています。この判別式がグループの違いを明らかにする構造であります。
個体で判別するのが、k近傍法であるとすれば、判別分析やベイズの決定境界などは構造がどうなっているのかを明らかにしていくことが多いのです(こういう場合は判別分析の中の正準判別分析という手法を使う)。
構造を見つける方法として、判別分析やベイズは、誤って判断してしまう確率を最小とするように基準線を定める方法でして、マハラノビスは、データの全体構成をみて基準線を定める(重心を求める感覚)方法です。
計算は直線で求めるより、柔軟な対応ができる曲線で求める方が大変です。k近傍法に比べて計算は大変なのですが、一度基準線を作ると、データが大幅に変更されない限り未知のデータを当てはめるだけで済むので高速です。
また、k近傍法と同様に元データのクレンジングをしっかりしないと、誤って判断してしまいます。またデータの分布構造が予めある程度分かっていないと適用するのが難しいと思います。
Yes−No形式の決定木によって識別する方法
CART・ID3・C4.5・エキスパートシステム
決定木の見方ですが、一つのファクターごとにYes-No形式で最終結果を判定する方法です。学習過程が決定木(デシジョンツリー)として視覚化されます。
Yes-Noのように分岐が2つであるものは「2進決定木」と呼ばれます。決定木を作成する方針によって、分岐が3つある決定木や分岐数が色々ある混合木というものもありますが、基本的に「2分岐」が分かっていれば全部応用できます。
決定木の利点は、なんといっても「学習過程(モデル)が知識として理解しやすい」ということです。
図の決定木を見ればすぐ分かると思いますが、どのようなプロセスを辿ってどんな知識を得たのかが見えてきます。
ニューラルネットワーク(後述)は途中のプロセスがブラックボックスとなっているため、何が起きているのか理解できません。たとえ結果のみが必要な場合でもニューラルネットに「おまえはともかく与党だ」と言われると従うしかないのですが、決定木の場合は「真中の前の方にいるから与党なんだ」という明確な理由をつけて判断してくれます。
たとえば決定木のオプションとして、最終判断に下図のように過去データの統計グラフをつけてみます(下図の統計値はイメージです。)。そうすれば当てはまるデータが過去どれくらいの頻度で生じて、どのくらいの精度があるか知ることもできます。
なぜ、決定木に統計データを付加したかその理由を述べます。
決定木やニューラルネットワークによって最終結果が得られますが、それがどのくらい精度があるか必要になってくるからです。特に間違って判断してはならない時には必要です。
上図イメージのように政治家のパーティで「与党」か「野党」か見分けるのを間違っても大した問題ではありません。しかし、地雷探知機のデータにおいて「地雷」と「単なる金属」を識別する時に、「金属を地雷と誤識別」しても労力が余計にかかる程度ですみますが、「地雷を金属と誤識別」してしまえば命に関わってきます。
他にも「風邪」と「ガン」を識別するケースにも同じことが言えますし、「火事」を識別しスクリンプラーを作動させるかの判断はどちらにも誤ってもいけないケースです。このように各場面で誤って識別しないように統計データ等を加味しながら判断すれば、確実に判断できるケースとそうでないケースが見え、より意味のある知識が得られるでしょう。
最終結果の認識率だけみれば、ニューラルネットワークが上回ることが多いと思います。しかし、分岐を辿って判断した場合は時に決定木の方が精度が高いとも考えられます。(グラフをつけたところからは、著者のとっさの思いつきなので検証もしてませんし、間違いだらけかもしれませんが)
さて、決定木の見方は分かったと思いますが、次は決定木の作成方法を説明します。ID3とかエキスパートシステムとかいうのがその決定木の作成手法です。
ID3・C4.5・C5.0は同じようなものと思ってかまいません。ID3C4.5C5.0とバージョンアップしたものですから。これらはエントロピー(情報量)を求めて、効果のある要素の効果のあるポイントで分岐することを繰り返しています。
エキスパートシステムは、人間の専門家が経験や高度知識に基づいて決定木を作成するものです。
同じ決定木を作る方法なんですが、「コンピュータによって計算上最も効果のあるポイントで分岐する質問を作るシステム」と「人間の専門家が主観で分岐する質問を作るシステム」では対極的です。どっちがいいかというのはなんともいえないのですが、データが膨大であるマイニングである以上、前者寄りになるのは明らかでしょう。
決定木の分岐部分の求め方は、C4.5やエキスパートシステム、他にもCARTやCHAIDというものがありますが、重要なことはどこで分岐を止めるのかです。100%の答えを出したいからといって下図のように最後まで細分化して決定木を作成しても意味がないのです。
何故かといいますと、このように細分化して過去のデータで100%の答えを出しても、未来のデータに有効ではないからです。細分化も行き過ぎるとデータの本質をつかめなくなり、一つ一つの分岐を構成する要素が少なくなるために信頼性も落ちるだけなのです。どこまで分岐をするかは、最高分岐数を決めたり構成サンプル数で決めたりで様々ですが、ここは解析者の腕の見せ所でしょう。
最後に決定木を適用した場合について考えてみます。
決定木の計算方法は、比較的簡単ですし、モデル作成もしやすいと思います。また決定木を一度作ったら後は当てはめていくだけですから高速に処理できます。
なので、問題は決定木の精度という点になりますが、決定木は一つのファクターごとに判断するので、パリティデータのように一つの軸でみると完全に重なりあっているようなデータには全く対応できません。そうでなくても計算方法が単純すぎる分、精度が高いとは思えませんし(ID3・C4.5)、人間の主観で決めた基準に安定性と信頼性があるとも思えません(エキスパートシステム)。
で、また文章を書いててとっさに思ったことなんですが、決定木の分岐条件を1変数で判断するのではなく、「A*B/C+D          
< 2 が Yes or No」のように複数変数を一つの変数に集約して分岐させれば、1変数ごとの分岐問題が解決します。また、決定境界のように1つの境界線にとらわれることなく、複数の柔軟な境界線で区別すれば精度も高くなり、いいことだらけではないでしょうか? といってもどうやって導入するかまで考えていないのですが。
ちょっと話がそれてしまいましたが、決定木の精度は高くないので、適用にあたっては明確な目的をもってシミューレーションし、よい結果が得られなければ通用しないと思います。ただ、決定木はその学習過程からも十分な知識を得ることが出来ますので、とりあえずチャレンジすることも重要です。
ニューラルネットワーク
図ではニューラルネットワークの専門家ではない人(例えば著者)から見たニューラルネットワークのイメージを描いてしまいました。
単にニューラルネットワークの予測モデルといっても色々な求め方があります。大分類すると2つでしょうか。
332パターン認識
