
平均と偏差、分散、相関の概念
調査とか測定を行って得たデータの集まりがあったとき、その集団の構造を端的に表現してしている代表的な言葉が平均値と偏差値です。
偏差値の出し方はともかくとして、平均値の出し方ぐらいはご存じだと思いますが、その概念的なものはどうでしょう。また、偏差値もよく聞く言葉ですが、何かモヤモヤした感じを抱いていませんか?これらはデータの集まりである集団構造を一言で表せる言葉ですので、統計にはよく用いられます。
ここでは、平均値・偏差値・分散及び相関などの概念について説明します。
【平均】
平均値を求めるには、データを全て加え総個数で割る事で求めていますが、このやり方は算術平均と呼ばれています。平均にはこの他に幾何平均、調和平均がありますが、これらは特殊なもので、通常特に断りが無ければ平均と言えば算術平均の事を指しています。
幾何平均は比率の平均を出したいとき、対数正規分布の中心を求めるとき、人口の増加率の平均を求めるとき、細菌増殖時の平均算出などに用いられます。
実際の手計算では各変数を対数変換し、算術平均をした後真数に変換するといった厄介な手続きが必要です。
調和平均は逆数に意味のある変数の平均値を出したいとき、例えば所要時間から平均速度を求める場合などに用いられます。この他、移動平均、加重平均などありますが、これらは算術平均の仲間です。
数学的に、算術平均≧幾何平均≧調和平均の関係が成立していますので、通常算術平均で事足りる訳です。
平均値とはよく分布の中心あるいは重心を表すと言われますが、図1に示すように分布の釣り合った位置をさしています。(重心と言われるのはこの為です)
図1.平均値
このことから平均値は全体集団の存在する位置を代表させることが出来るので位置の代表値と言われます。
平均値を表す記号は、集団xの平均値を表す時はとχの上に‾バーを付けて(バーエックスと読む)表します。‾が付いていると算術平均を表しています。
ちなみに、幾何平均はG、調和平均はH でバーなしのアルファベット大文字斜体で表されます。
【偏差と分散】
図2.二つの分布
平均値は集団の構造を表すのに便利な指標ですが、例えば、図2のような分布を示すA,Bの2つの集団があるとします。
その分布は一方は広く、他方は狭い形をしています。互いに同じ平均値を持っていますが、分布状態がまるで違います。
そこで、平均値とは別に集団の分布のバラツキを表すものがあればそれもまた集団の構造を表す指標となります。
〜〜〜偏差の概念〜〜〜
2つの分布を比較するとAの平たい分布ではX1、X2やX9、x10となど平均値mから遠く離れた値がBの尖った分布よりも多い事が分かります。そこで考えつくのが(X1−m)、(X2−m)などを考えてみる事でしょう。
(個々のデータから平均値を引いたもの(X−m)のことを偏差と呼びます。)
この偏差を全てのXについて計算し、その総和を取ればバラツキの尺度が得られそうな気がします。しかし、実際にやってみると偏差のプラス値とマイナス値が打ち消しあって、その総和は常にゼロになってしまいます。実はこれが平均値の持つ性質なのです。これではバラツキの尺度にならないので、偏差の符号を消す(マイナス値にならないようにする)事を考えます。数学的に符号を消すには絶対値を取る方法と2乗する方法があります。が絶対値はその適応条件等数学的な取り扱いが面倒であるので、2乗する事を考えます。(符号を消し去るだけなので4乗とか6乗でも良いのですが、差が大きいものほど過大に評価されるので一般に2乗が都合がよいでしょう。また、これとは別に2乗する事によって統計上のいろいろな公式と関連させられるので都合が良いのです。)
これによって得られる偏差の2乗の総和は、分布のバラツキ具合を表すことができますが、この総和はXの個数に影響されるので、その影響を消すためにXの総個数nで割ることにします。これは偏差の平均値を求めることに相当しています。このようにして得られたバラツキの尺度を分散と呼び、特に母集団の分散との意味で、母分散と呼びσ2(シグマ2乗)と表します。
図3.σの意味
この母分散σ2が母集団のバラツキを表す尺度であるからその平方根σもまた母集団のバラツキの尺度を表します。このσを母標準偏差と呼びます。この意味づけを変曲点として解説している教科書も見かけますので、それに少し触れます。
数学的にと言うか、幾何学的に2乗する事は面積を表し、その平方根は線分を意味しています。σはその計算式において、平均変化率と解釈でき、これは微分係数としての意味を持っており、平均変化率が変化する点(これが変曲点)から分布の中心(平均値)までの距離を表しています。  
このことが、σの大きさによって分布の形状が分かる理論的裏付けになっているのであり、例えばある1つのものを繰り返し測定した場合、その測定値の分布は平たい分布(σが大きい)より尖った分布(σが小さい)の方が精度が良いとう事が言えます。また、幾つものデータを寄せ集めた場合に、σが大きいと裾広がりな多様な分布であり、σが小さいと平均値に集中している分布と言うことができます。
慣れてきますと、平均値(mean)と標準偏差(SD:standard
deviation )を見るだけで、グラフの形状が想像できるようになります。
これまで、分散や標準偏差の前に母をを付けて母分散とか、母標準偏差などと呼んでいますがこれには一応訳ありです。
一般に集められたデータはその母集団の平均値mやσが分からないのが普通です。そこで、実際の解析ではmやσそのものを使って統計解析をするよりも、mやσの推定値を使って解析を行う事が多いのです。したがって、母集団のmやσを得られたデータから推定することが必要になってきます。
ここで、あくまでも念頭に置きたい事ですが、手元にあるデータから推定した値は厳密に言えば母集団の値そのものではないと言うことです。
数学的にいろいろな手法で推定値を求めていますが、このことを頭の片隅にでも入れておいて下さい。
母平均mの最も良い推定値はきわめて慎重な理論的な検討から母平均mを求める式と同じ式で表すことが出来ます。式で書くと
母平均mの推定値=試料平均値  =1/n(試料の値xの総和)
で表せます。
ところが、母分散σ2の推定値の場合は少し様子が違います。
偏差の概念で書いた文章を式で表すと
σ2=   式1
となります。ここで、普通は母平均mが分からないので、mの代わりにその推定値を使わなければならない制約が出てきます。
その為σ2の推定値としてS2を定義し、これを使う事になります。
S2=  式2
ここで、nの代わりにn−1で割っている点に注意して下さい。
このnとかn−1の事を自由度と呼びますが、詳しい説明は長くなるので省略します。
このS2のことを不偏分散と呼び、その平方根Sの事を標準偏差と呼びます。
これら母集団と試料集団の違いは明確に意識して下さい。
例えばエクセルなどの表計算ソフトでも、関数式では
母標準偏差(σ2)=STDEVP、 試料標準偏差(S2)=STDEV
と区別されていますし、関数電卓類でも標準偏差を求めるボタンにnとn-1と区別しているものが多いです。
〜〜〜学術記号について〜〜〜
母集団の平均やSDから、試料平均やSDに話が移るとき、ギリシャ文字(σなど)からローマ文字(mとかs)に変わっている事に注意して下さい。
当講座では出来るだけ数式の類を分かりやすく説明していくつもりですが、数学書や専門書などを参考にする事も考えて、学術記号はかなり意識して明確に記述していくようにしています。ギリシャ文字やローマ文字に限らず、アラビア文字・数字、大文字・小文字、斜体、などそれぞれ全てなにがしかの定義がされているので、出来る限りそれに沿うように記載していきます。
【相関】
偏差や分散などは1つの変数の状態でその集団の構造を表すものですが、2つの変数の動きを同時に着目する場合が相関と回帰の考え方です。
(回帰については第2部で予定してますので今回は触れていません。)
よく統計の話の中にコリレーションとか言う言葉が出ますけど、相関は英語でcorrelationと書き、話し手は相関のことを指しています。
2つのデータ値などを比較して関係がありそうだとかを考える時に、相関関係を見ていくのですが、相関が見られるからと言って必ずしも因果関係があるわけではありません。ここではこのことを中心に話します。
2つのデータをX軸とY軸に対応させたグラフを考えます。このようなグラフを散布図あるいは相関図と呼びます。(図4)データ一つ一つのバラツキが図4の左2つような関係が見られる時2つのデータは何らかの関係があると予想され、これを相関性が伺えるとか言います。
図4.散布図 相関図
図左のように、一方が大きくなれば他方も大きくなる場合は2つの関係は正の相関があると言い、正の相関あるいは順相関と呼びます。
図の中央のように、一方が大きくなれば他方は小さくなる場合を負の相関があると言い、負の相関あるいは逆相関と呼びます。
図右のように2つのデータ値の大きさに無関係で明確な関係が見られないときを無相関と呼びます。
2つのデータの相互関係の強さを定量的に表す指標として、相関係数rと呼ばれるものがあります。rは−1からプラス1の間をとり、ゼロの時は無相関で相関の度合いが強いほどrの絶対値は1に近づきます。
〜〜〜相関係数に関する注意〜〜〜
図5.曲線相関
相関係数の値が±1に近いと言うことはXとYの間に強い直線関係があることを意味しています。また、ゼロに近いということは直線関係がないことを意味していますが、必ずしも無関係と言えません。
図5に示すような山形の曲線相関では相関係数がゼロにもかかわらず強い曲線関係が存在するからです。                         
つまり、相関係数は直線関係についてだけを述べているにすぎません。従って相関係数を計算する前に必ず散布図を作り確認する必要があります。 また、仮にある相関係数が0.8と0.2の場合を比較する際、0.8の方が0.2より強い直線関係があることは示せますが、その強さが4倍である事は言えません。更に、高い相関係数を持っていたとしても、XとYの間に直接の因果関係があるとは即断できません。相関係数はそこまで便利な性質は持っていないことに注意して下さい。
図6.外れ値の影響
相関では、他から遙かに離れた1つの点、外れ値(outlier)が大きな影響を与えます。外れ値は殆どの統計計算に影響を及ぼしますが、特に相関において著しい影響を与えます。図6では10個の黒点で計算するとr=0.8近辺になり強い相関をしめしますが、1個の外れ値赤点を含めた全てを計算するとr=0.3程度になり相関はあまりない事になります。わずか1個の点を含めるか除外するかで結論に大きな差が出てきます。         このことからも相関係数で結論を出す前に散布図をつくりグラフを眺めることが重要となってきます。
また、解析を混乱させるとの理由で外れ値を切り捨てることもよくありますが、安易に切り捨てないで下さい。時として外れ値は最もよい観察結果であることがあり、宝の山になる可能性を含んでいる事があります。
<相関係数から推定を行う前提>
1.データの対象は(大きな)集団からランダムに得ばれている事。
2.全ての対象間の関係は独立であること。対象者が意図的に集められていたりそれぞれを2度測定しそれを2つの別データとして扱ったりなどは独立性が損なわれる。
3.コントロールされたデータではないこと。(コントロール群は回帰で処理)
4.直線関係(線形1次式)であること。
5.全てのデータは1つの母集団から抽出されていること。
<相関係数と因果関係> 相関係数から因果関係を確定するには次の5点に留意しましょう。
1.関連の時間性:原因は結果の前に存在する。
2.関連の密接性:原因が結果と密接に関連する。(量反応関係があると因果関係の可能性が強い)
3.関連の特異性:原因が結果の発生に特異的に係わっている。
4.関連の普遍性:時期、対象、方法が異なっていても類似した結果が得られる。
5.関連の合理性:従来の経験、理論などから考えても矛盾しない説明がつく。
この5点が満たされていれば因果関係はかなり確実性が高いと言えるでしょう。(必ずしも1〜5全てを満たす必要はありません。)
図7.擬似相関
因果関係を調べていく場合には特に擬似相関に注意する事が必要です。擬似相関とは変数X、Y同士は直接関連がないにもかかわらず、XとZ(結果)に因果関係があり、またYとZも因果関係を共有している時変数X、Y同士に見かけの相関が表れることをいいます。
特に時系列を扱うデータではこのような関係がよく表れて来ますので、注意して下さい。
2つのデータの間で相関性をに因果関係があるかどうかは上記の前提条件を満たしている事を確認の上、因果関係の1〜5までの項目を考慮して2つの間の相関の強さを見るために相関係数を求めます。
データの標本数がある程度多い場合において相関係数の値により以下の事が言うことができます。
1.0≦相関係数r<0.7       :強い正の相関がある
0.7≦相関係数r<0.4       :かなり正の相関がある
0.4≦相関係数r<0.2       :やや正の相関がある
0.2≦相関係数r<−0.2     :殆ど相関はない
−0.2≦相関係数r<−0.4    :やや負の相関がある
−0.4≦相関係数r<−0.7    :かなり負の相関がある
−0.7≦相関係数r<−1.0    :強い負の相関がある
だだし、標本が少ない場合はこのように言えるとは限らないので、母相関係数の推定・検定を行う必要があります。
(扱う標本総数として60以上は欲しい所です。100個以上あれば申し分ないのですが。)
相関係数rの計算は、統計ソフトのみならずエクセルでもCORREL関数として用意されていますので、通常自分で計算する事はないと思いますが、一応数式は以下に載せておきます。
ここで、はXの平均値、SχはXの標準偏差、n(N)はデータの個数を表しています。
平均と偏差、分散、相関
