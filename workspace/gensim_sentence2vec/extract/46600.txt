テストコレクションとは、そもそもは、ある技術の性能を評価するための実験(テスト)に使用するためのデータセットという意味です。NTCIRプロジェクトでは、情報検索やテキスト処理技術の研究開発において、繰り返し使用できる、大規模な実験用データセット(=テストコレクション)の構築、研究目的利用、これを用いた評価型ワークショップの開催を行っています。情報検索システムの研究開発で使用する実験用データセットをさす言葉として使用される場合もありますが、NTCIRプロジェクトでは、広く、情報検索だけではなく、要約、Q&A、情報抽出などの種々のテキスト処理などの「技術の性能を評価するための実験用データセット」という意味で使用しています。情報検索用のテストコレクションは、情報検索システムの検索性能(質)を評価するために用いる、(1)文書集合、(2)検索要求、(3)各検索要求に適合する正解文書の網羅的リスト、という3つのものからなる実験用データセットです。共通の標準的テストコレクションを使用することにより、複数の検索手法の比較が可能になり、評価の信頼性も高まります。また、評価環境を用意するための研究者の負担を軽減し、研究の質を高め、情報検索研究への新規参入を容易にすると期待されます。情報検索研究では、新しく提案する検索手法は、文献数が数十万件の大規模テストコレクションを用いて検索性能を評価するのが国際的な研究コミュニティの通例になっています。また、情報検索は、言語に依存した処理を多く含むため、日本語情報検索研究には、日本語の大規模テストコレクションが必要です。欧米諸国語のほか、中国語、韓国語の大規模テストコレクションがありますが、日本語については、NTCIRプロジェクトを開始した時点では、利用可能なものは BMIR-J2(新聞記事)のみであり、一層の拡充が必要でした。検索性能(search effectiveness,検索有効性ともいう)は、精度(precision)、再現率(recall) という尺度を用いて評価します。精度(precision): 検索された文書の中で、レレバントな文書数の割合、再現率(recall): データベース中のすべてのレレバントな文書の中で検索された文書数の割合
再現率は、データベース中の全レレバント*文書数が必要なため、実験的環境でしか算出できません。また、大規模なデータベースで、各検索要求にレレバントな文献を網羅的に調べるには多くの労力が必要であり、研究者個人が行うのは非常に困難です。また、安易な方法で、レレバント文献の集合を作成すると、再現率が不当に高く評価されるという危険もあります。このような危険を回避し、正当な評価を行うためにも、標準的テストコレクションは、情報検索研究の基盤として重要です。また、現在、情報検索システムでは、利用者が入力した問合せに適合している順に検索結果が表示される「適合度順出力」が主流です。このような検索システムでは、検索実験を通じて最適なパラメータを設定する必要があり、情報検索システムの研究・開発には、検索実験に使用できるテストコレクションが不可欠です。しかし、大規模なデータベースから特定の検索要求に対する正解文書を網羅的に探し出すことは容易なことではありません。一つの有望な方法として提案されているのが、「プーリング(pooling)」です。これは、異なる複数の検索システムが同一の検索要求について検索を行い、その検索結果を集めて、正解文書の候補とする方法です。その後、これらの正解候補文書を人間の判定者が1件ずつみて正解か不正解かを判定します。検索システムは、その基づいているモデルによって、特性があります。ですから、複数のシステムの検索結果を「持ち寄る=プール」することによって、それぞれの弱点が補われ、網羅的な正解候補の収集ができるというわけです。この「プーリング」のためには、複数の検索システムが同一の課題に取り組む評価型のワークショップが有効だと言われています。そこで、我々も、テストコレクションを構築する過程で、ワークショップを開催することにしました。、このワークショップは、参加者の方々のご協力によって、大規模なテストコレクションを構築する上で、最も大変な作業の一つである「プーリング」(正解候補文書の網羅な収集)を行い、今後の情報検索研究コミュニティが利用できるリソースの構築を進めるとともに、情報検索の様々なアプローチについて、効果を比較したり、互いに意見交換、情報交換をする場をも提供することによって、情報検索研究の一層の発展をめざすものです。情報検索の検索性能評価の実験のスタイルが古くから確立しているのに対し、テキスト自動要約の研究は、1950年代から盛んになったといわれ、長い歴史がありますが、その評価は非常に難しいとされてきました。最近、米国のTipsterのSUMMAC、その後継プロジェクトであるTIDESのDUCなどを通じて、テキスト要約技術の評価に対する研究や議論が盛り上がりを見せています。そのような中で、NTCIRでも、テキスト自動要約、Q&Aなどを取り上げています。これらの技術をどのように評価するか!? これは、これらの技術の研究をしている多くの研究者の議論を通じたコンセンサスとしてなりたっていくものだと思います。ですので、どうぞ、NTCIRに参加し、また、そのタスクの定義や評価方法の議論にも積極的に参加してください。つねに新しい技術には、それに適した評価法が必要です。このような趣旨をご理解の上、ワークショップにも、ぜひ、ご協力、ご参加いただければ、幸いです。また、テストコレクションの構築、ワークショップの運営などについても、忌憚のない、ご意見、ご提案などがございましたら、お気軽に、ワークショップ事務局ntc-adminまで、ご連絡ください。*レレバント(relevant): 検索要求に適合する
テストコレクションとは - NTCIR プロジェクト
