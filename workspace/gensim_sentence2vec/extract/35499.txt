複数人のタグ付けの一致度を判定する
23:46
勉強中のメモで,いい加減な書き方をしまくっていると思いますが,それでも公開した方が有益な気がするので公開します,お許し下さい。
コーパスに人手でタグ付けしてもらうとき,そのタグ付けがどのくらい信頼できるのかが問題になります。この場合,タグ付けを複数人にお願いしてその一致度を見て,全然一致していなければ,それはタグ付け基準(か,タグ付け要員)に問題がある,ということになります。			問題はその「一致度」をどうやって測るかです。そこで,Artstein and Poesio (to appear)*1 に基づいて,一致度を測るやり方についてまとめてみました。
タグ付け要員が2人のとき
単純な一致率を使う
AさんとBさんが同じタグを付けた項目の割合をもって一致度とする。
項目 i について,i でAさんとBさんのタグ付けが一致してるとき ,そうでないとき  として
これは単純でいいけど,タグ付けが偶然一致する場合があるってことを考慮してないので,異なるタグ付け作業間で数字を比較することができない。具体的には次の2点が問題。
選択肢が少ないと偶然一致する率が高くなるので,そのぶん高い数字が出てしまう(たとえば「具体名詞」か「抽象名詞」か,でタグ付けするのに比べて,「具体名詞」を生物と無生物に分けましょう,とかになると一致度は下がってしまう)
選択肢の頻度に偏りがあるとやはり偶然一致する率が高くなるので,そのぶん高い数字が出てしまう(たとえばある発話が新しいトピックの導入かどうか,でタグ付けしたとして,そんな発話が仮に全体のごく少数だとしたら,当然タグ付けの一致度は非常に高くなってしまう,が,その一致度の高さは適切にタグ付けできているかどうかの参考にならない)
これらの問題を一部あるいは全部解決した指標に,S,π, κ がある。これらはどれも,「偶然一致することが期待されるぶんはおいといて,残りがどんだけの割合一致したか」を求める指標。実際一致した割合を ,偶然一致することが期待される割合を  で書くと
「偶然一致するものの割合」をどう想定するかによって S,π, κ の3種類に分かれる。
S (Bennett, Alpert and Goldstein 1954)
これは,上に挙げた単純な一致率を使うことの問題点のうち,選択肢の数によって偶然一致する率が変わってきてしまうことだけに対応したもの。選ばれやすい選択肢とそうじゃない選択肢があるかもしれない,という点には対応していない。
これだと,選択肢が n 種類なら偶然一致する率は 1/n になるので
ということになる。
π (Scott 1955)
Scott's Pi。これは,選択肢によって選ばれやすさに違いがある,ということも考慮。ただし,個人差があるとは想定しない。
選ばれやすさは,実際にその選択肢が2人のタグ付け要員によってどれだけ選ばれたか,によって見積もる。例えば A さんが「これは対照の意味!」と判断した「は」が32個,Bさんがそう判断した「は」が28個で,タグ付けしなきゃいけない「は」の数が 100 個だったとすると,「対照の意味」という選択肢が全体に占める割合は (32+28)/(100*2) で 0.3 (30%) ということになる。0.3 ということは,二人とも偶然その選択肢を選ぶ確率は二乗して 0.09 (9%) となる。と,いうようなことを各選択肢について考えればいい。まとめると,二人のタグ付け要員が選択肢 k を選んだ数の合計を ,全体項目数を i として
となる。
κ (Cohen 1960)
Cohen's Kappa。これは個人差も考慮する。さっきと同じ例で話をすると,つまりAさんは「対照の意味」を選びまくるが,Bさんは全然選ばないかもしれない。Aさんが「これは対照の意味!」と判断した「は」が40個だとすると,Aさんが「は」を選ぶ率は 0.4 (40%)。Bさんがそう判断した「は」は6個だけだとすると,Bさんが「は」を選ぶ率は 0.06 (6%)。二人が「対照の意味」で偶然一致する確率は,掛けて 0.024 (2.4%)。まとめると,二人が選択肢 k を選んだ数をそれぞれ ,  として
となる。
この尺度はけっこう有名みたいだ。日本語で読める解説。
κ 統計量( 一致率の検定 )
no title
3人以上にも使えるように拡張した尺度
Multi-π (Fleiss 1971)
基本的に pairwise agreement というのを考えていく。つまりタグ付け要員が c 人いたら,そのなかの「2人の組み合わせ」の数は c(c-1)/2。そのうちタグ付けが一致している「2人の組み合わせ」がどんだけあるか,を考える。それの各項目についての平均がこれまでの  の代わりになる。 についても
とすればいい。
Multi-κ (Davies and Fleiss 1982)
基本的に Multi-π の場合と似たようなかんじで κ を拡張できる。
「不一致にも色々ある」ことに対応できる尺度
タグ付けの不一致を全部一律に扱うのではなくて,惜しい不一致と,まったくの不一致とを区別できる尺度。すみません,まだよくわかりません。
α (Krippendorff 1980, 2004)
重み付きκ (Cohen 1968)
*1:Artstein and Poesio (to appear). Inter-coder agreement for computational linguistics. Computational Linguistics. http://www.mitpressjournals.org/doi/abs/10.1162/coli.07-034-R2
複数人のタグ付けの一致度を判定する - asaokitan のメモ
