統計的学習理論(1): フィッシャー情報量とクラメールラオ下限と最尤法
勉強したことメモ。数式を使わずに書く。また、行間をスキップせずに、多少くどいかもしれないくらいにきっちり順を追って説明を書いたので長いけどわかりやすくなっているはず。第一回はベイズの手前まで、最尤法のあたりまでの話をする。
推定量
データを表す確率変数があってその密度関数は何らかのパラメータであらわされているとする。観測したデータから合理的にパラメータを決定するタスクのことを推定という。推定を世界で最初にガッチリ研究したのはフィッシャーという人で、彼は推定方法の良しあしを判断する基準として、(A)不偏性、(B)有効性、(C)一致性、(D)漸近正規性、(E)十分性、などを考えた。データからパラメータを推定する手続きは、データの関数として表せる。そういう関数を推定関数、そうやって計算した値を推定量と呼ぶ。観測されうるデータは確率変数なので、推定量も確率変数となる。推定量が確率変数だってのは当たり前のことであえて言うべきことでもないと思うかもしれない。しかしこれは重要なことだ。なぜかというと推定量が確率変数であるならば、何か実験をしてデータを観測するという行為をせずとも、推定量の良しあしは理論的に評価できるということになるからだ。これがすごく大事。そういうわけで、実験をやってみてその都度ケースバイケースでパラメータ推定値の良しあしを評価するというのではなく、あらかじめ理論的に推定関数のレベルで良しあしを評価しておくことによって「パフォーマンスが既知なパラメータ推定フレームワーク」が作れるってことになるのだから期待を持って議論を進めよう。もともと設定した「データを表す確率変数」には何らかの密度関数が付与されているのであった。なので推定量はその密度関数を使って(そういう関数をネストして計算する値として)表現されるはずだ。つまり推定量はデータの密度関数とパラメータの推定関数によってあらわされる。後者の推定関数はパラメータ推定のやりかたによっていろいろバリエーションがあるだろうが、前者の密度関数は実験を行う前に何らかのモデル化を行って決めてあるはず。よってデータの密度関数は既知として議論を進めてOKだ。
推定量の良しあし:フィッシャー情報量
じゃあデータの密度関数があるとして、「推定量の良しあし」を評価する方法はないだろうか。なんらかの指標を使って、その指標が大きいか小さいかで良しあしを計れないだろうか、というのが次の課題となる。で、フィッシャーはうまい指標を編み出した。
データの密度関数の対数をとり、パラメータで偏微分する。これをスコアと呼ぶ。
するとスコアの期待値は理論的にゼロとなる
スコアの分散は定数にはならないが元の密度関数を使って表せる
この「スコアの分散」をフィッシャー情報量と呼んでおこう
つまり「スコアの分散」という指標が「推定量の良しあし」を評価するのに都合がよいようなのだ。その根拠は以降で見てゆく。その前にちょっと補足しておく。まずこのスコアと、フィッシャー情報量についてもう一度念押ししておきたいのは、「ここまでパラメータの推定方法についての議論なし」という点。つまりスコアやフィッシャー情報量というのは元のデータをどうモデル化したかには依存しているが、パラメータ推定をどうやるかについては特に触れずに定義した数量だということに注意しておきたいのだ。また、スコアもフィッシャー情報量も、「データとパラメータの関数」であることにも注意しておきたい。今やっている作業は、上述したように実験を行ってデータを観測したときに使えるようなフレームワークを理論的に作っておくという作業であった。なので実験を行うと、スコアやフィッシャー情報量には具体的なデータの値が代入されて「パラメータの関数」になる。であれば今フレームワークを作っている段階ですでにデータはあるものとして議論を進めちゃっても同じことだ。スコアもフィッシャー情報量も(観測データを想定として含んだうえでの)「パラメータの関数」として考えて先へ進もう。
推定量のばらつき:クラメールラオの不等式
さて、補足はここまでにして、議論をもどそう。上で半ば天下り的に定義した「フィッシャー情報量」は具体的なパラメータの推定方法についてなんにも言ってないにも関わらず推定量の良しあしを評価するのに使えるという話。それはクラメールラオの不等式というものだ。順を追って説明しよう。上のほうで少し述べたが、パラメータの推定量というのは確率変数なのであった。なので「実験するデータを観測するパラメータ推定値を算出する」というのを何セットも繰り返せばそのパラメータ推定値はあるばらつきをもったサンプルとして集まるだろう。ここで少しだけパラメータの推定方法に条件をおく。すなわち「そうやって集めたサンプルの期待値がもとのパラメータと等しい」という条件。この条件を冒頭のほうで述べた(A)不偏性と呼ぶ。
ちょっと脱線:正規分布の母分散の推定についての注意
この不偏性についてまた紛らわしい話がいくつかあるので、ちょっとここで脱線気味になるが整理・補足をしておく。まず、ここで「パラメータ」と呼んでいるものについてだが、もし元のデータの密度関数が正規分布だとしたらパラメータは正規分布の母平均と母分散ということになる。つまりパラメータは母平均かもしれないし母分散かもしれない。もちろんその両方のペアでもよい(ただここまでパラメータはスカラーとして話を進めてきたので、フィッシャー情報量はスカラー量としている。パラメータが複数の変数すなわち多変量だとしたらフィッシャー情報量はフィッシャー情報行列になる)。推定したいパラメータが正規分布の母平均であれば、不偏性という条件を満たす推定方法は簡単に作れる。毎回の実験で「実験するデータをN個観測するN個のデータの平均(標本平均と呼ぶ)をとる」とやればいい。そうすれば実験を数セットやって得られる推定値たちは不偏性を満たす。推定したいパラメータが正規分布の母分散だったときはちょっとややこしい。単純に毎回の実験で「実験するデータをN個観測するN個のデータの分散(標本分散と呼ぶ)を求める」とやってしまうと、不偏性を満たさなくなってしまうのだ。それはなぜかというと、標本分散を計算するにはまず標本平均を計算して、各データの平均からのずれを計る必要があるのだけど、そのときに平均(つまりそれも推定量なので確率変数だ)の分だけ自由度が一つ減ってしまい、標本分散としてNで割ると分母のほうが1つ多くカウントされた状態で分散を母分散より低くみつもってしまうのだ。例えば、仮にN=2だとしたら、標本平均はその2つのデータのちょうど半分のとこだ。なので標本分散は「2つのデータの間隔の半分の2乗」の2つぶんだ。それをN=2で割ってしまうと、「2つのデータの間隔の半分の2乗」が残る。これはばらつきとしてはずいぶん小さく見積もってしまっている。この場合Nから1を引いてN=1で割らないといけないのだ。Nで割る代わりにN-1で割る推定方法を不偏分散と呼ぶ。まとめると、正規分布のパラメータの不偏推定量は、母平均はなんも考えずに標本平均でOKだが、母分散はNじゃなくてN-1で割る不偏分散というのを使う必要があるので注意が必要だ、ということだ。そういう風に、1回の実験におけるパラメータ推定値の算出方法の話と、そういう実験を何セットもやったときに各実験(そういうのを「試行(トライアル)」とよぶ)を総合してみたときにどうなるかという話で、レイヤが一つ違うってことを意識してないと混乱するのでそれも注意が必要なのだ。さぁ、脱線はこのくらいにして、また本題にもどろう。
不偏性と有効性、そのトレードオフ
不偏性すなわちサンプルの期待値がもとのパラメータと等しいという条件が満たせたとする。実験を何セットやっても平均はちゃんと偏らずにいてくれることが保証されているわけだ。じゃあ推定値のばらつき、つまり分散はどうなるのかという議論が次に来るのは自然だ。実はこの推定量の分散のほうは、不偏性の条件を置いただけで自動的に下限が決まってしまうのだ。その下限とは、「｛データの個数×フィッシャー情報量｝の逆数」である。これをクラメールラオの下限と呼ぶ。どんなパラメータ推定方法であってもそれが不偏性を満たしているだけで、繰り返し実験を行ったとしても最低でもクラメールラオの下限の値だけバラつきが出てしまうのだ。もちろんそれ以上にばらつきが大きいようなイケてない推定方法もありうるし、バラつきを抑えようとすると今度は不偏性を満たすことができなかったりしてしまう。まさに、あちらをたてればこちらが立たず、という状況だ。とりあえず、クラメールの下限を満たす推定方法はバラつきという観点では優秀と呼べるので、名前を付けておこう。バラつきが下限と等しくなるような推定量を冒頭で述べた(B)有効性を満たす、有効推定量と呼ぶ。不偏性は推定量の平均についての条件、有効性は推定量のバラつきについての条件なので、それぞれはトレードオフの関係になることもある。さて、推定量の良しあしを判断する尺度としてフィッシャー情報量、不偏性、有効性、という概念を導入してきた。ではせっかくなので具体的な推定方法についても何か汎用的なフレームワークを作っておこうではないか。それはなるべくシンプルなものがいいだろう。そしてそうやって作った推定量について、不偏性や有効性がどうなるかを予め調べておこう。もしそれが不偏性や有効性という条件をうまい具合に満たしてくれてたら、もちろん嬉しいことだ。
そんなに万能じゃない気もする!?:最尤推定量
フィッシャーは考えた。とりあえず超簡単に「実験するデータをN個観測するもとのデータの密度関数に観測データを代入して出来上がる『パラメータの関数』が最大になるようなパラメータを求める」というのはどうだろう?この推定方法を最尤法と呼び、そうやってもとめる推定量を最尤推定量と呼ぼう。上のほうでパラメータが正規分布の母平均や母分散のときの話を少し脱線気味にしたが、そのときに挙げた標本平均と標本分散はこの最尤法の枠組みから導き出すことができる。それについて不偏性と有効性を検証してみよう。まず正規分布の母平均の最尤推定値として標本平均はもちろん不偏性を満たす。そして実は有効性も満たすのだ。つまり母平均を推定する推定方法の中で分散が最小になる。母平均の推定において最尤法のパフォーマンスは実にすばらしいとわかった。では正規分布の母分散の最尤推定値のほうはどうか。それは標本分散ということになるが、上述したとおり不偏性は満たさないのだ。よって有効性についての議論もケースバイケースになってしまいそうで難しい。どうやら母分散の推定における最尤法のパフォーマンスは芳しくないような気がする。しかし、最尤法のシンプルさを見るに、かなりの汎用性を秘めていると思われるフシもある。だったら最尤法がもつ真価をもう少し吟味してあげようではないか。そうやって考えていくと、一致性と漸近正規性というのが浮かび上がってくるのだ。
最尤法のいいところ:一致性と漸近正規性
最尤推定量は正規分布の母分散みたいな基本的なところでさえ不偏性をみたさなかったりするのであったが、視点を実験の回数を重ねるところではなく、一回の実験で採取する観測データの個数のほうに向けてみよう。正規分布の場合、1回の実験でとる観測データの個数をどんどん増やしていくと、標本分散の値は限りなく母分散に近づいてゆく。もちろん母平均の方もだ。そして、正規分布のパラメータに限らず、最尤推定量はちょっとした条件は必要となるが、基本的にはデータの個数が無限にあればその推定量はかならず元のパラメータに一致する。なので、実験を繰り返さなくても、観測データが無限にあれば実験は1回やるだけで真のパラメータがビシっと求まってしまうのだ。…なんかオカシイこと言ってませんかとツッコミ食らいそうな論法だけれども、これが(C)一致性という性質なのである。1回の実験でデータの数を無限に観測できれば実験1回で済むって、そりゃ当たり前じゃないか、何言ってんだオマエって思われるかもしれない。けど、ここで注意しなければならないのは、今作ってるのは「フレームワーク」なのだ。つまり、1回の実験で観測データの数がたくさん取れるような問題設定の場合と、観測することによって対象物に影響を与えてしまってあんまり観測データをたくさん取れないけどリセットして何度もトライアルを繰り返せる場合と、そういう状況に合わせて使えるフレームワークを作ろうとしているのだ。だから、最尤法が活きてくるのはこれこれこういう条件のときですよ、というのをハッキリさせておくことは、その条件がちょっと紛らわしかったとしても本質的に重要なことなのだ。さて、一致性についてはそういう感じで納得することにして、最尤推定量にはもう一ついいところがある。それが(D)漸近正規性だ。これは何かというと、一致性について述べたような観測データの数を増やしていくと推定量が真のパラメータに近づいていくだけでなく、データの分布自体つまり適当に区間を区切った時の度数分布の形状が正規分布そのものに近づいていくという性質があるのだ。しかし、そもそも一致性があるのだから、漸近正規性によって正規分布の形になるといってもどんどん針のようにとがった正規分布になるだけでその形が正規分布と同じかどうかなんてそんなに意味のあることじゃないじゃないか、とアナタが思ったとしたらアナタはisobeと同じくらい頭がよく、そしてisobeと同じくらい頭が悪い人だと思われる。では次の段落を読む前に、この漸近一致性の嬉しさについてちょっと考えてみてほしい。5〜10分くらいで私と同じ答えが出てきたら、やはりisobeと同レベルの頭脳の持ち主ということになるだろう。私の考えは、こうだ。確かに一致性がある限りデータが増えれば針のようにとがっていくので針は針であって形状は関係ないのはそうなのだ。拡大すれば正規分布になるとかもそんなに本質的に嬉しい話ではない。むしろ問題は「すそ野」のほうなのだ。正規分布というのは「2乗の指数関数」という裾野の減り方が非常に激しい分布なのだ。つまりそれがどんどん尖っていくということは針の近くにあった裾野が激しい勢いで密度を失っていくということなのだ。ちなみに本当の答えは、こうだ。正規分布どうしの比で考えた場合にどうなるか。尤度比という統計量を計算することで「検定」ができるようになる。漸近正規性があると尤度比がカイ二乗分布に収束するという性質が導かれるため、検定を行うときに強力な武器となるのだ。こちらを想像できた貴方はisobeよりも頭脳と見識が優れた方だといえるだろう。漸近と聞いて検定問題がパッと思い浮かぶというのは良いことだ。
十分性、特異モデル、そしてベイズ推定へ
さて、推定量について冒頭で挙げた(A)〜(D)についての説明をした。残るは十分性というテーマだが、これはもう少し色々面白いことがある上に、ベイズ推定との面白い関係性もあるので回を改めて説明しようと思う。そして、次にくる話はフィッシャー情報量が不定となるケースだ。本記事ではフィッシャー情報量はスカラーとして考えていた。まずそれがゼロになってしまうケース、つまりクラメールラオの下限が無限に発散してしまうケース、そしてパラメータ空間が多次元、つまり多変量解析の世界についてフィッシャー情報行列を考え、それがランク落ちするケース、そういうものを考えていく。では、次回また。
ツイートする
Permalink | コメント(0) | トラックバック(0) | 04:49 
統計的学習理論(1): フィッシャー情報量とクラメールラオ下限と最尤法 - アドファイブ日記
