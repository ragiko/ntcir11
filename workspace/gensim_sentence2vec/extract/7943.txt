
open(OUT, &apos;>result.csv&apos;);
open(IN1, &apos;enquate.csv&apos;);
# アンケートデータを1行ずつ処理する
while(my $line1 = <IN1>){
# 1行を4つに分ける
chomp($line1);
my ($id, $ans1, $ans2, $ans3) = split(/,/, $line1, 4);
# この行にマッチする顧客データを検索する
my $name    = &apos;&apos;;
my $address = &apos;&apos;;
open(IN2, &apos;address.csv&apos;);
while(my $line2 = <IN2>){
chomp($line2);
my ($tmp_id, $tmp_name, $tmp_address) = split(/,/, $line2, 3);
if($tmp_id eq $id){
# 対象となる顧客が見つかった!
$name    = $tmp_name;
$address = $tmp_address;
last;
}
}
close(IN2);
# 出力する
print OUT join(&apos;,&apos;, $id, $ans1, $ans2, $ans3, $name, $address), "\n";
}
close(OUT);
close(IN1);
結果が出たらメールで送信して、今日はさっさと定時で帰ろうと思っていた新人クン。ところが、いつまで待ってもこの処理は終わる兆しが見えません。処理が終わるまでの待ち時間を利用してコードを1行ずつ何度も見直したのですが、論理的には間違えていないコーディングに見えます。いったいなぜ終わらないのでしょう?30億回のループ処理 新人クンのスクリプトがなかなか終了しなかった原因は、ファイルの読み込み処理が2重のループになっていることにあります。外側の「enquate.csv」の件数が3万件で、その1行1行について「address.csv」を毎回開いてデータを検索しています。「address.csv」の行数が10万行ですので、単純計算で10万行×3万回=30億行をファイルから読み込む処理であるということになります(ただし、検索が完了したらlastしているので、実際にはもっと少ない行数で済みますが)。 こんな回数をループ処理していれば、処理が終わらないのは当たり前です。筆者の環境では1時間待っても処理が終わらなかったので、とうとう［Ctrl］+［C］で強制終了してしまいました。ハッシュを有効に使う では、この処理を現実的な時間で終えるためにはどうすればいいのでしょうか? そのためには、「address.csv」から対象行を呼び出す処理を高速化する必要があります。 前回の記事でも述べた内容ですが、ここで、取り扱うファイルの大きさが問題となってきます。今回扱っている「address.csv」は5MB程度の大きさですので、全てメモリに取り込んでも大きな問題になることはなさそうです。そこで、3万回繰り返していた「address.csv」の読み込み処理を最初に持ってきて、全てメモリに持つように変更してみます。これでファイルの読み込み行数は30億行から10万行に減るはずです。 検索の高速化にはもう一つ重要なことがあります。それは、メモリ上のデータを高速に検索するための索引、つまり、インデックスを張ることです。インデックスが用意されていなければ、対象となる顧客IDに対応する行を探すのに、毎回全データを走査しなければならなくなりますが、それはあまりに非効率的過ぎます。 Perlにおいて、このインデックスとして適任なのはハッシュです。キーに対する値を、高速に探し出すことができるからです。今回の例では、顧客IDから対象となる行がユニークに定まりますので、顧客IDをキーとしたハッシュに「address.csv」のデータを保存します。 コードは以下のようになりました。

