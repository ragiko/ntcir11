
Transcript
1.
可変長オーダー Linear-Chain CRF 京都大学 情報学研究科 知能情報学専攻(黒橋研究室) 修士 2 年 真鍋宏史 
2.
発表の流れ 研究の背景 関連研究 提案手法の概要 Linear-Chain CRF 提案手法 ( 可変長オーダー Linear-Chain CRF) 期待値計算 デコード 考察 
3.
研究の背景 品詞タグ付け:二つ前・後ろのタグ情報が有効 (Stanford Tagger: MEMM の一種、二つ前・後ろのタグ情報を素性として利用 ) Linear-Chain CRF では? 一般的には  bigram  素性まで 次数を  1  上げるごとに、計算量がラベル数倍 離れた関係が利用できない 
4.
関連研究 -Semi Markov CRF(1) Semi-Markov CRF (出所) "Semi-Markov Conditional Random Fields  を用いた固有表現抽出に関する研究   2006 年 福岡 健太 
5.
関連研究 -Semi Markov CRFs(2) Sarawagi et al., 2004 ノードの長さを可変長に チャンクを対象とするものに対して有効 (チャンキング・固有表現抽出等) 計算量は  Linear-Chain CRF  以上 次数を上げることは同様に困難 
6.
関連研究 -Sparse Higher Order CRFs Qian et al, 2009 可変長オーダー Z s:t  = Z s  × Z s+1  ×… × Z t  の形で表されるラベル列の集合に対して素性関数を設定 本研究と手法は違うが、それぞれの優位点は未調査 
7.
提案手法の概要 可変長オーダーの  Linear-Chain CRF  に対する期待値を多項式時間で計算 アクティブな素性を持つパスに限定 動的計画法( DP ) SHO-CRFs  と違い、すべてのパスに対して素性関数を独立に設定 
8.
Linear-Chain CRF(1) 最大エントロピーモデルの一種 MEMM :遷移ごとの最大エントロピー CRF :可能な系列すべての最大エントロピー 可能な系列->指数的 DP (前向き・後ろ向きアルゴリズム)で期待値計算 
9.
Linear-Chain CRF(2) Forward-backward Score を計算 -> Edge  の期待値を計算 (Normalization factor: Forward score  で ) a a&apos; b b&apos; [BOS] [EOS] 3 0.5 3 4 2 4 2 1 F:3 F:0.5 F:10 F:14 F:34=Z B :1 B :2 B :1 B :10 B :8 F:1 B :34=Z (3 ×4× 1 )/34 
10.
高次  Linear-Chain CRF Linear-Chain CRF  は一般には 1 次 これを単純に高次に拡張 -> 計算量の問題 Feature  数が指数的に増加 パス数も指数的に増加 
11.
計算量の問題:素性数 (1) 例えば、 3 次の場合 : ある  feature function : f ( X , y i-3 , y i-2 , y i-1 , y i ) X   について条件を設定 ( ex:  大文字で始まる) X   についての条件がなければ、純粋な遷移素性 (transition feature) すべてのラベルの組み合わせについて素性関数を生成 
12.
計算量の問題:素性数 (2) ラベル  L:[L1, L2]  、 f(x i )  を x に関する関数とすると   f 1 ( X , y i-3 , y i-2 , y i-1 , y i )=1 if y i-3  = L1 and y i-2  = L1 and y i-1  = L1 and y i  = L1 and f(x i )   f 2 ( X , y i-3 , y i-2 , y i-1 , y i )=1 if y i-3  = L2 and y i-2  = L1 and y i-1  = L1 and y i  = L1 and f(x i )  f 1 ( X , y i-3 , y i-2 , y i-1 , y i )=1 if y i-3  = L1 and y i-2  = L2 and y i-1  = L1 and y i  = L1 and f(x i )   f 2 ( X , y i-3 , y i-2 , y i-1 , y i )=1 if y i-3  = L2 and y i-2  = L2 and y i-1  = L1 and y i  = L1 and f(x i ) … ( 2^4 通り) ->次数に対して指数的に増加 
13.
計算量の問題:パス数 (1) 前向き・後ろ向きアルゴリズム 動的計画法 1 次の Linear-Chain CRF の場合で Θ(L 2 )[L: ラベル数 ] 次数が上がるとラベル数倍 すべての可能なラベルの組み合わせについて計算 
14.
計算量の問題:パス数 (2) 3 次の場合: L1L1L1 L2L1L1 L1L2L1 L2L2L1 L1L1L2 L2L1L2 L1L2L2 L2L2L2 L1L1L1 L2L1L1 L1L2L1 L2L2L1 L1L1L2 L2L1L2 L1L2L2 L2L2L2 L1L1L1 L2L1L1 L1L2L1 L2L2L1 L1L1L2 L2L1L2 L1L2L2 L2L2L2 L1L1L1 L2L1L1 L1L2L1 L2L2L1 L1L1L2 L2L1L2 L1L2L2 L2L2L2 … 2 3   通りのノード、 2 4  通りのパス(指数的) 
15.
提案手法: 可変長オーダー  Linear-Chain CRF Feature function  のオーダーを限定しない 訓練データ中で アクティブになる  feature  に限定  (sparse feature set) 訓練データ中に出現するパターン数は訓練データのサイズに限定される(オーダーが増えても  exponentially  には増加しない) 
16.
Sparse Feature Set FlexCRF(Phan et al. 2005) 、 CRFSuite(Okazaki, 2007) で実装 (optional in CRFSuite) Training data  中でアクティブになった  feature  のみを使用 Feature  数を大幅に減らすことが可能 計算量 :  半分以下 Model size: 15%  程度 Tagging accuracy: 96.06%(dense) vs. 96.00%(sparse) (CRFSuite, CoNLL 2000 chunking shared task) 
17.
Feature function  の生成方法 すべての  f( X ,  Y )  のパターンを生成するのではなく、 訓練データ中で出現するラベル列があった時のみ素性関数を生成する (次数は可変) f( X ) の形でそれぞれの位置に対する「属性 (attributes) 」を与え、訓練データ中でその属性に先行するラベル列を記録、足切り(出現回数・頻度等)を行った上で f( X ,  Y ) の形の素性関数を生成する 
18.
Feature function  の生成(例) f( X ) = UC(x i ) = 1 if x i  begins with upper case 訓練データ : ... nonexective /JJ   director /NN   Nov. /NNP  ... ... chairman /NN   of /IN   Consolidated /NNP  ... ... that /WDT   makes /VBZ   Kent /NNP  ...  f 1 ( X ,  Y ) = 1 if UC(x i ) and y i  = NNP f 2 ( X ,  Y ) = 1 if UC(x i ) and y i  = NNP and y i-1  = NN f 3 ( X ,  Y ) = 1 if UC(x i ) and y i  = NNP and y i-1  = IN ... 
19.
可変長前向き・後ろ向きアルゴリズム 前向き・後ろ向きアルゴリズムでは、すべての 可能な組み合わせを考える 出現した並びのみを考える ->特別なアルゴリズムが必要 
20.
可変長前向き・後ろ向き  (1) すべての素性関数について、その重みに対して  exp  を求めておく -> 指数重み (重複計算を避ける) 
21.
デモ (1)-features 属性 (attribute) ""  であれば無条件 ラベル列 (label sequence) 長さ 1: 状態素性 長さ 2 以上 : 遷移素性 重みの指数を取ったもの exp of weight 
22.
可変長前向き・後ろ向き  (2) 一般的な前向き・後ろ向きアルゴリズム 状態素性はノード、遷移素性はエッジ 提案手法 状態・遷移素性ともにノードのように扱う -> パス 今注目している位置の属性 (attributes) に対する  feature function を列挙、同じラベル列を持つ feature の指数重みをかけ合わせ、まとめる 次の列で終わるパスから、そのラベル列の末尾を削除したもの-> チェックポイント 次の位置のために累積スコアを計算 
23.
デモ (2)- パスとチェックポイント ラベル付け対象 属性列 (attributes) パス チェックポイント パスの指数重み パスのスコア 
24.
可変長前向き・後ろ向き  (3) その列のパスとチェックポイントをソート (後ろから見たラベル順、一方がもう一方を含む場合は長いほうから、パスとチェックポイントが同じラベル列を持つ場合はパスが先) 
25.
可変長前向き・後ろ向き  (4) ソートした順の逆順(短い ->長い) にパスのみをたどる 短いパスの倍率(素性関数の指数重み)をそれを含む長いパスにかけ合わせる (長いパスでは短いパスの素性もアクティブになっているので) 
26.
可変長前向き・後ろ向き  (4)- 続き [ 短いパスの倍率を長いパスにかける手続き ] 短いものから順にパスをチェック 前のパスを完全に含む場合、スタックに前のパスの長さと前のパスまでの倍率をプッシュ そうでなければ、前のパスとの共通部分の長さまでスタックをポップ 今のパスの倍率に、スタックの一番上の倍率をかける (計算量:パスの数のオーダー) 
27.
可変長前向き・後ろ向き  (5) パスとチェックポイントを正順(長い ->短い)にたどる パスであれば : 今注目している列のラベルを削除して、それをキーとして前列のノードのスコアを取得 そのパスを含む、より長いパスによってスコアが使われている場合はそれを差し引く 差し引いた残りのスコアにパスの倍率をかけて、パスのスコアとして記録、現在のスコアに加算 
28.
可変長前向き・後ろ向き  (5)- 続き チェックポイントであれば : そのチェックポイントを含む、より長いチェックポイントのスコアを累積したものに現在のスコアを加算、チェックポイントに対応するスコアとして記録 手順 (3),(4) を末尾まで行う 末尾チェックポイントのスコア=正規化係数 (ここまで前向き) 
29.
可変長前向き・後ろ向き  (6) (ここから後ろ向き、末尾から先頭に) 今注目している列のチェックポイントとパスをソートの逆順(短い ->長い)にたどる チェックポイントであれば : そのチェックポイントのラベル列をキーとして、後列で記録されたスコアを取得、現在のスコアに加算 
30.
可変長前向き・後ろ向き  (6)- 続き パスであれば : 前向きで求めたスコアに現在のスコアをかけ、 パスのスコアを上書き 現在のスコアにパスの倍率をかけたものを求める。それから、そのパスに含まれる、より短いパスに対応するスコアを差し引いて、今注目している列のラベルを削除したものをキーとするスコアに加算 
31.
可変長前向き・後ろ向き  (7) ソートした順(長い ->短い)にパスのみをたどる 短いパスに対して、それを含む、より長いパスのスコアを足し込む  (長いパスでは短いパスの素性もアクティブになっているので) そのスコアを正規化係数で割ったものが、そのパス(素性関数)の期待値 
32.
可変長前向き・後ろ向き  (8) 全データに対してこれを行い、それぞれの素性関数の期待値を求め、訓練データ中での出現回数と合わせて重みを更新 ( 一般的な CRF でのパラメータ更新 ) 更新された重みに基づき、可変長前向き・後ろ向きアルゴリズムで期待値を再計算 -> 収束するまで繰り返す 
33.
可変長前向き・後ろ向き:まとめ 動的計画法により、各パスへの前向き・後ろ向きスコアを計算 累積スコアの差分を取っていくことで、パスの数に対して線形オーダーで計算可能 TODO: 帰納法により定式化 
34.
デコード 可変長前向きアルゴリズムと同様にパスとチェックポイントを作る チェックポイントでは、前のチェックポイントからの最大スコアのみを記録する パスでは、対応する前のチェックポイントと、それを含むより長いチェックポイントの中で使用済みでないものから最大スコアを求め、パスの指数重みをかけてスコアを求める (使用済みフラグは、現在位置のラベルが変わるたびにクリア) 
35.
考察 実際の応用では定数項の部分も重要 実装によって大きな差がある 単純なアルゴリズムのほうが高速化しやすい 高次の素性が「効いてくる」タスクでないと 優位性を得ることが難しい 高次の素性がスパースであるとは限らない 計算量はアクティブになる素性関数の数の オーダー 訓練データのサイズが上限になるが、 それでもかなり大きくなる場合もある 
Variable-Order CRFs
