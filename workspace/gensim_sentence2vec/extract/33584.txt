こんにちは、初心者です。
適当なニュース記事があったとして、ニュースのカテゴリを推測するみたいな、よくあるやつをやってみました。Python3.3を使いました。
何をやるの?
データセットはlivedoorニュースコーパスを使いました。
http://www.rondhuit.com/download.html#ldcc
クリエイティブ・コモンズライセンスが適用されるニュース記事だけを集めてるそうです。
トピックニュース、Sports Watch、ITライフハック、家電チャンネル 、MOVIE ENTER、独女通信、エスマックス、livedoor HOMME、Peachy というクラスがあります。
データは、1記事1テキストファイルの形式で、クラス別のディレクトリにいっぱい入っています。
これを学習して、未知の文章に対して、お前は独女通信っぽい、お前は家電チャンネルっぽい、みたいに、分類ができればOKとします。
どうやるの?
文書のベクトル表現方法はいろいろあるみたいだけど、簡単そうなBoW(Bag of words)を使います。係り受けの情報を捨てて、単語だけをバッグにぽいぽい入れちゃったイメージだそうです。誰のバッグだよ。僕のバッグそんなことない。すっごい、ポケットいっぱいあるし。
で、文章の特徴を表しそうな単語(特徴語)を何個か(何千個?)決めて、それの頻度をカウントすれば、文書をベクトルで表現できるということだそうです。
ということは、特徴語を決定する必要がある。このあたりは、Gensimというライブラリが良いそうなので、脳停止でそれ使います。
あと、日本語文章の特徴語を抽出するということは、その前に形態素解析をする必要がある。これはMeCabを使えば良いでしょう。
手順をまとめると、
1. 記事からMeCabで単語だけ切り出して記事を単語リストに変換
2. 単語リスト群から、Gensimで特徴語の辞書を定義
3. BoWの要領で各文章に特徴語が何個あるかカウントして特徴ベクトル作る
4. この特徴ベクトルで学習。
5. 未知の文章も、3の方法で特徴ベクトルを作れば、分類器にかけてカテゴリを当てられるはず
という感じだと思います。
各種インストール
ちまたにはPython2でのやり方が多かったのですが、Python3でやりました。多少ハマりましたがPython3で大丈夫です。Python3使おう!
scikit-learnインストール
前にやったので入れ方忘れました。
http://breakbee.hatenablog.jp/entry/2013/12/02/020834
このあたりで良いんじゃないでしょうか。Macなので、できるだけbrewで入れてます。
MeCabとmecab-pythonインストール
これに助けられました。
http://qiita.com/namoshika/items/37e1351f7ffefd505eec
昔brewで入れたmecabは、pythonから呼ぶと落ちたので消しました。
Gensimインストール
pip gensim でサクッと入れたんですけど、Python3では動きませんでした。調べたら、GensimPy3というのが別であった。うかつだった。
http://qiita.com/katryo/items/e40da7dc3cb666391e28
以上です。
1.MeCabで単語切り出し
PythonからMeCab呼べてすごく便利
http://stmind.hatenablog.com/entry/2013/11/04/164608
ここを丸パク……マネして、名詞だけ取り出しています。
簡単のために、7000記事のうち2記事、しかも一部だけ取り出してやってます。
# -*- coding: utf-8 -*-
import MeCab
mecab = MeCab.Tagger('mecabrc')
def tokenize(text):
'''
とりあえず形態素解析して名詞だけ取り出す感じにしてる
'''
node = mecab.parseToNode(text)
while node:
if node.feature.split(',')[0] == '名詞':
yield node.surface.lower()
node = node.next
def get_words(contents):
'''
記事群のdictについて、形態素解析してリストにして返す
'''
ret = []
for k, content in contents.items():
ret.append(get_words_main(content))
return ret
def get_words_main(content):
'''
一つの記事を形態素解析して返す
'''
return [token for token in tokenize(content)]
# 2記事の一部だけ取り出しました
# 1つめがITライフハック、2つめが独女通信の記事です。
if __name__ == '__main__':
words = get_words({'it-life-hack-001.txt': 'アナタはまだブラウザのブックマーク? ブックマーク管理はライフリストがオススメ 最近ネットサーフィンをする際にもっぱら利用しているのが「ライフリスト」というサイトだ。この「ライフリスト」は、ひとことで言うと自分専用のブックマークサイトである。というよりブラウザのスタートページにするとブラウザのブックマーク管理が不要になる便利なサイトなのである。', 'dokujo-tsushin-001.txt': 'たとえば、馴れ馴れしく近づいてくるチャラ男、クールを装って迫ってくるエロエロ既婚男性etc…に対し「下心、見え見え〜」と思ったことはないだろうか? 下心と一言で言うと、特に男性が女性のからだを目的に執拗に口説くなど、イヤらしい言葉に聞こえてしまう。実際、辞書で「下心」の意味を調べてみると、心の底で考えていること。かねて心に期すること、かねてのたくらみ。特に、わるだくみ。(広辞苑より)という意味があるのだから仕方がないのかもしれない。'})
print(words)
[['アナタ', 'ブラウザ', 'ブック', 'マーク', 'ブック', 'マーク', '管理', 'ライフ', 'リスト', 'オススメ', '最近', 'ネット', 'サーフィン', '際', '利用', 'の', 'ライフ', 'リスト', 'サイト', 'ライフ', 'リスト', 'ひとこと', '自分', '専用', 'ブックマークサイト', 'ブラウザ', 'スタート', 'ページ', 'ブラウザ', 'ブック', 'マーク', '管理', '不要', '便利', 'サイト', 'の'],['チャラ', '男', 'クール', 'エロ', 'エロ', '既婚', '男性', 'etc', '下心', '見え', '見え', 'こと', '下心', '一言', '男性', '女性', 'からだ', '目的', '執拗', 'イヤ', '言葉', '辞書', '下心', '意味', '心', '底', 'こと', '心', 'こと', 'わる', 'くみ', '広辞苑', '意味', 'の', '仕方', 'の']]
各記事が、名詞の配列になりました。なんとなーく、文章の感じは残ってる。
単語に重複がありますが、これは、あとで頻度を調べるのでもちろんそのままです。2個目のようにエロが2個続いてれば、それだけエロい文章なんでしょう。
それよりも、「の」とか要らない単語があります。「の」の数が文章の特徴に寄与しないのは明らか。捨てたい。
これをどうするかですが、
・ストップワードを定義して消す
・高頻度すぎる単語、低頻度すぎる単語を消す
とかがあるようです。
前者はこの時点でやってしまってもいいようです。ストップワードは、例えばI, my, his とか、どんな文章にも出てくるくだらない単語とかです。僕は、ここで数字だけのワードを消すようにしました。上のテストでは出ていないですが、全記事で試すと、意味の無い数字だけの単語が結構増えます。
後者は、次の工程でGensimを使って簡単にできるようです。ストップワードのいくらかは、こちらの作業でも消える気がします。
2.Gensimで特徴語辞書を作る
さっきの単語のリストから辞書を作るよ
これを参考にしました
http://sucrose.hatenablog.com/entry/2013/10/29/001041
scikit-learnとgensimでニュース記事を分類する - Qiita
