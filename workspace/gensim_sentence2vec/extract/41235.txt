本稿ではコンパクトなモデルサイズでありながら、次単語予測精度、スパースデータに対する信頼性、タスクのずれに対する頑健さを兼ね備えた言語モデルを提案する。このモデルではMAP推定により単語と品詞情報を連続的に補間した新しい単語特徴量に基づいて単語のクラス分類が行われる。このモデルは単語N-gramに対して50%モデルサイズを縮小しておりながら、単語2-gramに比べパープレキシティにおいては訓練セットと同一タスクでは3%、異なるタスクでは15%低く、さらに連続単語認識の結果においては、それぞれ16%、および28%単語誤認識率が低くなっている。
In this paper, an accurate and compact language model is proposed to cope robustly with data sparseness and task dependencies. This language model adopts new categories which are generated by continuously interpolating POS word-class categories and word categories using MAP estimation. This modeling reduces the model size to 50% of the conventional models. The bi-directional word-cluster N-grams generated by this modeling have 3% lower perplexity measured on a matched domain and 15% lower on a mismatched domain compared to a conventional word N-gram. More importantly, the word error rate for continuous word recognition was reduced by 16% for matched and 28% for mismatched domain.

