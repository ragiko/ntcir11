[論文] MMRate: Inferring Multi-aspect Diffusion Networks with Multi-pattern Cascades (KDD 2014) 読んだ
MMRate
概要
既存の情報拡散モデルでは情報のトピックに関わらず同じような拡散をするという仮定が置かれている.
しかし,あるトピックでは2ユーザ間で起こる伝搬が,別のトピックでは同ユーザ間では起こらないという現象は存在するだろう.
トピックごとに異なる情報拡散をモデル化する.
contributions は次の二点.
複数の側面にもとづくユーザの相互作用と,カスケードにおける様々な伝搬パターンを利用するために, multi-aspect diffusion network を推定する問題を定式化した.
30万リツイートで構成されるカスケードを分析した結果,(1) 複数の伝搬パターンが存在すること,2) ユーザのretweet行動は著しく異なることを発見した.
問題設定
カスケード : .Nはユーザ数で固定.ベクトルの中身はそのユーザが当該情報に接触したtimestamp.∞ の場合は接触していないものとする.
この時,(1) 自分より前の時刻に感染したノードからしか感染しない,(2) 感染元となるノードはひとつだけとする(またこの時感染元をparent,被感染側をchildと呼ぶ) という二つの仮定を置く
カスケードデータセット : カスケードをM個集めたもの.入力データ.
Aspect distribution : まず,カスケードデータセット全体でK次元のベクトルを持つ.これはAspect distribution.その上で,各カスケード i がK次元のベクトルを持つ.これは1-of-K表現のベクトルであり,各カスケードは単一のaspectを持つ.
拡散パターン : カスケードごとの拡散パターンを,感染が起こった2ノード間の時間差を引数とする関数で表現する.は拡散の早さを決めるパラメータ.
Aspect-sensitive information diffusion graph : Aspect ごとにユーザ間に異なるエッジがある非対称なサブグラフを考える.この関係はそもそもデータには存在しない.
問題はカスケードデータセットからや,時間に伴うパラメータ,カスケードごとのAspectを推定する事.
推定
まず,カスケードの Aspect k において,ノード i がノード j に感染させる確率を
とする.は時間差で,は拡散パターンを示すパラメータ.については指数,power-law,レイリー分布の三つを考える.すなわち,時間差とノード間の強さの二つのパラメータで情報拡散をモデル化する.
あとは尤度の式が書ければEMでパラメータ推定が可能.尤度の式にの分布に従って survival function の項が入ってくるあたりに注意が必要.ひとつながりの連続時間を直接扱うモデルはこういう項が入ってくるのが正直めんどくさい.
[論文] Identifying and Labeling Search Tasks via Query-based Hawkes Processes (KDD 2014) 読んだ
Identifying and Labeling Search Tasks via Query-based Hawkes Processes | Yahoo Labs
概要
タイムスタンプ付の検索クエリ集合から search task (同じ information need を満たすために入力されたクエリ集合) を特定する.
その際,点過程で用いられる Hawkes process と LDA を組み合わせることによってタスクを推定する.
手法
search task を推定するのは意外と厄介.以降 Figure 1を参照.
連続している2クエリが常に同 task であるとは限らない
むしろ連続していなくても同 task であることもある
ユーザごとにクエリを入れる頻度や間隔も異なる
まずはLDAをベースに手法を考えるが,既存のLDAベースの手法ではユーザごとに topic を学習するため,全ユーザでの傾向が反映できない.
提案手法では, influence という,あるクエリが別のクエリの入力を引き起こすような現象を考慮する.
具体的には,influence と クエリ-トピックの関係を潜在変数で表現する.
Query Co-occurrence and LDA
クエリ集合にLDAを適用してK個のトピックを抽出しようとすると,ユーザmに対するK次元ベクトルと,mのn回目のクエリに対するトピックを表すベクトルを推定するタスクになる.
しかし,このままYを(通常のLDAの枠組みのまま,つまりは時間情報を抜きにして)推定するのは困難.なぜならば,2つのクエリが同一トピックに属するかどうかは時間的な距離(間隔)に大きく依存するためである.
そうなると次に問題なのは,時間情報をどう扱うか,言い換えれば,LDAでの入力(「文書」と呼ばれる単位)の構築にどう利用するか,ということである.一番シンプルな方法はある時間幅でクエリ集合を分割することだけど,
異なる時間幅でセッションを分割する最適解は大抵存在しない
時間幅の重複を許すと,余分な情報が入ってしまうか,もしくは,短い時間間隔で存在する2クエリが別タスクに分割されてしまう
ユーザ固有のパターンを無視してしまうか,誤解してしまう
という問題がある.
提案手法では,クエリのペアにどのような influence の関係があるかを推定することによってこのような問題を解決する.influenceを用いる事によって,各ユーザの通常のパターンからtaskにもとづくパターンを抽出することが可能になる.
Hawkes Process
Hawkes process は点過程の中でも self-exciting と呼ばれる現象を表現するモデル.これは,過去のイベントによってその発生確率が増加するモデルで地震などの分析に用いられている(一度地震が起こるとその後余震が引き起こされやすい).
Hawkes processではN(s)を過去に起こったイベントの回数とすると,あるinfinitesimal(無限に小さい)tにおけるイベントの rate function はで表現できる.はself-excitingを表現するカーネル.
このままシンプルにモデル化すると,全クエリペアで self-exciting が起こるモデルになるので,ユーザmのn番目とn'番目のクエリに influence が存在するかどうかを示す変数を導入して rate function を
とする.あとはを推定すればいい.
LDA-Hawkes
ここでLDAとHawkes processをつなげる.influenceを表現するを
とする.いってしまえば,二つのクエリが同じトピックを持っていれば influence が存在するとする.
こうするとLDAとHawkes process両方にメリットがある.
LDAにとっては,2つのクエリの関係性を考慮した上でトピックを表現することができる
Hawkes Process にとっては,通常 influence を考える場合には,時間情報を用いるだけでは全クエリ対についての influence を考えなければならない.これは非常に高コスト.LDAのトピックという概念を用いて推定することでこの候補を狭めることができる.
生成過程は以下のとおり.
トピック k について, V 次元のベクトル を Dirichlet から引く
ユーザ m について,K 次元のベクトル を Dirichlet から引く
ユーザ m の n 番目のクエリについて
トピック  を 多項分布  から引く
ユーザ m の n 番目のクエリに含まれる i 番目の単語を 多項分布  から引く
ユーザ m のクエリのタイムスタンプについて
と  を引く(どの分布から?)
を から求める
を Hawkes Process から求める
あとは mean-field variational Bayesian inference で推定.
[論文] Modeling Human Location Data with Mixtures of Kernel Densities (KDD 2014) 読んだ
概要
Modeling Human Location Data with Mixtures of Kernel Densities(pdf)
位置情報にもとづくデータから個人レベルの粒度にもとづく密度を推定する問題を解く.
混合モデルベースのカーネル密度推定(KDE)を応用して個人レベルの情報と全体の傾向を取り扱う.
問題設定
データはユーザiに関するチェックインデータ の tuple 集合 の集合.
密度を推定する問題を考える.
既存研究
混合ガウスによる推定の問題点
混合数が自明でない
データがスパース
そもそも環境的な要因と移動経路などによって密度が制限される -> ガウシアンにならない
カーネル密度推定(KDE)の問題点
次元の呪い -> 今回は d = 2 なので問題ない
予測時に全データ点を持ってないといけない -> もうメモリも高価じゃないから問題ない
提案手法
通常のKDE
は対角要素にhを持つ2x2行列.
hはバンド幅と呼ばれるパラメータ.大きくしたり小さくしたりすることによってどれぐらいの幅でカーネルを当てはめるかが決まる.
Adaptive bandwidth method
バンド幅 h を入力データの近傍 k 番目の点とのユークリッド距離として,データ点ごとに h を変える.実験では k = 5 が一番良い.
Mixture of kernel density models
個人レベルのKDE
もっと荒いKDE
の二つを混ぜる.混ぜ方は
という感じで推定に使うデータを変えつつそれぞれの重みを推定する.はそのユーザの全データ,はユーザに限らない全データ,の場合は適当に決める(C=3の場合のc=2は例えば地域を9x9の81gridsに区切ってそのユーザのデータが最も含まれるgridにする).
学習は training data から validation data を分けてパラメータ推定に使う.
[論文] Product selection problem: improve market share by learning consumer behavior (KDD 2014) 読んだ
Product selection problem
KDD2014も終わったので興味がある論文を少しずつ読んでいく.
概要
属性付の商品データ,自社製品,他社製品が市場に存在するとする.自社の新商品集合について,どれを出すのがもっともシェアを奪えるかを推定する.
手法
理解があやふやなため,途中から未知の記号が出てくることや,そもそも(実データを使っているが)推定と呼べるのかどうか,が分かっていない.
間違った理解をしているとしか思えないが,とりあえず書いていく.
記号・前提
消費者集合,商品集合が存在する.商品集合は自社商品集合と他社商品集合で構成されている.
各商品には,d次元の属性ベクトルが紐付いている.同時に,各消費者も要件ベクトルを持っている.
商品選択モデル
消費者の商品選択モデル (consumer adoptation model) は次の三段階(論文では二段階で書かれているがわかりにくい)で行われる.
まず,消費者は属性の値全てが要件ベクトルの値を上回る商品集合を全商品の中から取得する
次に,の中からある距離尺度が最大になる商品集合を抽出する.
そもそもこの時点で商品が一意に決まるのではないか
その後,が商品を選ぶ確率とする,つまりはからランダムに一つ選ぶ
距離尺度dは
すなわち,上回っている要素がスコアになる.
は次の4つを考える.
DM: 要件が全て満たされていたら1を返す
NM: 全ての w = 1.0
PM: 価格に関する次元だけw=1にし,他は0にする
RM: 価格以外を1にし,価格を0にする
マーケットシェアの定義
全ユーザ,全商品に対して計算した商品選択確率の総和
distance metric learning
実シェアがわかっている時には,それぞれの距離尺度によるシェアの重み付総和が実シェアとなるので,そのように,すなわち距離尺度の選択確率を求めることができる.
top-1 algorithm
ここで,k-MMPというのを考える.これは新商品候補からk個の商品を市場に投入した時,自社のシェアが最大にあるような商品集合を指す.
まずはこれの k = 1 を考える.
擬似コードでアルゴリズムが示されているが,話は非常にシンプルで距離尺度,ユーザ,新商品候補ごとにマーケットシェアの増加分を計算して最大になるものを選ぶというもの.
わからないを列挙しておく.
全体の for 文 はユーザごとの i で囲まれているはずでは
salesというのは何かと思ったら関数ではなくて hashmap か
あとはこれの top-k を求めるのがNP-Hardだという主張につながる.なんでNP-Hardか理解していなかったけどこれある商品を投入すると他の商品で奪うことができるシェアが変化するから,という理屈だと思う.
実験
人口データと実データで実験.
実データの実験がかなり理解できない.
we do not have the information about products' real-world market share and consumers' adoption models
という話からはじまっていて,適当に決めたからデータを作ってそれを推定するとうまくいく,という話になっていくけど,ここで説かれている問題は制約付きの線形回帰なんだし,そもそも問題としての難易度が高いとは思えない.その上,適当に決めたデータから作ったデータを推定しているというのは前節で行っていた人工データによる実験とどう違うのかが分からない.
6.2 の impact of distance metrics においても,距離尺度が違えば追加される新商品も変わってくる,と著者らは主張しているが,そもそも距離尺度が変われば出てくる結果は当然変わるだろうし(例えば価格だけを見る指標なら安い商品しか出てこない),精度評価などを度外視した話なのでその結果に対して良し悪しを言うことができないと思う.この実験結果がどういう意味を持っているのかよくわからない.
タイトルに Learning Consumer Behavior と入っているので推定を行っているのかと思ったら,そもそも推定を行っておらず(実データでの実験において消費者の属性に対する選好はデータ中に存在するレビューの評点を用いている),論文の主となっているk-MMPの探索も効率的に発見するという話で(良い悪いの話ではなく)パラメータ推定を行っている論文では無いように思える.そもそも,実データを用いたとしても新商品でシェアがどの程度変化するかという話は実験上確かめられないと思う.例えば,購買履歴をある時間で区切って,それ以降に登場した新商品を使ってそれがどの程度のシェアを得るか,というのを推定するなら話はわかるけれど,本論でそのような設定の実験は行われていない.
モデルもかなりナイーブに作られているので(消費者の選好に対する重み付けが共通なことや,そもそもの重みの付け方がその4つしか存在しないのかということ),きっと論文の主眼はNP-Hardな探索が効率的にできるという話なのだろう.
最後まですっきりしなかったので,何か根本的に理解がおかしい.
[論文] The Dynamics of Repeat Consumption(WWW 2014) 読んだ
The Dynamics of Repeat Consumption(pdf)
概要
同じ商品を繰り返し買う行動はいかにして起こるのか? の論文
結論としては「繰り返し買われるものは,最近買われた商品」の一言.
データセット
youtubeの再生履歴,google+のチェックイン履歴,Brightkiteのチェックイン履歴,wikipediaの閲覧履歴,YES.comの再生履歴,シェークスピア(?)など.
基礎分析
データに関する統計.重要なのは
ユーザは人気のアイテムを消費している
Satiation(ユーザがあるアイテムを飽きること)は今回は観測されていない
repeat consumingの割合を見ると程度の差こそあれ,全てのデータセットで現れている事がわかる
「分布を見るとパラメータの異なるポアソン分布に似ている」とあるが後半での利用無し
よくよく考えてみると,リピートの傾向が2つの峰を持つという事がそもそも考えにくいのでこのような形になるのは当然と言えば当然
recensyに関する話,直近のK回の商品がK+1回目に含まれている率をhit ratioとしてplot
この hit ratio に色々と補正をしていくけれども,hit ratioのupper boundが1 - ユニークな商品数/総履歴数というのがわからない.両方1だと0になる.
モデル
Quality model
商品そのものだけでリピートするかしないかを決めるモデル.商品eを選択する確率はに比例する.確率値にするにはsoftmaxを取る.
Recency model
重みに従い,t回前にその商品を買っていたら重みを足す.例えば,ある商品を3回前,4回前,11回前に買っていたら値はとなる.極めてシンプルなモデル.確率値にするにはsoftmaxを取る.
Hybrid model
Quality と Recency の積
Tipping behavior
recency model の について,が全てについて成立するとき,ある段階からユーザは同じ商品しか買わなくなる.その証明
実験
上述したデータセットを使った実験.
sとwの推定方法を変え複数パターンで実験している.
もっとも対数尤度が良かったものを1とし,それからどれぐらい変化したかでしか記述されていない.何故オリジナルの値が無いのか.
s/wともにlearnedが1.0/1.0として,sがuniform/wがlearnedで0.9ぐらいの精度が出ている.recencyだけが重要で,Qualityはほとんど寄与していないと見るのが妥当と思われる.
YES.com による実験結果が記述されていない.何故載せられていないのかが理解できない
恐らく, s と w の学習結果が彼らの論文にそぐわない形であったため省かれている
「多くのデータセットで普遍的に発生する現象だ」という主張が根底にあったはずなので,ちょっと納得がいかない
後段では「PLECOというモデルではうまくいかなかった」とか触れていてここもアンバランス
ADDITIVITY
ちょっと何を言っているか理解できなかった.
『*[論文]』の検索結果 - 糞ネット弁慶
