高精度音素認識と音声合成を共通のモデルから実現することを目指して,調音特徴(AF)に基づくワンモデル音声認識合成方式を検討している.本報告では,これまでのmonophoneモデルをtriphoneモデルに拡張し,MFCCとAFの音素認識性能を比較する.AF系列は,振幅変化がカテゴリカル,すなわち離散的に変化する傾向を持つため,混合数の増加に対する性能向上が少ない.そこで,状態数を増加させることにより,時間方向にモデルを精緻化することを検討する.実験結果から,状態数を増加させたことで,正解精度が大きく向上することを示す.同時に,triphoneモデルにおいもAFは,MFCCに対して優位であることが確認される.音声合成では,CELP方式の閉ループを利用し,合成による分析を通して残差素片の符号帳をHMMの状態に割当てると共に,LSP合成器を駆動する等の改良を行う.これにより,合成音声の主観評価結果が向上することを示す.
Speech recognition (SR) and speech synthesis (SS) based on one-model of articulatory movement HMMs that are commonly applied to both an SR module and an SS module are described. The SR module has an articulatory feature (AF) extractor with multi-layer neural networks (MLNs) that output an AF sequence to HMMs. In the SS module, the speaker-invariant HMMs are applied to generate an articulatory feature (AF) sequence, and then, after converting AFs into vocal tract parameters by using a multi-layer neural network (MLN), a speech signal is synthesized by an LSP (Line Spectrum Pairs) digital filter. CELP coding technique is applied to improve sound quality when generating voice source from embedded codes in the corresponding state of HMMs. The proposed speech synthesis system separate phonetic information and speaker individuality. Therefore, target speaker's voice can be synthesized with a small amount of speech data. The experimental results show that the proposed system can produce good quality speech with only two-sentences.

