 Googleは28日、ウェブマスター向けガイドラインを一部更新した。新しいガイドラインでは、ページ内で使用しているJavaScriptやCSS、画像ファイルに、Googlebot(Googleのクローラー)がアクセスできるよう、robots.txtでこれらのファイルへのアクセスを拒否しないことを推奨している。
Googleでは、現在のインデックス登録システムでは、一般的な最新のブラウザーと同様にウェブページをレンダリングしていると説明。robots.txtを使ってサイト内のJavaScriptやCSS、画像ファイルのクロールを拒否するように設定すると、Googleのアルゴリズムによるコンテンツのレンダリングとインデックス登録を妨げ、最適な結果が得られなくなる可能性があるとしている。
注意点としては、Googleのレンダリングエンジンもすべての技術に対応しているとは限らないため、未サポートのブラウザーやシステムにも基本的なコンテンツや機能を提供する「プログレッシブエンハンスメント」の考え方に基づいてページをデザインすることや、レンダリングを高速化するためにページのパフォーマンスを最適化すること、JavaScriptやCSSファイルをGooglebotに配信する際の負荷をサーバーが処理できるようにすることを挙げている。
また、ウェブマスターツールの「Fetch as Google」機能も更新。この機能により、Googleのシステムがページをどのようにレンダリングしているかを把握できるようになり、robots.txtによる不適切な制限やGooglebotが対応できないリダイレクトなど、インデックス登録に関する多くの問題を特定できる。
JavaScriptやCSSもクロール対象に、Googleがウェブマスター向けガイド更新 -INTERNET Watch
