今日は研究室のミーティングで統計的言語モデルの話をしました。
確率的言語モデルは与えられた文(や文書)sに対して確率p(s)を与えるモデルで、より正しそうな文に対して高い確率を与えます(実際は与えるようにモデル化する)。パラメーターは大量の文(加工されていない文)を利用して推定します。
言語モデルは音声認識、統計的機械翻訳、文字認識、情報検索(クエリーと対象文書の言語モデルのKL-divergenceを測る)などに使われています。多くの問題では入力fに対して、その出力の自然言語文eを求める問題でp(e|f)が最大となるeを求めるのですが、このp(e|f)を変形してp(e|f)=p(f|e)p(e)/p(f) として、p(e)の部分で言語モデルを利用します(p(f)はeを選ぶ際には関係ない)
よく使われている言語モデルはn-gramと呼ばれるモデルです。文sが単語列 w1...wm からなる時、p(s)はp(s) = p(w1...wn) = p(w1)p(w2|w1)p(w3|w1w2)p(w4|w1..w3)...p(wm|w1...wm-1) と直前の単語列で条件付けされた単語の生成確率の積に分解できます。n-gramは直前の単語列をn-1単語で打ち切ったモデルで、例えば2-gram modelはp(w1)p(w2|w1)p(w3|w2)...p(wm|wm-1) となります。n-gramを最尤推定を用いて、各パラメータをp(w2|w1) = c(w1w2) / c(w1) と推定する場合、過学習を起こすので、より短い履歴を利用するなどしてスムージングを行います。
#最近、最も優れているスムージングの一つのKneser-Ney法の意味づけがPitman-Yor過程を用いた階層型ベイズ言語モデルからできるという論文が出ました。(link)
他にも言語モデルはいろいろ提案されていて、直接パーシングを利用するもの (immediate head parsingを利用したものk)や、topicをベイズ統計の枠組みで捕らえたもの(言語処理年次大会チュートリアル)や、discriminativeなモデルを利用するもの(link)、決定木を組み合わせたもの(random forest)などがあります。
--
言語モデルはデータ圧縮とよく似ています。データ圧縮でも言語モデルと同様に与えられたデータd を最大の確率で表せるモデルを選び(学習)、そのモデルを用いてdを符号化します。この二つが使う理論的枠組みは同じですが、実用上は結構違います。
言語モデルは大量のデータを用いて巨大なモデルを作り、それを利用して未知の正しそうな文の確率が大きくなるようにします。モデル自体のサイズは関係ありません。
それに対し、データ圧縮ではこのモデル自体も復号時に必要となり、モデル自体と圧縮した後のデータの合計サイズが圧縮後のデータとなります。また、モデルの適用対象となるデータは、学習データそのものであり、モデルの記述サイズとのバランスを考えた上でぎりぎりまで過学習させた方がいいことになります。
データ圧縮ではこのモデルの記述量が無視できないため、大抵はモデルを符号/復号時に動的に構築するか(PPM法,LZ法)、モデルが短く記述できるようにデータを変換しています(BWT法は変換操作によってn-gram情報を位置情報に巧妙に変換しているとみることもできます)。
データ圧縮と学習理論の関係はmackay本が詳しいみたいです
言語モデル: DO++
