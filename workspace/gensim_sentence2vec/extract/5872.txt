
1.基本構造
2.パーセプトロン
3.Winner-Take-All
4.競合学習
5.バックプロパゲーション
1.基本構造
構造
脳の中には多数のニューロン(神経細胞)が存在しています.各ニューロンは,多数の他のニューロンから信号を受け取り,また,他の多数のニューロンへ信号を受け渡しています.脳は,この信号の流れによって,様々な情報処理を行っています.
この仕組みをコンピュータ内に実現しようとしたものがニューラルネットワークです.下の図は,ニューラルネットワークの典型的な構造を示したものです.これらの図において,円や四角は一つのニューロンに対応し,また,矢印が付いた枝は信号の流れを表しています.
左の図は,相互結合ネットワークを表し,また,中央と右の図は,階層的なネットワークを表しています.四角で書かれたユニットは,入力ユニットと呼ばれ,外部からの信号を受け取ります.また,赤いユニットは,出力ユニットと呼ばれ,外部へ信号を出力します.なお,相互結合ネットワークの場合は,すべてのユニットが,入力ユニットと出力ユニットを兼ねるケースが多く見受けられます.
中央及び右図のように,多層構造になっている場合,入力ユニットから構成されている層を入力層,また,出力ユニットから構成されている層を出力層と呼びます.右図の中間にあるユニット(緑色のユニット)は,隠れユニットと呼ばれ,その層は,隠れ層(中間層)と呼ばれます.
ユニット
あるユニット ui を拡大すると,右図のようになります.ユニット ui は,複数のユニット uj ( j = 1, 2, ・・・, n )から情報を入力し,何らかの処理をした後,情報を出力します.
ユニットの入力及び出力値の記述方法は,ネットワークの種類によって異なりますが,通常,以下に示すいずれかの方法によります.
離散値の場合: {0, 1} or {-1, 0, 1}
連続値の場合: [0, 1] or [-1, 1]
各ユニットにおける処理は以下のようになります.まず,入力値を元に,以下の値を計算します.
ここで,
bi : ユニット ui のバイアス
wi,j : ユニット uj からユニット ui に与える影響の強さを表し,重みと呼ばれる
uj : ユニット uj の出力値
とします.また,図の赤い部分で示したように,出力値の強さが常に 1 である仮想的なユニット u0 を仮定し,ユニット u0 からユニット ui への重み wi,0 を bi とみなすことによって,(1) 式は,以下のようにも記述できます.
(1) または (2) 式によって計算された Si をもとに,ユニット ui の出力値が以下のようにして計算されます.
ui = f(Si)
ここで,関数 f(x) として,例えば,以下のようなものが使用されます.
学習
ニューラルネットワークを作成しただけでは,入力ユニットにデータを与えても,出力ユニットから出る値が適切なものである可能性はほとんどありません.何らかの方法で,ユニット間を繋ぐ枝の重み(バイアスも含む)を調整し,希望する出力が得られるようにしてやる必要があります.このことを行うのが,学習です.
学習方法には,大きく分けて 2 つの方法があります.それは,教師付き学習と教師無し学習です.教師付き学習は,幾つかの学習例と各学習例に対する目標出力を与え,目標出力と実際の出力が一致するように重みを調整する方法です.また,教師無し学習では,学習例からコンピュータ自身が何らかの基準に基づき重みを調整します.
また,この重みの調整には,学習例から 1 回の計算だけで行う方法と,学習例を繰り返し入力し,徐々に調整していく方法とがあります.
2.パーセプトロン  C/C++ 及び Java によるプログラム例
まず,最初に,最も古典的なネットワークであるパーセプトロンについて説明します.
構造
パーセプトロンは,右図に示すように,入力層と出力層だけから成る 2 層のネットワークです.また,入力ユニットの数(p 個)は任意ですが,出力ユニット( p+1 番目が出力ユニット)は 1 つだけです.
今,
重み: w = [wp+1,0, wp+1,1, wp+1,2, ・・・, wp+1,p]T
k 番目の訓練例: ek = [u0, u1, u2, ・・・, up]T  k = 1, 2, ・・・, N   u0 は,常に 1
k 番目の訓練例に対する目標出力: ck = +1 or -1  k = 1, 2, ・・・, N
とすると,出力(ユニット up+1 の出力) o は,以下のようにして計算されます.
S = up+1 = wTek   (3)
o = f(S) = +1  for S > 0
= 0  for S = 0
= -1  for S 
パーセプトロンは,各訓練例に対してその出力が目標出力に一致するように,重みを修正するネットワークです.つまり,与えられた訓練例を,目標出力が +1 か -1 によって,2 つのグループに分類することが目的となります.例えば,p = 2 の場合であれば,(3) 式は,
S = w3,0 + w3,1u1 + w3,2u2
となります.u1,及び,u2 を x,および,y とおくと,パーセプトロンの目標は,訓練例を正しく分類するような直線,
0 = w3,0 + w3,1 x + w3,2 y
の係数 w3,0, w3,1,及び,w3,2 を求めることになります.図示すれば,右図のような直線を求めることになります.そこで,(3) 式のような関数を線形識別関数と呼びます.
以上の説明からも明らかなように,線形識別関数では分類できない場合が存在します.例えば,右図に示すような場合は,2 つのグループに分類する直線を引くことができません.分類しようとすれば,図に示すような曲線になってしまいます.
従って,パーセプトロンでは,
u1  u2  c
-1  -1  -1
-1  +1  +1
+1  -1  +1
+1  +1  +1
のような論理和をネットワークによって実現することは可能(分類する直線が引ける)ですが,
u1  u2  c
-1  -1  -1
-1  +1  +1
+1  -1  +1
+1  +1  -1
のような排他的論理和を実現することは不可能(分類する直線を引けない)となります.
学習方法
パーセプトロンの学習アルゴリズムは,以下のようになります.
w = 0 とする
ランダムに,訓練例 ek を選ぶ
もし,正しく分類されたら,つまり,
wTek > 0 , かつ, ck = +1 ,または,
wTek k = -1
であるなら,
もし,現在の重み w を使用し,続けて正しく分類した回数が,保存してある重み wold による回数より多く,かつ,w が,保存してある重み wold より多くの例を分類したならば,
保存してある重み wold を w で置き換え,続けて正しく分類した回数を記憶する
そうでなければ
以下の式で重みを修正し,かつ,続けて正しく分類した回数を 0 にする
w = w + ckek
B に戻る
例 入力ユニットの数が 2 であるパーセプトロンに対して,以下の学習例を使用して学習を行います.
e1 = [1 -1 -1 -1]  c1 = +1
e2 = [1  1 -1 -1]  c2 = -1
e3 = [1  1  1  1]  c3 = +1
このとき,例えば,以下のようにして学習が行われます.
繰返し  重み       例 結果  行動
1.   [0  0  0  0]  e1  no    w = w + e1
2.   [1 -1 -1 -1]  e2  no    w = w - e2
3.   [0 -2  0  0]  e1  yes   変更されない
4.   [0 -2  0  0]  e3  no    w = w + e3
5.   [1 -1  1  1]  e1  no    w = w + e1
6.   [2 -2  0  0]  e3  no    w = w + e3
7.   [3 -1  1  1]  e1  yes   変更されない
8.   [3 -1  1  1]  e2  no    w = w - e2
9.   [2 -2  2  2]  e1  no    w = w + e1
10.   [3 -3  1  1]
3.Winner-Take-All  C/C++ 及び Java によるプログラム例
Winner-Take-All ニューラルネットワークとは,出力ユニットの内,最大の出力を持つユニットだけが発火することによって,与えられたデータを分類しようとするものです.基本的に,パーセプトロンとほぼ同等のネットワークですが,出力ユニットが複数存在するため,重みの変更方法が多少異なっています.
パーセプトロンと同様,訓練例を正しく分類できた場合は重みの変更は行いません.しかし,主力ユニット cor が発火すべき k 番目の訓練例に対して,出力ユニット mis が発火したような場合は,以下に式に従って重みを修正します.
wcor = wcor + ek
wmis = wmis - ek
ただし,wi は,各入力ユニットから出力ユニット i へ向かう重みとします.
4.競合学習  C/C++ 及び Java によるプログラム例
競合学習モデルは,右図に示すように,入力層(ユニットの数: p )と出力層(ユニットの数: q )からだけからなるネットワークです.以下の式に従って,出力層にある各ユニットの出力(活性度)を計算し,最も大きな出力値を持つユニットの出力が,ネットワークの出力として外部に出て行きます.結局,与えられた入力を,その入力値によって q 個のパターンに分類することになります.
入力: {0, 1}
ui = 1  if Si > Sk for all k ≠ i  (ユニット i が勝ったとき)
= 0  その他
i = p+1, p+2, ・・・, p+q
学習は,パーセプトロンとは異なり,教師無しの方法で行われます.具体的な学習方法(重みの修正方法)は,以下に示すとおりです.
ただし,
ρ: パラメータ(0 uj: 入力ユニット j の値
m: 活性度が 1 である入力ユニットの数
とします.
5.バックプロパゲーション  C/C++ 及び Java によるプログラム例
バックプロパゲーションモデルは,右図に示すように,入力層,隠れ層,及び,出力層から成る多層のネットワークです.ある層のユニットから,それより上にある任意の層のユニットへ接続することは許されますが,同じ層にあるユニットや下の層にあるユニットへの接続は許されません.
学習方法の基本は以下に示すとおりです.訓練例(パターン) p に対する二乗誤差,
を最小にするように枝に付加された重みを学習によって修正します.ただし,
tpj: パターン p に対する出力ユニット j の目標出力
opj: パターン p に対する実際の出力ユニット j の出力
とします.
この記述のままでは,具体的な重みの修正方法が不明ですので,以下,それを導出してみます.まず,重みの修正量は,以下のように記述できます.
ただし,
opk: パターン p が与えられたときのユニット k の出力であり,ユニット k が入力ユニットの場合は,与えられた入力値,そうでない場合は,opk = fk(netpk) として計算される.ただし,k = 0 のときは,常に 1 とする.
wji: ユニット i から j へ向かう枝に付加された重み.ただし,i = 0 のときは,ユニット j のバイアスとする.
とします.(5) 式の 2 番目の微分は,
となりますので,
とおくと,(5) 式は,
となります.次に,δpj を計算します.δpj は,
のように書き表せます.この式の 2 番目の微分は,
となります.また,(7) 式の 1 番目の微分において,ユニット j が出力層である場合は,(4) 式より,
となります.従って,(7) 式は,
となります.また,ユニット j が隠れ層である場合は,
となりますので,(7) 式は,
となります.今,
とすると,
となります.従って,以上をまとめると,(7)式は,出力ユニットについては,
δpj = (tpj - opj) opj (1 - opj)
また,隠れユニットについては,
δpj = opj (1 - opj) Σ δpk wkj
となります.
以上の結果を,アルゴリズムの形でまとめると,以下のようになります(ⅠからⅣまでを,収束するまで繰り返す).このアルゴリズムからも明らかなように,ステップⅡにおいては,入力層から出力層に向かって各ユニットの出力を計算します.その結果,出力ユニットにおける目標出力との誤差が得られます.その後,ステップⅢ及びⅣにおいて,得られた誤差を下位の層に伝搬しながら重みを修正していきます.これが,バックプロパゲーションという名前の由来です.
もし,すべての重みを同じ値に設定し学習を開始すると,すべての重みは同じように変化し,非対象な解は得られません.そこで,重み(及び,バイアス)の初期値は,異なる値にしておく必要があります(通常,乱数を使用して小さな値を設定します).
パターン(訓練例) p を選択する
各ユニット j の出力 opj を計算する.ここで,opj は次の式によって計算される(入力層に対しては,opj = 入力値).
ただし,wji は,ユニット i から j へ向かう枝に付加された重みであり,i = 0 のときは,ユニット j のバイアスとする.
隠れユニット i (または,入力ユニット)から出力ユニット j に向かう枝に付加された重み wji を次式によって修正する.
Δwji(n+1) = η δpj opi + α Δwji(n)
ただし,
η: 学習係数(学習速度の調整)
α: 慣性係数(前回の重みの変化の影響を表す)
とし,また,δpj は出力ユニット j の誤差信号であり,
δpj = (tpj - opj) opj (1 - opj)
とする.
隠れユニット i (または,入力ユニット)から隠れユニット j に向かう枝に付加された重み wji 前式によって修正する.ただし,
δpj = opj (1 - opj) Σ δpk wkj
とする.ここで,
δpk: ユニット j が出力を送っているユニット k の誤差信号
wkj: ユニット j から k に向かう枝に付加された重み
である.
静岡理工科大学
菅沼ホーム
目次
索引
ニューラルネットワーク
