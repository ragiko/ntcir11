話者識別において話者モデルとしてガウス混合分布(GMM)が広く用いられている.これはGMMが複雑な話者の特徴を確率分布として表現でき,EMアルゴリズムでモデルのパラメータを推定できるからである.しかし,モデルを推定するための訓練データが少ない場合や,訓練データに対して混合数が多い場合は過学習を起こすという問題がある.また,話者モデルとしてGMMを用いる場合,各ガウス分布が音響特徴が類似した音声の一部とそれぞれ対応することが望ましい.しかし,最尤推定でパラメータを推定した場合は,必ずしも対応関係があるとはいえない.そこでクラスタリングに基づいたGMM学習法を提案する.これはクラスタリングを使用することで,ある一部の音声と各ガウス分布との対応関係を明確にし,それぞれの分布に属する特微量を調整することで過学習を防ぐ.本論文では,この提案方法によりパラメータを推定したGMMと,最尤推定でパラメータを推定したGMMの話者識別率を比較した.その結果,提案方法で推定したGMMは最尤推定でパラメータを推定したGMMに比べて最大11.6%精度の改善が得られた.
In the speaker identification research fields, Gaussian Mixture Models (GMM) are widely used as speaker models because characteristics of the speaker can be represented by using many Gaussians, and parameters of GMM can be estimated automatically by using the EM algorithm. However, there is a overfitting problem when the number of training samples is small, or a number of parameters should be estimated. In general, a speaker model represents many kinds of speech. Therefore, it seems to be natural that each Gaussian in a GMM corresponds to each part of speech, such as phoneme, words, and other kinds of clusters. However, we cannot find any correspondence between Gaussians and speech data.

