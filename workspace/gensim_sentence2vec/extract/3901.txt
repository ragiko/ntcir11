  
　しかし、特許文献１に記載された発明では、会話の内容がある程度長ければ話題を特定できるが、会話が短いと、提供すべき話題を決めることが難しいという問題がある。特に、音声翻訳を用いた会話のように、発話が非常に短くなる場合には、特許文献１に記載された発明を適用することはきわめて難しい。
　さらに、音声翻訳において特有の問題として、どのような発話をすると正しく音声認識され、正しく翻訳されるかを話者が考えすぎてしまうという問題がある。そのために何をどのように発話したらよいか迷ってしまい、結果として音声翻訳の精度を下げてしまうことも多い。もちろん、ユーザが音声認識しやすく、自動翻訳しやすい発話をすれば、音声翻訳にとっては好ましい。しかしそのために発話自体を難しくしてしまうのでは音声翻訳の効果を十分に生かすことができない。
　それゆえにこの発明は、音声認識の精度を高めることができ、かつ、ユーザが利用しやすい音声処理システム、及びそうした音声処理システムで利用される端末装置を提供することである。
　この発明の別の目的は、音声認識の精度と、音声認識の結果を用いて提供される音声サービスの精度とを高めることができ、かつ、ユーザが利用しやすい音声処理システム、及びそうした音声処理システムで利用される端末装置を提供することである。
　本発明の第１の局面に係る音声処理システムは、発話を表す音声信号と、当該発話がなされた環境を表す所定の環境情報とを含む発話情報の入力を受けるための発話入力手段と、発話入力手段が受けた発話情報内の音声信号に対して音声認識を行ない、認識結果をテキストとして出力するための音声認識手段と、音声認識手段が出力したテキストに対して所定のデータ処理を実行するデータ処理手段と、発話のテキストと、所定の環境情報とを受けると、所定の発話集合の中の発話について、テキストにより表される発話の後に連接して発話される確率が算出可能なように統計的に学習済の発話連接モデルを記憶するための発話連接モデル記憶手段と、所定の発話集合内の発話と、当該発話集合内の発話の各々について、データ処理手段で処理したときの、データ処理の信頼度を記憶するための発話記憶手段と、発話入力手段が受けた発話情報に対する音声認識手段の認識結果と、当該発話情報に含まれる環境情報とを用い、発話連接モデル記憶手段に記憶された発話連接モデルを用いて所定の集合内の各発話に対して算出される確率と、発話記憶手段に記憶された、所定の集合内の各発話に対するデータ処理の信頼度とを所定の形式で組合わせた評価スコアに基づいて、音声認識手段が認識した発話を行なったユーザにリコメンドする発話の候補を複数の発話の集合内でスコア付けし、当該スコアに基づいて発話候補をユーザにリコメンドするための発話候補リコメンド手段とを含む。
　発話入力手段が発話情報の入力を受けると、音声認識手段が発話情報中の音声信号に対する音声認識を行ない、認識結果のテキストを出力する。データ処理手段は、このテキストに対してデータ処理を行なう。認識結果のテキストと、発話情報に含まれていた環境情報とを用い、発話候補リコメンド手段が、発話連接モデルにより各発話が次の発話となる確率を算出し、さらに、この確率と、データ処理における各発話の信頼度とを用いて次の発話としてリコメンドする発話のスコア付けを行ない、その結果に従って次の発話候補をリコメンドする。
　次の発話の候補がリコメンドされるので、ユーザはこの発話の候補をヒントとして次の発話を考えることができる。そのため、発話が思い浮かばずユーザが戸惑ってしまうという事態が発生する危険性を低くできる。
　好ましくは、データ処理手段は、ある発話を受けて音声認識手段の出力する認識結果を受け、当該認識結果をある発話の言語と異なる他の言語に自動翻訳しテキストとして出力するための自動翻訳手段を含む。信頼度は、当該自動翻訳手段による翻訳結果がある発話に対する他の言語の翻訳である尤度である。
　より好ましくは、データ処理手段はさらに、自動翻訳手段が出力する他の言語のテキストに基づき、他の言語の音声信号を合成するための音声合成手段を含む。
　発話候補リコメンド手段は、発話連接モデルを用いて所定の集合内の各発話に対して算出される確率と、発話記憶手段に記憶された、所定の集合内の各発話に対する信頼度との線形和による評価に基づいて、音声認識手段が音声認識した発話に後続する発話の候補を複数の発話の集合内で推定するための手段を含んでもよい。この場合、線形和における信頼度と確率との係数はいずれも正である。
　好ましくは、音声処理システムはさらに、発話候補リコメンド手段によりリコメンドされた次の発話候補をユーザに対して提示するための発話候補提示手段を含む。
　より好ましくは、音声処理システムは、発話を表すテキストと、所定の環境情報とを含む発話テキスト情報の入力を受け、当該発話テキスト情報内のテキストを音声認識手段の出力に代えて発話候補リコメンド手段及びデータ処理手段に与えるための発話テキスト情報入力手段をさらに含む。
　本発明の第２の局面に係る端末装置は、マイクロフォンと、周囲の環境に関する情報を収集するためのセンサの集合と、表示装置と、通信装置と、マイクロフォン、センサの集合、及び通信装置に接続され、マイクロフォンが発話を受けて出力する信号から得られる音声信号と、当該音声信号が得られたときのセンサの集合から得られた情報とを含む発話情報を通信装置を介して所定の音声処理サーバに送信し、音声認識と認識結果に対する所定のデータ処理とを依頼するための発話情報送信手段とを含む。端末装置はさらに、通信装置に接続され、依頼に応答して音声処理サーバから送信されてくるデータ処理の処理結果を受けて、当該処理結果をユーザに提示するための処理結果提示手段と、音声処理サーバから複数の発話候補としてリコメンドされる発話候補リコメンドリストを受けて、表示装置に表示することによってユーザに発話候補をリコメンドするための発話候補リコメンドリスト表示手段とを含む。
　好ましくは、音声処理サーバが音声認識の結果に対して行なう所定のデータ処理は、発話を、当該発話の言語と異なる他の言語に自動翻訳し、さらに、当該自動翻訳の結果の音声を合成する処理である。音声処理サーバから送信されるデータ処理の処理結果は、音声処理サーバにより合成された音声を表す音声信号である。処理結果提示手段は、スピーカと、音声処理サーバにより合成された音声を表す音声信号でスピーカを駆動するための手段とを含む。
　さらに好ましくは、端末装置はさらに、発話候補リコメンドリスト表示手段により表示された発話の候補のいずれかを選択するためにユーザが操作可能な選択手段と、選択手段により発話候補リコメンドリスト内の発話候補のいずれかが選択されたことに応答して、当該選択された発話候補のテキストと、センサの集合から得られた情報とを含む発話テキスト情報を通信装置を介して所定の音声処理サーバに送信し、当該発話テキスト情報に対する所定のデータ処理を依頼するための発話テキスト情報送信手段とを含む。
　以上のように本発明によれば、音声サービスを利用するユーザが、何を発話するかについて思いつかずに戸惑ってしまう、という可能性を小さくすることができ、音声サービスをより使いやすくできる。しかも、発話を思いつくまでに発する無意味な音声が少なくなり、音声認識の精度を高めることができ、当該音声認識の結果を用いる音声サービスの精度も高めることができる。
　その結果、音声認識の精度を高めることができ、かつ、ユーザが利用しやすい音声処理装置及び端末装置を提供できる。
　さらに、音声認識の精度と、音声認識の結果を用いる音声サービスの精度とを高めることができ、かつ、ユーザが利用しやすい音声処理装置及び端末装置を提供できる。
本発明の第１の実施の形態に係る音声翻訳システムの全体構成を模式的に示す図である。 図１に示すシステムで用いられる携帯型端末の画面に表示される音声翻訳のための画面を模式的に示す図である。 第１の実施の形態の音声翻訳システムにおいて、携帯型端末とサーバとの間で行なわれる音声翻訳のための処理シーケンスを示す図である。 携帯型端末のハードウェア構成を示すブロック図である。 携帯型端末における音声入力を用いた音声翻訳に伴う処理を実現するプログラムの制御構造を示すフローチャートである。 第１の実施の形態の音声翻訳システムにおいて、複数の携帯型端末からの音声入力を受けて指定された言語に翻訳し、さらにその音声を合成して携帯型端末に返送する処理を実行するサーバの機能的ブロック図である。 図６に示すサーバの機能を実現するためのプログラムのフローチャートである。 図６に示すサーバにおいて用いられる発話連接モデルの学習と、コーパス内の発話の翻訳スコアの算出とを行なうモデル生成部のブロック図である。 
　以下の説明及び図面では、同一の部品には同一の参照番号を付してある。したがって、それらについての詳細な説明は繰返さない。
　［第１の実施の形態］
　〈構成〉
　─全体構成─
　図１を参照して、この発明に係る音声翻訳システム１００は、インターネット１０２に接続された、音声翻訳サービスを行なうサーバ１０６と、インターネット１０２に接続可能で、音声翻訳サービスを利用するためのアプリケーションがインストールされた携帯型端末１０４とを含む。
　─アプリケーション画面─
　図２を参照して、携帯型端末１０４の音声翻訳サービスを利用するためのアプリケーション画面１３０は、大きく分けて６つの領域に分割されている。すなわち、音声翻訳サービスの対象となっている言語の対（ソース言語とターゲット言語）を表示するための言語表示領域１４０と、ソース言語の音声で入力された文の音声認識結果、又はテキスト入力結果を表示するための入力テキスト表示領域１５０と、音声認識された文を自動翻訳した結果のテキストが表示される翻訳結果表示領域１７０、翻訳結果を元の言語に逆翻訳した文を表示する逆翻訳領域１６０、次の発話候補としてリコメンドされる発話候補のリスト（発話候補リコメンドリスト）が表示される発話候補リコメンド領域１９０、及び音声翻訳システムの利用状況を表示するステータス領域１８０である。
　言語表示領域１４０には、ソース言語の言語名が左側に、ターゲット言語の言語名が右側に、それぞれソース言語の文字で表示される。なお、アプリケーション画面１３０では、翻訳結果の文以外のテキストはいずれもソース言語の文字で表示される。ソース及びターゲット言語名の間には、音声翻訳の言語の組合せを設定するための設定ボタン１４２が表示される。
　入力テキスト表示領域１５０には、ソース言語の言語名の表示１５６と、音声入力を行なう際にユーザが操作する音声入力ボタン１５２と、音声入力ではなく、入力文のテキストを直接に入力するテキスト入力画面を表示させるためのテキスト入力ボタン１５４とが表示される。音声入力の結果、及びテキスト入力の結果は、いずれも入力テキスト表示領域１５０内に入力テキスト１５８として表示される。なお、本実施の形態では、音声入力ボタン１５２を押している間、音声が収録される。音声入力ボタン１５２を押すのを中断すると音声の収録が終了する。
　逆翻訳領域１６０には、音声入力の結果から生成され、自動翻訳されたターゲット言語の文を、ソース言語の文に逆翻訳した結果の文１６２と、文をテキスト入力したとき等に、その文の翻訳を開始させるための翻訳ボタン１６４とが表示される。入力されたソース言語の文から得られた翻訳結果をさらにソース言語の文に逆翻訳して逆翻訳領域１６０に表示することにより、翻訳が発話者の意図を正しく伝えるものか否かを判定できる。ただし、本実施の形態の説明では、実施の形態の説明を分かりやすくするため、この逆翻訳に関連する機能部分についての詳細は説明しない。
　翻訳結果表示領域１７０には、ターゲット言語の言語名１７４と、自動翻訳の結果の文（ターゲット言語の文）のテキスト１７６と、テキスト１７６の合成音声を再生させるための再生ボタン１７２とが表示される。音声翻訳の結果は自動的に合成音声として発話されるが、繰返して聞きたい場合に再生ボタン１７２を操作する。
　発話候補リコメンド領域１９０には、直前のユーザの発話に続いて発話される可能性が高く、かつ自動翻訳において受理の可能性が高い発話としてリコメンドされる発話からなる発話候補リコメンドリスト１９２と、発話候補リコメンドリスト１９２の各発話に対応して表示され、各発話についての翻訳リクエストを発生させるための翻訳リクエストボタン１９４とが表示される。
　ステータス領域１８０には，利用回数等のシステムの利用状況と、マイクボタン１８２とが表示される。マイクボタン１８２は、音声入力ボタン１５２と同様、録音を開始させる機能を持つが、音声入力ボタン１５２とは異なり、マイクボタン１８２を１度押して離すと音声の収録を開始し、再度マイクボタン１８２を押して離すと音声の収録を終了する。
　─音声翻訳のシーケンス─
　図３を参照して、音声翻訳システム１００を用いた音声翻訳の際の、携帯型端末１０４とサーバ１０６との間の典型的な通信シーケンスを説明する。最初に、携帯型端末１０４において音声入力２００を行ない、その音声と、音声翻訳の言語の組合せ等の情報と、センサの集合から得られた環境情報とを含む音声認識リクエスト２０２を生成する。音声認識リクエスト２０２は携帯型端末１０４からサーバ１０６に送信される。サーバ１０６は、この音声認識リクエストを受信すると音声認識処理２２０を行ない、音声認識結果のテキスト（本実施の形態では音声認識処理２２０は統計的音声認識処理を行なうものであり、認識に伴うスコアが最も高い仮説を１つだけ）を出力する。このテキストは自動翻訳処理２２２に入力として与えられる。自動翻訳処理２２２は、入力されたソース言語の文をターゲット言語に自動翻訳し、ターゲット言語の文を生成する。このターゲット言語の文は音声合成処理２２４に与えられる。音声合成処理２２４は与えられたターゲット言語の文から音声を合成する。
　一方、音声認識処理２２０の認識結果のテキストは、環境情報とともに発話候補リスト作成処理２３０にも与えられる。サーバ１０６は、発話の集合内の各発話について、与えられた発話の次に発話される確率を算出できるように予め学習済の統計的モデルである発話連接モデル２２６と、ソース言語の発話の、自動翻訳処理２２２における受理されやすさを表す翻訳スコアが付された発話の集合を記憶するための発話候補記憶部２２８とを含む。発話候補リスト作成処理２３０では、発話連接モデル２２６により発話集合内の各発話について算出される確率と、発話候補記憶部２２８に記憶された発話ごとの翻訳スコアとを用い、次に発話される可能性の高い発話であって、かつ翻訳スコアの高いものを所定個数選択し、リストとして出力する。本実施の形態では、この評価には、ある発話が次に発話される確率と、その発話の翻訳スコアとの線形和からなる評価スコアを用いる。この実施の形態の場合、評価スコアが大きいものが次候補にふさわしいものとする。したがって、この線形和の各係数はいずれも正である。この係数の値は、確率のオーダーと翻訳スコアのオーダーとにより異なってくるため、具体的な組合せに基づいて適宜定める必要がある。
　発話連接モデル２２６は、発話のテキストと、その発話がされたときの環境情報とを受けると、所定の発話集合の中の発話について、テキストにより表される発話の後に連接して発話される確率が算出可能なように統計的に学習済である。
　音声認識処理２２０の出力した音声認識結果のテキスト、自動翻訳処理２２２により得られた翻訳結果のテキスト、音声合成処理２２４により合成された合成音声データ、及び発話候補リスト作成処理２３０が生成した発話候補リストは、いずれもこれらを携帯型端末１０４に送信する送信処理部２３２に与えられる。送信処理部２３２は、与えられたこれらデータを送信のための所定のフォーマットに変換し、携帯型端末１０４に送信する。
　送信処理部２３２からのデータを受信した携帯型端末１０４は、受信した音声認識結果、自動翻訳結果、及び発話候補リストを表示する（ステップ２０４）。携帯型端末１０４はさらに、送信処理部２３２から受信した合成音声の発話を行なう（ステップ２０６）。発話後は、携帯型端末１０４は次の音声入力２００を待ち受ける状態に戻る。
　なお、前述したとおり、図３に示したのは典型的な処理シーケンスである。音声入力ではなくテキスト入力を行なった場合には、図３に示すものとは異なる処理シーケンスが実行される。
　─携帯型端末１０４─
　図４を参照して、携帯型端末１０４は、所定のプログラムを実行して携帯型端末１０４の各部を制御することにより、種々の機能を実現するためのプロセッサ２５０と、プロセッサ２５０が実行するプログラム、及びそのプログラムの実行に必要なデータを記憶し、プロセッサ２５０の作業領域としても機能するメモリ２５２と、プロセッサ２５０と後述する各種センサ等との間のインターフェイス２５４とを含む。以下に説明する構成要素は、いずれも、インターフェイス２５４を介してプロセッサ２５０と通信可能である。
　携帯型端末１０４はさらに、ＧＰＳ機能により携帯型端末１０４の位置の経度及び緯度情報を取得するためのＧＰＳ受信機２５８、携帯型端末１０４の３軸方向の加速度を検出するための加速度センサ２６０、携帯型端末１０４の３軸に関する傾きを検出するための傾きセンサ２６２、携帯型端末１０４の周囲の磁気を検出する磁気センサ２６４、携帯型端末１０４の周囲の明るさを検出する明るさセンサ２６６、携帯型端末１０４の所定位置に係る圧力を検知する圧力センサ２６８、及び携帯型端末１０４の周囲の温度を検出する温度センサ２７０等の多数のセンサと、マイクロフォン２５６と、無線通信により図示しない基地局を介してインターネット１０２に接続可能な通信装置２７２と、タッチパネル２７４と、タッチパネル２７４とは別に携帯型端末１０４の筐体に設けられた操作ボタン２７６と、スピーカ２８０とを含む。
　ＧＰＳ受信機２５８、加速度センサ２６０、傾きセンサ２６２、磁気センサ２６４、明るさセンサ２６６、圧力センサ２６８、及び温度センサ２７０等は、発話がされたときの環境を表す情報を収集するためのものである。本実施の形態では、音声入力がされたときのこれら各種センサの出力は、音声翻訳の言語対等の設定情報と、音声から得られたＡＤＰＣＭ音声信号とともに、発話時の環境を表す環境情報としてサーバ１０６に所定の形式で送信される。こうしてサーバ１０６に送信される情報は、発話音声に対する音声認識と、音声認識の結果に対するデータ処理である自動翻訳及び翻訳結果の音声合成とを依頼する音声翻訳リクエストである。
　携帯型端末１０４の機能を実現する各種プログラムのうち、音声翻訳サービスを利用するためのアプリケーションは、以下のような制御構造を持つ。図５を参照して、このプログラムが起動されると、メモリ領域の確保、各メモリロケーションを所定の初期値で初期化する初期設定処理を行なう（ステップ３００）。初期化完了後、携帯型端末１０４のタッチパネル２７４に音声翻訳サービスのための初期画面を表示する（ステップ３０２）。初期画面では、音声入力ボタン１５２、テキスト入力ボタン１５４、マイクボタン１８２、及び設定ボタン１４２は活性化されているが、翻訳ボタン１６４、及び再生ボタン１７２は無効化されている。
　続いてユーザからの入力を待ち、どのような入力がされたかにより制御の流れを分岐させる（ステップ３０４）。
　音声入力ボタン（図２の音声入力ボタン１５２）が押されると、音声入力処理を実行する（ステップ３１０）。音声入力処理は，音声入力のＡＰＩ（Ａｐｐｌｉｃａｔｉｏｎ　Ｐｒｏｇｒａｍｍｉｎｇ　Ｉｎｔｅｒｆａｃｅ）を呼び出すことにより行なわれる。続いて、入力された音声に対して所定の信号処理を行ない、ＡＤＰＣＭ（Ａｄａｐｔｉｖｅ　Ｄｉｆｆｅｒｅｎｔｉａｌ　Ｐｕｌｓｅ　Ｃｏｄｅ　Ｍｏｄｕｌａｔｉｏｎ）形式の音声信号を生成する（ステップ３１２）。さらに、この音声信号と、このときの各センサの出力と、翻訳言語等の設定情報とに基づいて、音声翻訳リクエストを生成し、サーバ１０６に対して送信する（ステップ３１４）。この後、サーバ１０６から音声認識結果、自動翻訳結果、その合成音声、発話候補のリストを受信し（ステップ３１６）、音声認識結果のテキスト、自動翻訳結果のテキスト、及び発話候補のリストをそれぞれ図２の入力テキスト表示領域１５０、逆翻訳領域１６０、及び翻訳結果表示領域１７０に表示するため、所定のメモリ領域に格納する（ステップ３１８）。さらに、自動翻訳結果の合成音声をスピーカ２８０を駆動して発生させる（ステップ３２０）。すなわち、スピーカ２８０を駆動することで、要求した発話の翻訳結果がユーザに対して音声の形で提示される。最後に、アプリケーション画面１３０の更新を行ない（ステップ３２２）、ステップ３０４の入力待ち状態に戻る。このとき、音声入力ボタン１５２、テキスト入力ボタン１５４、及びマイクボタン１８２に加え、再生ボタン１７２が活性化される。
　ステップ３０４でテキスト入力ボタン（図２のテキスト入力ボタン１５４）が押されると、テキスト入力のＡＰＩを呼ぶことにより、テキストの入力を受け（ステップ３４０）、入力されたテキストを保存し（ステップ３４２）、入力されたテキストが入力テキスト表示領域１５０及び逆翻訳領域１６０に表示されるように画面を更新して（ステップ３２２）、ステップ３０４に戻る。このとき、更新後の画面では翻訳ボタン１６４が活性化され、再生ボタン１７２は無効化される。
　ステップ３０４で翻訳ボタン（図２の翻訳ボタン１６４）が押されると、ステップ３４０で入力されステップ３４２で保存されたテキストと、そのときの各種センサの出力と、設定情報とを用いてテキスト翻訳リクエストを生成し、サーバ１０６に送信する（ステップ３６０）。続いてこのリクエストに対する翻訳結果と、翻訳結果の合成音声と、発話候補リコメンドリストとを受信する（ステップ３６２）。受信した翻訳結果のテキストを翻訳結果表示領域１７０（図２）に表示し（ステップ３６４）、翻訳結果の合成音声を発話する（ステップ３６６）。この後、アプリケーション画面１３０を更新し（ステップ３２２）、ステップ３０４に戻る。
　最後に、図２に示す設定ボタン１４２が操作されると、設定変更のために予め準備した画面を表示することで設定の変更を受付け（ステップ３８０）、設定が終了するとその設定結果をメモリ２５２に保存し（ステップ３８２）、変更後の設定にしたがってアプリケーション画面１３０を更新して（ステップ３２２）、制御をステップ３０４に戻す。
　─サーバ１０６─
　図６を参照して、サーバ１０６は以下のような機能的な構成を持つ。なお、サーバ１０６はハードウェアとしては大容量の外部記憶装置を持つコンピュータと、そのコンピュータ上で実行される音声翻訳サーバプログラムとにより実現される。
　サーバ１０６は、インターネット１０２（図１参照）を介して、携帯型端末１０４のような不特定多数の端末からの音声翻訳リクエスト及びテキスト翻訳リクエストを受信するための受信処理部４０６と、受信処理部４０６が受信したリクエストを解析し、解析結果に応じてデータを所定の機能モジュールに供給したり、データ選択のための制御信号を出力したりすることで音声翻訳の全体の制御をするための制御部４０８とを含む。サーバ１０６はさらに、複数のソース言語の音声認識を行なうため、及び、自動翻訳におけるターゲット言語の文の生成のために予め準備された、複数の言語に対してそれぞれ準備された言語別音声認識用リソース４００と、複数のソース言語と複数のターゲット言語との組合せごとに、自動翻訳のために予め準備された言語ペア別リソース４０２と、ターゲット言語の各々について音声合成をするために予め準備された言語別音声合成用リソース４０４とを含む。
　本実施の形態では、言語別音声認識用リソース４００は、音声認識のための各言語の統計的音響モデル、辞書、及び統計的言語モデルを含む。辞書及び言語モデルはその言語がターゲット言語となるときには自動翻訳においても用いられる。したがって、サーバ１０６はさらに、言語別音声認識用リソース４００の中から音声翻訳リクエストにより指定されるソース言語のリソースを選択する選択部４１０と、言語別音声認識用リソース４００の中から音声翻訳リクエストにより指定されるターゲット言語のリソースを選択する選択部４１２とを含む。
　言語ペア別リソース４０２は、ソース言語とターゲット言語との組合せごとに、予め準備された統計的翻訳モデルを含む。サーバ１０６はさらに、言語ペア別リソース４０２の中から、音声翻訳リクエストにより指定される言語ペアのリソースを選択する選択部４１４を含む。
　言語別音声合成用リソース４０４は、ターゲット言語の音声合成のために必要な言語別のリソースを含む。音声合成が素片接続型であれば、言語別の素片ＤＢがこのリソースに含まれる。サーバ１０６はさらに、言語別音声合成用リソース４０４の中から、音声翻訳リクエストにより指定されるターゲット言語のリソースを選択する選択部４１６を含む。
　制御部４０８は、音声翻訳リクエストに含まれるソース言語とターゲット言語との組合せに基づいて、選択部４１０、選択部４１２、選択部４１４及び選択部４１６に対し、それぞれ適切なリソースを選択するように制御信号を送る機能を持つ。
　サーバ１０６はさらに、制御部４０８から音声翻訳リクエスト中のＡＤＰＣＭデータを受け、選択部４１０により選択されたリソースを用い、ソース言語の音声認識を行なってそのテキストデータを出力する音声認識エンジン４１８と、音声認識エンジン４１８の出力するソース言語のテキストデータを受け、選択部４１２により選択されたターゲット言語のリソースと、選択部４１４により選択されたソース言語とターゲット言語との言語ペアに対応するリソースとを用いてソース言語の文をターゲット言語の文に翻訳し、テキストデータとして出力する自動翻訳エンジン４２２と、自動翻訳エンジン４２２の出力するターゲット言語のテキストデータを受け、選択部４１６により選択された言語リソースを用いて音声合成を行なうための音声合成処理部４２４とを含む。
　音声認識エンジン４１８の出力と自動翻訳エンジン４２２の入力との間には、２つの入力と、自動翻訳エンジン４２２の入力に接続された出力とを持つ選択部４２０が挿入されている。選択部４２０の一方入力には音声認識エンジン４１８の出力４３８が接続される。他方の入力には、制御部４０８から出力される、テキスト翻訳リクエスト中のテキストデータ４４０が入力される。前述したとおり、携帯型端末１０４では、音声入力だけでなく、テキストによる入力も行なわれる。音声入力の場合には音声認識エンジン４１８の出力４３８であるテキストデータが自動翻訳エンジン４２２に与えられるが、テキストによる入力だった場合には、音声認識エンジン４１８を経由せず、リクエスト中のテキストデータ４４０がそのまま自動翻訳エンジン４２２に与えられる。選択部４２０が２つの入力のいずれを選択するかは制御部４０８が音声翻訳リクエストの内容を見て切替信号４４２により制御する。テキスト翻訳リクエストに対する処理は、音声翻訳リクエストに対する処理と入力が異なるだけであり、自動翻訳以下の処理は音声翻訳リクエストの処理時と同じである。したがってここではテキスト翻訳リクエストに対する処理の詳細な説明は繰返さない。なお、音声認識エンジン４１８の出力するテキストデータにも、制御部４０８から直接に選択部４２０に与えられるテキストデータにも、音声翻訳リクエスト中の各種センサの出力値が付与されている。
　サーバ１０６はさらに、音声翻訳リクエスト中に付された各種センサの出力値を、次の発話候補を推定する際に使用する特徴量ベクトルの要素の値に変換するために使用される要素決定用各種ＤＢ４３０と、対象となる複数の言語に対して準備された複数の発話連接モデル２２６の記憶装置と、対象となる複数の言語に対して準備された複数の発話候補記憶部２２８と、制御部４０８の制御にしたがって、複数の発話連接モデル２２６及び複数の発話候補記憶部２２８から、発話言語に対応するモデル及び発話候補記憶部を選択し、発話確率算出部４２６及び発話候補リコメンドリスト作成部４２８に接続するための選択部４３４及び選択部４３６とを含む。サーバ１０６はさらに、選択部４２０の出力するテキストデータを受け、要素決定用各種ＤＢ４３０及び発話連接モデル２２６を用い、与えられた発話の次の発話となる確率を、予め準備された発話の各々について算出するための発話確率算出部４２６と、発話確率算出部４２６により各発話について算出された確率と、発話候補記憶部２２８に記憶された各発話の翻訳及び音声認識スコアとに基づいて、次に発話される可能性が高い発話であって、かつ自動翻訳エンジン４２２により受理される（正しく翻訳される）可能性が高い発話を、そのスコアの上位から複数個選択することによって、発話候補リコメンドリストを作成するための発話候補リコメンドリスト作成部４２８と、自動翻訳エンジン４２２の出力する翻訳結果のテキストデータ、音声合成処理部４２４が出力する合成音声、及び発話候補リコメンドリスト作成部４２８が生成した発話候補リコメンドリストとから所定のデータフォーマットの返信データを組立て、音声翻訳リクエストを送信してきた端末（携帯型端末１０４等）に返信するための送信処理部４３２とを含む。
　ところで、発話連接モデル２２６及び発話候補記憶部２２８については、予め準備しておく必要がある。そのためにモデル生成部１０８が設けられている。モデル生成部１０８の構成については後述する。
　図７を参照して、サーバ１０６を構成するコンピュータのハードウェアにより実行されることにより、図６に示す制御部４０８の機能を実現するためのプログラムは，以下のような制御構造を持つ。このプログラムが起動されると，まず，必要な記憶領域の確保及び初期化等、起動直後に１度だけ実行する必要がある処理を行なう初期設定ステップ４５０と、初期設定後に、インターネット１０２を介して他の端末装置から音声翻訳に関するリクエストを受信するのを待つステップ４５２と、ステップ４５２で受信したリクエストが何かによって制御の流れを分岐させるステップ４５４とを含む。
　リクエストが音声翻訳リクエストであると判定されると、リクエストに含まれる言語対の情報に基づき、ソース言語とターゲット言語とを設定し、図６に示す選択部４１０、選択部４１２、選択部４１４、及び選択部４１６を設定し（ステップ４６０）、音声認識を実行する（ステップ４６２）。音声認識が終了すると、その結果のソース言語のテキストを入力として、ターゲット言語への自動翻訳を行なう（ステップ４６４）。自動翻訳が終了すると、自動翻訳で得られた出力のテキストを入力とし、音声合成を行なう（ステップ４６６）。さらに、ステップ４６２で得られたテキストデータに基づき、次の発話を推定するための特徴量ベクトルを生成する（ステップ４６８）。この特徴量ベクトルの生成の際には、リクエストに含まれる生のセンサ出力を用いる場合もあるし、生のセンサ出力を要素決定用各種ＤＢ４３０（図６参照）を用いて他のカテゴリの値に変換する場合もある。例えば、ＧＰＳの出力は緯度及び経度の情報だが、これらをそのまま特徴量ベクトルの要素とすることも可能だし、緯度及び経度の情報と、その位置に存在する施設名又はその位置を含む地域名との対応関係を要素決定用各種ＤＢ４３０に記憶しておき、施設名又は地域名等に変換した後、変換後の値を特徴量ベクトルの要素としてもよい。生成された特徴量ベクトルを用い、図６に示す発話確率算出部４２６を用いて次に発話される確率の高い発話候補を所定個数推定し、さらに、図６に示す発話候補記憶部２２８を用い、各発話候補の確率と、それらの翻訳及び音声認識スコアとを組合わせた評価スコアにより、次の発話となる確率が高く、かつ翻訳スコアも高い発話候補を所定個数選択して発話候補リコメンドリストを作成する（ステップ４７０）。最後に、ステップ４６２で得られたソース言語の音声認識結果と、ステップ４６４で得られた自動翻訳結果のテキストと、ステップ４６６で得られた音声合成と、ステップ４７０で得られた発話候補リコメンドリストとを相手端末に送信し（ステップ４７２）、制御をステップ４５２に戻す。
　一方、ステップ４５４でリクエストがテキスト翻訳リクエストであると判定されると、リクエストに含まれる言語対の設定情報にしたがって、図６に示す選択部４１０、選択部４１２、選択部４１４、及び選択部４１６を設定し（ステップ４７８）、入力されたソース言語のテキストをターゲット言語に自動翻訳する（ステップ４８０）。得られたターゲット言語のテキストに基づいて、ターゲット言語の音声を合成し（ステップ４８２）、リクエストに含まれていたソース言語の入力テキストと環境情報とに基づき、特徴量ベクトルを生成して（ステップ４８４）、発話連接モデル２２６を参照することにより、次に発話される可能性が高く、かつ、翻訳スコアも高い発話候補からなる発話候補リコメンドリストを作成する（ステップ４８６）。最後に、ステップ４８０で得られたターゲット言語のテキストデータと、ステップ４８２で得られたターゲット言語の合成音声と、ステップ４８６で得られた発話候補とを携帯型端末に送信し（ステップ４８８）、制御をステップ４５２に戻す。
　以上が、図６に示すサーバ１０６を実現するプログラムの制御構造の概略である。
　図６に示す発話連接モデル２２６及び発話候補記憶部２２８に記憶される発話候補の集合は予め準備しておく必要がある。そのためのモデル生成部１０８の構成を図８に示す。なお、本実施の形態では、図６に示す要素決定用各種ＤＢ４３０として、ＧＰＳから得られる緯度・経度情報と、その緯度・経度情報により特定される国、地域、州、都府県、市、町等の地域情報との対応関係を記憶したＧＰＳ・地域情報変換ＤＢ５１８と、ＩＰアドレスと、そのＩＰアドレスが割当てられた施設名との対応関係を記憶したＩＰアドレス・施設名変換アドレスＤＢ５２２とを含むものとする。
　図８を参照して、モデル生成部１０８は、単言語の発話テキストを多数含むコーパス５１０を含む。コーパス５１０に記憶された発話はその発話の音声データと、発話の書起こしのテキストとを含む。書起こしテキストは、いずれも形態素等所定の単位に分割され、各単位には、ＤＡ（Ｄｉａｌｏｇ　Ａｃｔｉｖｉｔｙ）タグと呼ばれる、発話が行なわれたときの状況を示すタグが付されている。各発話にはさらに、その発話がされたときの日時、発話したユーザのユーザＩＤ又は端末ＩＤ、その発話が収集されたときのユーザの位置（ＧＰＳにより得られた緯度及び経度）、その発話を送信して来た端末のＩＰアドレス、端末の各センサが検知した加速度、傾き、磁気、明るさ、圧力、温度等の情報が付されている。これら情報のうち、対応するセンサがない等の事情により携帯型端末では得られなかった情報には、情報がないことを示す所定の値が代入される。
　モデル生成部１０８はさらに、コーパス５１０に含まれる発話データに対し、手動で形態素解析、タグ付等の処理を行なう際にユーザが使用する入力部５１２と、コーパス５１０に記憶されている各文から、発話連接モデル２２６の学習を行なうための学習データを生成するための基礎特徴ベクトルを作成する基礎特徴ベクトル作成部５１４とを含む。基礎特徴ベクトル作成部５１４は少なくともコーパス５１０に記憶された各文について、それら文に付された日時、ユーザＩＤ又は端末ＩＤ、及びその他の各種センサ情報を所定の順番で並べ、さらに、次に発話された発話の識別番号を要素としたベクトルを作成する。
　モデル生成部１０８はさらに、基礎特徴ベクトル作成部５１４により生成された各特徴ベクトルに含まれる緯度・経度情報をＧＰＳ・地域情報変換ＤＢ５１８と照合することにより、その特徴ベクトルに対応する発話がなされた国、地域、州、都府県、市、町等の地域情報を得て、特徴ベクトル中のしかるべき位置に挿入する処理を行なう地域情報付加部５１６と、地域情報付加部５１６から特徴ベクトルを受け、その中に含まれるＩＰアドレスをＩＰアドレス・施設名変換ＤＢ５２２と照合することにより、その発話がなされた施設名を得て、特徴ベクトル内のしかるべき位置に挿入する処理を行なう施設情報付加部５２０と、施設情報付加部５２０から出力される特徴ベクトルを蓄積するための特徴ベクトル記憶部５２６と、特徴ベクトル記憶部５２６に記憶された特徴ベクトルを学習データとして、発話連接モデル２２６の統計的学習を行なうための発話連接モデル学習部５２４とを含む。
　モデル生成部１０８はさらに、コーパス５１０に含まれる発話のうち、同じ発話を集約することにより、互いに異なる発話のみからなる集合を生成するための発話集約部５４０と、発話集約部５４０により集約された発話の各々について、複数の言語に翻訳を行ない、各発話について翻訳結果のスコアを出力する翻訳エンジン５４４と、翻訳エンジン５４４による複数の言語への翻訳により得られた翻訳結果のスコアを発話ごとに平均し、各発話の平均の翻訳スコアを算出し発話候補記憶部２２８に記憶させる翻訳スコア算出部５４２とを含む。翻訳エンジン５４４は、統計的な自動翻訳を行なうものが想定され、翻訳結果の尤度を翻訳スコアとする。翻訳スコアが高いほど、その元となったソース言語の発話が翻訳しやすいと考えることができる。
　モデル生成部１０８はさらに、コーパス５１０に含まれる各発話の音声データを音声認識する音声認識エンジン５４６と、コーパス５１０に含まれる各発話の書起こしデータと、その発話に対する音声認識エンジン５４６の認識結果とを比較することにより、各発話の音声認識スコアを算出し、各発話に付して発話候補記憶部２２８に記憶させるための音声認識スコア算出部５４８とを含む。
　なお、図８ではモデル生成部１０８は単一のものとして示してあるが、モデル生成部１０８は翻訳のソース言語ごとに生成する必要がある。翻訳エンジン５４４については、あるソース言語について利用可能な翻訳エンジンを全て用い、それらによる翻訳スコアを平均するようにしてもよいし、特定の複数の言語のみをターゲット言語として翻訳スコアを算出し、それらの平均を用いても良い。場合によってはある単一の言語のみをターゲット言語とする翻訳エンジン５４４を用いるようにしてもよい。
　〈動作〉
　─概要─
　この音声翻訳システム１００は以下のように動作する。サーバ１０６は、動作フェーズとして２つを持つ。第１はモデル生成部１０８による発話連接モデル２２６及び発話候補記憶部２２８の学習であり、第２は、学習が終了した発話連接モデル２２６及び発話候補記憶部２２８を用いた音声翻訳サービスの実行である。以下、最初に学習フェーズにおけるサーバ１０６の動作を、次に音声翻訳サービスのフェーズにおける携帯型端末１０４とサーバ１０６の動作とについて説明する。
　─学習─
発話連接モデル２２６及び発話候補記憶部２２８に記憶される各発話の翻訳及び音声認識スコアの学習を予め行なっておく必要がある。そのためにまず、処理の対象となる全ての言語について、別々に文を集めコーパス５１０を言語ごとに作成する。コーパス５１０内の各文については、予め形態素解析等が行なわれ、ＤＡタグの付与等が行なわれていれば好ましいが、必要に応じ、入力部５１２を用いてそうした処理を行なう。
　ある言語のコーパス５１０（音声データと書起こしテキストとの双方を含む。）に含まれる各文について、以下を行なう。すなわち、その文に付されている情報を用いて基礎特徴ベクトル作成部５１４（図８）により基礎的な特徴ベクトルを作成する。このとき、各発話に付されているユーザＩＤ及び発話の日時の情報に基づいて、ある発話の次にどの発話がされたかを特定し、特徴ベクトル内のしかるべき位置の要素に、次の発話を特定する情報を代入する。次に、各特徴ベクトルに含まれる緯度・経度情報をＧＰＳ・地域情報変換ＤＢ５１８と照合することにより、緯度・経度情報から国、地域、州、都府県、市、町等の地域情報を得て、特徴ベクトル中のしかるべき要素に代入する。対応する情報がない場合には、その情報がなかったことを示す特定の値をその要素に代入する（他の要素についても同様）。さらに、特徴ベクトルに含まれるＩＰアドレスをＩＰアドレス・施設名変換ＤＢ５２２と照合することにより、その発話を収録したＩＰアドレスを、そのＩＰアドレスに対応する施設名に変換し、特徴ベクトルのしかるべき要素に代入する。こうして特徴ベクトルを特徴ベクトル記憶部５２６に蓄積する。
　特徴ベクトル記憶部５２６への特徴ベクトルの蓄積が完了すると、又は蓄積と並行して、発話連接モデル学習部５２４が発話連接モデル２２６の統計的学習処理を実行する。この統計的学習により、発話日時、ユーザＩＤ、ＩＰアドレス、緯度・経度情報、各種センサの値を要素とする、ある発話の特徴ベクトルが与えられると、その発話の次に発話される確率を各発話について算出できるような発話連接モデル２２６が得られる。これら発話連接モデル２２６は、望ましくは不揮発性の記憶装置に記憶される。
　一方、発話集約部５４０は、コーパス５１０に含まれる各発話のうち、テキストとして同じ発話を集約する。翻訳スコア算出部５４２は、集約された各発話を翻訳エンジン５４４に与えることで、その発話の翻訳スコアを算出する。このとき、本実施の形態では、翻訳エンジン５４４として各ターゲット言語への翻訳を行なう統計的なものを用意し、それらにより得られる翻訳結果の尤度の平均をその発話の翻訳スコアとする。翻訳スコア算出部５４２は、各発話について得られた翻訳スコアからなる翻訳スコアＤＢを各ソース言語について作成し、発話候補記憶部２２８に格納する。以上で発話連接モデル２２６及び翻訳スコアＤＢの学習は終了する。さらに、音声認識エンジン５４６及び音声認識スコア算出部５４８により、コーパス５１０に含まれる各文の音声認識時の音声認識スコアが算出される。すなわち、音声認識エンジン５４６が各文の音声データに対する音声認識を行なう。音声認識スコア算出部５４８は、音声認識エンジン５４６による音声認識結果と、予めその文に付されていた書起こし文とを比較することにより、その文の音声認識スコアを算出する。この音声認識スコアは、コーパス５１０に格納されている各発話ごとに発話候補記憶部２２８に記憶される。この結果、発話候補記憶部２２８に記憶されている情報を用い、コーパス５１０に記憶された各発話の翻訳及び音声認識スコアを算出できる。
　─音声翻訳サービス─
　携帯型端末１０４等には、図２に示すような音声翻訳アプリケーションを予め配布しておくことが必要である。本実施の形態では、携帯型端末１０４が接続可能なサーバ１０６は、音声翻訳アプリケーションにより固定されているものとする。もちろん、サーバ１０６が複数個あるなら、ユーザがそれらの中から所望のものを選択するようにしてもよい。
　サーバ１０６の音声翻訳サービスを利用しようとする場合、ユーザには大きく分けて２つの選択肢がある。１番目は音声翻訳であり、２番目はテキスト翻訳である。以下、最初に音声翻訳サービスを利用するときのユーザの操作、並びに携帯型端末１０４及びサーバ１０６の動作を説明し、次にテキスト翻訳を利用するときのユーザの操作、並びに携帯型端末１０４及びサーバ１０６の動作を説明する。なおこれに先立ち、ユーザは、図２の設定ボタン１４２を操作することで設定画面を呼出し、自分が利用しようとするソース言語とターゲット言語との組合せを選択しておく必要がある。
　音声翻訳を行なおうとする場合、ユーザには２通りの方法がある。１番目は音声入力ボタン１５２を押し、押している間に発話をし、発話が終了したら音声入力ボタン１５２を離す、という方法である。２番目は、マイクボタン１８２を押すことにより音声の収録を開始させて発話を行ない、発話が終了したら再度マイクボタン１８２を押すことで音声の収録を終了させる、というものである。どちらを選択するにせよ、プログラムは、図５のステップ３１０及び３１２において、選択された処理に対応するＡＰＩを呼ぶことにより、音声の収録と、信号処理とを行なって所定形式の音声データを生成できる。
　音声の収録が終了すると（音声入力ボタン１５２が離されるか、音声の収録を実行中にマイクボタン１８２が再度押されると）、図５に示すステップ３１４の処理が行なわれ、音声翻訳のリクエスト命令と、設定にしたがった音声ペアの情報と、音声データと、環境情報とがサーバ１０６に送信される。このリクエストには、設定にしたがった言語ペアの情報と、発話日時と、ユーザの識別情報と、ＧＰＳ受信機２５８、加速度センサ２６０、傾きセンサ２６２、磁気センサ２６４、明るさセンサ２６６、圧力センサ２６８、及び温度センサ２７０の出力からなる環境情報とが付されている。
　サーバ１０６は、この音声翻訳リクエストを受信すると（図７のステップ４５２）、リクエスト中の言語ペア情報にしたがって言語ペアを選択し（ステップ４５４からステップ４６０）、選択された言語の組合せに応じて選択部４１０、選択部４１２、選択部４１４、選択部４１６、選択部４３４、及び選択部４３６を制御し、適切な要素を選択する。サーバ１０６はさらに、音声認識（ステップ４６２）、音声認識結果に対する自動翻訳（ステップ４６４）、翻訳結果に対する音声合成（ステップ４６６）を行なう。その後、サーバ１０６は、音声認識の結果と音声翻訳リクエストに付されていた各種の情報とから入力発話の特徴ベクトルを生成し（ステップ４６８）、発話連接モデル２２６及び発話候補記憶部２２８内の各発話の翻訳及び音声認識スコアを用いて、次に発話される可能性が高く、かつ翻訳及び音声認識スコアも高い発話候補からなる発話候補リコメンドリストを生成する（ステップ４７８）。サーバ１０６は、最後に、音声認識結果と、翻訳結果のテキストデータと、その合成音声と、発話候補リコメンドリストとを携帯型端末１０４に送信する（ステップ４７２）。この処理が終了すると、サーバ１０６は次のリクエストの処理に移る（ステップ４５２）。
　図２を参照して、携帯型端末１０４は、サーバ１０６からの返信を受取る（図５のステップ３１６）と、音声認識結果を入力テキスト表示領域１５０に、自動翻訳結果のテキストを翻訳結果表示領域１７０に、発話候補リコメンドリスト１９２及び翻訳リクエストボタン１９４を発話候補リコメンド領域１９０に、それぞれ表示し（ステップ３１８）、さらに合成音声データにしたがってスピーカ２８０を駆動することで発話する（ステップ３２０）。ステップ３１８の結果により画面を更新して（ステップ３２２）、ユーザの操作待ち状態に戻る（ステップ３０４）。
　ユーザが次に発話するときには、発話候補リコメンド領域１９０に表示された発話候補リコメンドリストを参考にできる。何もない状態で次の発話を考えるのではなく、具体的な文を目にして次の発話を行なうことができるので、ユーザが戸惑うことはない。さらに、発話としてはその前の発話に続いて発話される可能性の高いものが表示されるので、ユーザが携帯型端末１０４を操作するときにあらためて考えるべき事項が少なく済む可能性が高い。しかも、発話候補リコメンドリストに表示される発話は、翻訳及び音声認識スコアが高いので、その発話にならった発話を行なうと、音声翻訳を行なったときに、音声認識の結果が正しくなる可能性が高くなり、さらに、その結果を用いた自動翻訳の結果がターゲット言語の発話として正しい発話になる可能性も高くなる。したがって、携帯型端末１０４のユーザと他言語のユーザとの対話を、誤解なく、かつスムーズに運ぶことができる。
　本実施の形態では、発話候補リコメンドリスト１９２に表示された翻訳リクエストボタン１９４は、いずれも対応する発話についての翻訳リクエストを発生させるボタンとして機能する。すなわち、発話候補のいずれかのテキストに対応する翻訳リクエストボタン１９４をユーザが押すと、そのテキストが次の発話として選択されてサーバ１０６に送信され、次の自動翻訳の対象となる。
　テキスト翻訳の場合、携帯型端末１０４は以下のように動作する。図２を参照して、ユーザは、テキスト入力ボタン１５４を操作して、テキスト入力画面を呼び出す。携帯型端末１０４では、図５に示すフローチャートにおいて、ステップ３０４からステップ３４０が呼び出される。テキストの入力が終了するとユーザは、テキスト入力を終了するボタン（図２では図示せず）を押す。携帯型端末１０４は、入力されたテキストを保存し（ステップ３４２）、入力されたテキストを入力テキスト表示領域１５０に表示するよう、画面を更新する（ステップ３２２）。ユーザがさらに翻訳ボタン１６４を押すと、図５においてステップ３０４～３６０の処理が実行され、テキスト翻訳リクエストが生成され、サーバ１０６に送信される。このリクエストには、テキスト翻訳命令と、設定にしたがった言語ペアに関する情報と、入力されたソース言語のテキストデータと、翻訳ボタン１６４が押されたときの日時及びユーザの識別情報と、各種センサの出力とが付されている。
　サーバ１０６は、このリクエストを受信すると、図７に示すステップ４５４、４７８，４８０、４８２、４８４、４８６の経路により、入力テキストをターゲット言語に翻訳し、その音声を合成する。サーバ１０６はさらに、入力テキストとテキスト翻訳リクエストに付されていた各種情報とによって特徴ベクトルを生成し、発話連接モデル２２６及び発話候補記憶部２２８に記憶された各発話の翻訳スコアを用いて、次発話として可能性が高く、かつ翻訳スコアも高いソース言語の発話のリストからなる発話候補リコメンドリストを生成する。そして、翻訳結果テキスト、その合成音声、及び発話候補リコメンドリストを携帯型端末１０４に対して返送する。
　携帯型端末１０４は、この返送を受けると、翻訳結果テキストを翻訳結果表示領域１７０に表示し、発話候補リコメンドリスト１９２及び翻訳リクエストボタン１９４を発話候補リコメンド領域１９０に表示する。
　この場合も、音声翻訳の場合と同様、ユーザが次の発話を考えるための発話候補リコメンドリストが発話候補リコメンド領域１９０に表示されるので、次の発話を音声で行なうにせよ、ボタン入力するにせよ、戸惑うことなく容易に次の発話を考えることができる。
　以上のように本実施の形態によれば、音声翻訳を行なう音声翻訳システム１００において、携帯型端末１０４のユーザが発話した後、次の発話を考える際には発話候補リコメンドリストが携帯型端末１０４の画面に表示される。表示される発話候補は、いずれも、先の発話に続いて発話される可能性が高く、かつ、翻訳したときに正しく翻訳される可能性が高い。したがってユーザがこれら発話候補を参考に次の発話を考えることで、相手との対話をスムーズに続けることが可能になり、かつ互いのコミュニケーションに、誤訳により誤解が導入される恐れを小さくできる。
　今回開示された実施の形態は単に例示であって、本発明が上記した実施の形態のみに制限されるわけではない。本発明の範囲は、発明の詳細な説明の記載を参酌した上で、特許請求の範囲の各請求項によって示され、そこに記載された文言と均等の意味及び範囲内での全ての変更を含む。
　この発明は、複数の人、特に異言語を母語とする人の間のコミュニケーションを補助する端末装置に利用できる。
１００　音声翻訳システム
１０４　携帯型端末
１０６　サーバ
１０８　モデル生成部
１３０　アプリケーション画面
１４０　言語表示領域
１５０　入力テキスト表示領域
１６０　逆翻訳領域
１８０　ステータス領域
１９０　発話候補リコメンド領域
１９２　発話候補リコメンドリスト
２２０　音声認識処理
２２２　自動翻訳処理
２２４　音声合成処理
２２６　発話連接モデル
２２８　発話候補記憶部
２３０　発話候補リスト作成処理
２３２　送信処理部
４１８　音声認識エンジン
４２２　自動翻訳エンジン
４２４　音声合成処理部
４２６　発話確率算出部
４２８　発話候補リコメンドリスト作成部
Patent WO2014010450A1 - 音声処理システム及び端末装置 - Google Patents
