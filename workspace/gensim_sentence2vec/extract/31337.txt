テキストデータなどの自然文を機械処理するときには、まず最初に単語に分割するということをよく行います。一般的にはMeCabやChasenといった形態素解析エンジンに投げて行います。形態素と単語の区別という話もあるのですが、ここでは大雑把に「連続した文字列の単位」くらいの意味で話します。
検索という文脈ですと形態素インデックスという言葉がありますが、これは検索の最小単位を文字単位ではなくて形態素の単位にするということです。例えば「東京都」は「東京」「都」に分かれるため、「京都」というクエリに対して見つかるのを防ぐなど、精度を上げる効果があります。反面、深刻な検索漏れを引き起こす可能性があるため嫌われることが多いです。こうした漏れは検索に限らず、テキストマイニングなどの文脈でも問題となることが多いです。
主な原因は2つあります。ひとつは解析精度の問題。しかし、もっと根本的な問題として、解析単位の問題があります。システムが仮定する単語の単位は、往々にしてユーザーが仮定する単位と異なります。例えば、「登山家」という単語があった場合、「登山」というクエリはここにヒットすべきでしょうか? システムが「登山家」を最小単位として取り扱っている限り、「登山」というクエリがヒットする事はありません。そもそも「登山家」は「登山」「家」という形態素だ、という意見もあるでしょう。では、「登山」に対して「山」は見つかるべきでしょうか? そこまでくると文字単位で処理すればいいように思いますが、先の「東京都」の例は見つからなくていい気がします。単語単位と文字単位の間で、柔軟な処理をすることはできないか。今回紹介する論文は、この間を狙った研究です。
日本語同様、単語境界にスペースを入れない言語の代表格、中国語言語処理界隈では今でも単語分割の研究は盛んに行われているようです。近年の単語分割の研究は、多くの場合人手で分割したデータを再現できるかという軸で行います。しかし、この中の分割単位で一悶着あるのです。例えば、先程の「登山家」の例でも、これは「登山家」なのか「登山」「家」なのか。日本語もそうですが、「東京特許許可局」のようにいくらでも漢字をつなげると長い単語ができてしまいます。この単語単位が、正解と仮定するデータによって異なっていたら・・・。ここにおける単語単位とはなんだろうか? 細分割してしまうと、究極的には文字単位になってしまう。そこで単語という単位を捨てて、「文字間の関係」で「単語」が表現できていればいいのではないか、という提案がこの論文の主旨です。
関係の表現方法として2つの実験をしています。まず、従来の単語分割のモデルだと、例えば「日比谷でビールを飲んだ」という文をMeCabに食わせると、以下のような6つの単語の連続になります。
単語と文字の話  |  Preferred Research
