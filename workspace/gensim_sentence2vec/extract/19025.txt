さて今回は,いよいよ確率版の線形回帰をベイズ化します。
まずはベイジアンとはどういう考え方だったのか,ベイズ化するとなにが嬉しいのか,そのあたりを簡単に復習するところからはじめましょう。
ベイジアンふたたび
例えば,「コインを3回振ったら2回表が出た。だからこのコインの表の出る確率は2/3だ」と聞くとどのように感じますか?
「いや,それはたまたまでしょう」「いくらなんでも結論が早すぎる」とかいった印象を受けるのではないでしょうか。その印象の正体は,試行回数が少なすぎてコインの確率はおおむね1/2だろうという事前知識(先入観)に反する結果を信用できない,というあたりでしょう。
これがもし「コインを300回振ったら200回表が出た。だからこのコインの表の出る確率は2/3だ」だったらどうでしょう。
こちらなら「ああ,それならそうかもしれない。変なコイン……」と思えるのではないでしょうか。試行回数が増えて,事前知識に反する結果でも受け入れられるくらい信頼度が上がったわけです。
でもちょっと待ってください。この短い例題の中に,困ったことが2つもあります。
まず,その2種類の印象の違いを答えだけから知りようがありません。答えはどちらも同じ「2/3」ですからね。
そして,試行回数が3回では足りなくて,300回あれば十分,というのは主観的すぎるのではないでしょうか。答えを信用するには何回試行すればいいか,その基準があいまいです。
このような問題を解決する1つの方法が,第10回で紹介したベイジアンの考え方でした。答えを1つの値として求めるのではなく,値それぞれに対する「自信」を確率分布として求めるものでしたね。
事前分布などのこまかな話はおいておき,上の問題を試しにベイジアンのもとで解くと,「コインを3回振ったら2回表が出たとき」と「300回振ったら200回表が出たとき」の,「コインの表の出る確率」の自信(分布)はそれぞれ図の赤線と青線のようになります。
赤線の分布なら「一応2/3が一番可能性高いけど,まだまだ全然違う値である可能性も捨てきれない」ことがわかりますし,青線の分布なら「少しくらいぶれるかもしれないけど,ほぼ2/3!」ということが言えます。
答えを確率分布として得ることで,その高さや分布の広がり方(分散)からいろいろな情報を読みとることができるのがベイジアンのメリットです。
ベイジアン+線形回帰
さてベイジアンの良さはわかりましたが,それをわざわざ線形回帰に当てはめる必要があるのでしょうか。よくよく考えてみると,線形回帰にも先ほどと同じ問題点が,もっと切実な形で存在することがわかります。
線形回帰の答えは「係数wのある値」として得られます。その値だけを見ていても,どれくらい信頼できる数字なのかは同様にわかりません。しかも,「データがいくつあれば十分信頼できるか」という判断はコインの場合よりさらに難しくなります。
こういったグラフを描くことができれば,「だいたいOK」「いまいち」のような判断をすることもできるかもしれません。しかしやはり主観的なレベルになってしまいますし,高次元になるとそれすらかないません。
そう,だからこそ,ベイジアンの出番です。
ここまでの線形回帰では「答え」を係数wの値として求めていました。そのwを直接求めるのではなく,それぞれの値の「自信」を確率分布として求めるのがベイジアンの考え方になります。
準備として,前回までの記号を簡単にまとめます。
(x,t):観測された値(確率変数)。入力xに対して出力tを返す関数を求める
φi(x):基底関数(i=1,...,M)
:基底関数の線形和(求める関数の候補)
そして前回導入したtの条件付き分布p(t|w,x)です。線形回帰を確率化するのに必要だったこの分布が今回も活躍します。
実際には複数のデータ点を扱うので,(x1,t1),...,(xN,tN)のように確率変数の列を考えるのですが,複数個の場合は後回しにして,まずはxやtは1個だけで話を進めます。特にxは入力値なので定数扱いして,p(t|w,x)もp(t|w)と記します。
さて,ベイジアンで問題を解く手順をすべて説明し直しているとさすがに話が長くなりすぎるため,要点だけなぞっていきましょう。復習したい場合は,事前分布・事後分布などについては第10回を,ベイズ公式や乗法・加法定理については第2回を参照してください。
まずは事前分布,つまりベースになるwの分布p(w)を仮定します。これについてはすぐ後で説明します。
続いて,そのp(w)とデータ点(x,t)を使って,wの分布を事後分布p(w|t)に更新します。この事後分布が求める「答えの自信」です。
事後分布への更新式はベイズ公式と乗法・加法定理から得られます。
第13回 ベイズ線形回帰［後編］:機械学習 はじめよう｜gihyo.jp … 技術評論社
