
方法論から 
統計的パタン認識  
確率分布のダイバージェンスはKullback-Leiblerダイバージェンスをはじめとする幾つか有用なものが提案されている.それに対して判別関数のダイバージェンスはどうだろうか?そのとき,どんな性質が要請されるべきか?この素朴な疑問からこの研究はスタートされた.Neyman-Pearson補題の再考察から新しいロス関数のクラスを提案している.従来,クレジットスコアーリング,Area
under ROC Curve (AUROC) などの別々の内容で考えられていたロスも共通の観点から統一化できる.このロスのクラスの中でロジステックモデルの下で一致性があるための必要十分条件が与えられる.一般化線形モデルの文脈で推定方程式がロジスティック回帰判別のそれに重みが加わった形で与えられ統計言語Sで容易にデータ解析が可能である.これよりロジスティックタイプの判別関数と呼んでいる.[p22],[r15],[r17].
この研究に対して人工知能の分野で提唱されたブースト学習アルゴリズムと密接な関連性があることに気づいた.上で提案されたロス関数は更にマルチクラスに拡張され,U-ロス関数と呼んだ.このロスから自然にアダブーストを含むクラスをU-ブーストを導いた.その統計的性質,特にベイズルールとの同値性について考察している
[p27], [r18], [r21], [r22].  研究のスタートだった判別関数のダイバージェンスは,
確率密度関数上の汎関数から有限測度の密度関数上の汎関数に拡張することで豊かなものが得られることが分かった.特に,
U-ロス関数を生成するU-ダイバージェンスに集中した議論をしているが,
その性質については未だよく分かってないことがある.特徴ベクトルを
x とし,クラスラベルを y としよう. ある判別関数F(x, y)から定義される判別ルール argmin y F(x, y)は,その判別関数のどんな単調変換 φをしても,それから定められる判別ルールと同じである.
argmin y F(x, y)  =  argmin y φ(F(x, y)).
この不変性について更に理解を深めたい.
さて, U-ダイバージェンスからU-ブーストの導入を説明しよう. 2つ判別関数
F, G のU-ダイバージェンスのオリジナルの形は
DU (G, F) =∑y  ∫[U( F(x, y))− U(G(x, y))− p(y | x){F (x, y)− G(x, y)}]Q(dx)
で与えられる (一般論は数理を参照).ここで  p(y | x) = (U ')-1 (G(x, y)),Q は特徴ベクトルを x の分布を表す前段で指摘したように2つ判別関数 F, G を次のように変換しても判別ルールは変わらない.  
F*(x, y) = F(x, y) −∑y F(x, y)p(y | x),  G*(x, y) = G(x, y) −∑y G(x, y)p(y | x).
従って,この変更によって
DU (G*, F*) = ∑y∫[U( F*(x, y))− U(G*(x, y))]Q(dx)
と簡単になる. トレーニングデータの条件付分布 p(y | x) が与えられたとき,U-ロス関数は期待値の形では
LU (F ) =  ∑y∫U( F*(x, y))Q(dx)
と書け,経験U-ロス関数は
LUemp (F ) =  ∑ i ∑ y [U(F(xi, y)−F(xi, yi))]
で与えられる.このロスに関して逐次最小化
: Ft+1 = Ft + αt ft によって U-ブーストが組まれる.
U-ブーストの中から特別にイータブーストを提案した.これはUη(F)= (1−η)exp(F) +ηF から作られ,,イータブーストの経験ロス関数は
∑ i ∑ y [(1−η)exp{F(xi, y)−F(xi, yi)}+ η{F(xi, y)−F(xi, yi)}]
で与えられる.ここで{(xi, yi) : i = 1,..., n } はトレーニングデータを表す. η= 0 ならばアダブーストを定める指数ロス関数に帰着される. 確率的な議論から
η はミスラベルの割合を表すことが示せる. 実際,
U-モデルは公式から
pη( y | x) = (1−η(1−g−1)) p0( y | x) + ηg−1 ∑y' ≠ y   p0( y' | x) 
となる.これはイータブーストの連想するU-モデルが, 確率モデル p0(y | x) からミスラベル割合 η の混入モデルとなっていることを示唆する. この考察を進めるとミスラベルに対するロバスト性が獲得できることが理論と実データの適用から示される
[p25].  下図のようにベイズ境界がサインカーブで与えられような設定から,
一旦,トレーニング・データ(例題)をシミュレイトする.
次に 人工的に境界に近い例題の幾つかをラベル・スウィッチを施した.
2次元の特徴空間に対して学習機の集合は, x-軸,または y-軸に平行な直線の判別境界を与える弱い判別機の集まりをを選んだ.
アルゴリズムの停止規則はK-ホールドの交互検証法で行った. 結果はアダブーストは過剰学習が示唆されるのに対して,
イータ・ブースは比較的良好な性能を示している.
幾つかの実データに対しても同様な考察をした.
ガウシアン・ミクスチュアー(GM)モデル
このGMモデルは統計学の分野では正規混合モデルと言われ,EMアルゴリズムをはじめ,広く議論されている.また人工知能の分野でも教師なし学習の内容で広く議論されている.GMモデルのアプローチは大きく分けて探索的と検証的なパラダイムに分けられる.この研究では,検証的なパラダイムに集中する.このとき尤度関数が非有界になることから最尤法の不適切な挙動が指摘されている.
この困難な問題に対してU-ダイバージェンスの特別な例であるベータ・ダイバージェンスの適用を考えた.特にロバスト推測の観点から考察した
[rm19].  次にゲノムデータであるSNPのタイピングのためにGMモデルにもとづくクラスター法をこの方法から適用した
[p26].更にチューニング・パラメータ β の選択について考察している [rm27].
独立成分分析 (ICA)
この分析方法は信号処理の研究者が音声分離の文脈から提案したある発見的な方法から端を発し,人工知能の学界において,ここ10年のうちに急速に進展された方法論である.少なくともインスタント・ミクスチュアーモデルは非常に単純である.独立な成分からなるベクトル
s が直接には観測できなくて未知の行列 A の変換を受けて観測される.すなわち観測ベクトルは y = As でとモデル化される.このような単純なモデルが統計学で基本解析として扱われてないことは驚きである.統計的には線形パラメータ
A の推定に他ならない.ここで技術的に注意しなければいけないことは s が正規分布に従っていれば, 行列 A は,識別不能パラメータとなることから,本質的には s は非正規分布に従うと仮定しなければいけない.しかし,統計学の多変量解析において基礎分布は正規分布である.
統計研究者が正規分布からの重縛から脱却できるまでには射影追跡のアイデアを待たねばいけなかったようにこのICAモデルは統計学のコミュニティからは提案されなかった.現在,ICAのアプローチは大きく分けて,基本形は
1.  Kullback-Leiblerダイバージェンス最小化
2.非ガウス尺度最大化
3.同時対角化
の3つにの方向がある. 本研究は特に 1 に関してロバスト化を目指したものである. 1で確立された方法は,基本的には,独立信号の分布を既知として実行される最尤法と等価である(偽最尤法).Kullback-Leiblerダイバージェンスを含むワン・パラメーターのダイバージェンスのクラスであるベータ・ダイバージェンスを情報最小化のために活用した.ベータを適切に選択すれば音声データに混入されるスパイク・ノイズなどにロバストであり,かつ信号の情報損失が少ない信号分離が可能になった
[p23]. 
データに適応的にβの選択を得る方法も開発されている [r19].2, 3の既存の方法のロバスト化も考察中である.またロバスト化を超えた構造変化の探索の問題についても検討中である.
主成分分析 (PCA)
主成分分析の歴史は Karl Pearsonの直交回帰にまで遡り,標本分散行列の固有分解の問題に帰着されている.一方で人工知能の分野ではオンライン学習アルゴリズムとしてさまざまな方法が議論されている.本研究は,その方向で提案されている自己組織化によるアルゴリズムの統計的な意味を明らかにした.とくに影響関数を明示的に表し,重要な領域において挙動の安定性について論じた.
また繰り返し再重み付けの標本分散行列を定義し,繰り返し主成分ベクトルを求めるアルゴリズムを提案した
[p16]. p-次元データ { xi : i = 1,..., n } が得られたとしよう. 例えば,第一主成分ベクトルγ を求めるため提案した方法はロス関数
L(γ, μ) =  n−1∑ iΨ(|| xi−μ ||2−{γT(xi−μ)}2) 
を(γ, μ) に関して最小にすることから求まる.ここで,Ψ(r) = r ならば, この最小化は固有問題に帰着され,通常のPCAになる.提案された方法はベータ・ダイバージェンスから作られるΨβ(r) = {1−exp(−βr)}/βを採用した. このとき最小化問題は陽には解けなくて,RM
(Reweighted Matrix) アルゴリズム
μ ∑ i w(xi−μ, γ) xi , γ  Eigen[∑ i w(xi−μ, γ)(xi−μ)(xi−μ)T]
で与えた.ここで Eigen[A]は 対称行列Aの最大固有値に対応する固有ベクトルを表し.重み関数wは
w(x−μ, γ) = Ψ '(|| x−μ ||2−{γT(x−μ )2)/{∑ iΨ '(|| xi−μ ||2−{γT(xi−μ)}2)}
で定める. 下図のように主成分ベクトルへ射影した2乗残差を非線形に重み付ける.これより自動的に主成分ベクトルに遠い観測ベクトルは小さな重みがかけられる.
この方法を含むロバストPCAのクラスを提示し,統計的一致性を証明し,漸近効率について公式を与えた
[p20]. ロバストPCAのクラスを指定するチューニングパラメーターの適応的選択についてK-fold
cross-validationによる方法を開発や,オンラインデータに対して
RMアルゴリズムと勾配法との収束性について調べた
[p28].
観測バイアスの感度分析アプローチ
意識アンケート調査,医学コホート調査など非実験的な観察からのデータは,欠損値,無回答,打ち切りなどの不完全観測が生じる事を避けては通れない.これから
データの不完全性に関するランダムネスに情報が加わることがある.
このようなノンランダムネスから生じるバイアスを観測バイアスと呼ぶ.これを直接推定するとモデルのmisspecification
からのバイアスと分離することが不可能であることを示した.これより統計概モデルによって感度分析からの観測バイアスの影響を調べることが提案されている.
潜在変数をZとおき,観測可能な変数YはY=h(Z)の関係で結ばれていると仮定する.ここで変換hは多対1対応である. EMアルゴリズムの文脈でZを完全データ,Yを不完全データという.ミッシング,センサリング,ミクスチュアーなどが典型例である.よく知られているように,Zの密度関数 fZ(z,θ)からYの密度関数 fY(y,θ)へ
fY(y,θ) =∫h−1(y)  fZ(z,θ)dz
なる形で導かれ, ZのスコアーベクトルsZ(z,θ)からYののスコアーベクトルsY(y,θ)は
sY(y,θ) = E[sZ(z,θ) | Y=y]
で結ばれる. 従って情報行列の差(情報損失)は
IZ−IY = var(sY) −var(sZ) = E[var(sZ|Y)]
と表される.さて,この基本設定では,Zではなく,Yだけが観測される不完全性はランダムであると仮定されている.
しかし観測の不完全性に何らかノンランダムネスが加わりバイアスが生じたケースを数学化しよう.
典型例はインフォマティブ・ミッシングである.観察されたデータより欠測値の方に情報がある場合だってあるわけだ.
ZのモデルMZ ={ fZ(z,θ) : θ∈Θ}に対して概モデルを
N(ε, MZ) = { gZ(z,θ) = fZ(z,θ)exp{εuZ(z,θ)−ε2/2} : uZ ∈ΞZ, θ∈Θ}
と表す. ここで ΞZ は fZのもとで平均0,分散1となる確率変数全体を表す.
任意のθ∈Θに対して KL(gZ, fZ) = ε2+ O(ε3)となることが分かる.このように標準化した空間ΞZがMZからN(ε, MZ)へ誘導する. すなわち,1対1対応
(ΞZ, MZ)       N(ε, MZ)
がある. ZがgZに従うならYの密度関数は,近似的に
gY(y,θ) = fY(y,θ)exp{εuY(y,θ)−ε2/2}
にとなる. ここでuY(y,θ) = E[uZ(z,θ) | Y=y]である.これより,(ΞZ|Y, MY)       N(ε, MY)となる. 以上の内容で MYとMYを強モデル, N(ε, MY) と N(ε, MY)を弱モデルと呼んだ. 強モデルのもとでの推論の標準的性質, すなわち, 最尤推定量の漸近一致性, 漸近有効性が弱モデルの下でどのくらい定量的に逸脱するのか上のフレームワークの下で調べられている.Cf. [r12], [p21], [rm26].
局所尤度法
尤度関数をカーネル関数によって空間的な局所情報を取り入れた方法のクラスを提案する.そのクラスの中には観測の打ち切りや切り取りなどがカーネル関数によるモデリングを典型例として含む.  密度関数 p(x) の推定において,適当なパラメトリックモデルを仮定することを考えよう.一般に統計モデル M = {p(x, θ) : θ ∈Θ }が仮に想定しよう. 各点 x ごとに密度関数 p(x) を推定するために, 各点 x ごとに尤度関数を局所化しよう.具体的には x を中心にする区間を考え,この区間に落ちた観測値だけに基づいて尤度を計算する.区間の外に落ちた観測値は,打ち切りと見なすか,切り取られたとするかでバージョンの違いがある.しかし基本的には,
点 x ごとにパラメータ θ の局所尤度関数を計算して,それの最大化によって推定量を求め,密度関数 p(x, θ) に代入する.これが密度推定の局所尤度法である.
区間の幅 h を小さくすれば密度推定のバイアスは小さくでき,区間の幅 h を大きくすれば分散は小さくできる.実際には,
スムージングのため区間で分けるのではなく,滑らかなカーネル関数K hで重みつける. 提案した局所尤度関数のクラスは,
L(θ, x) = ∑ i [Kh (xi −x) log p(xi , θ) − Ξ(Kh (xi −x), Eθ{Kh (X−x)})]
で与えられる.ここで,Ξ(・, ・) は局所化のバージョンを表す. このように区間幅 h はカーネル関数のバンド幅 h に対応させる.トレードによってカーネル関数のバンド幅の漸近最適性が決定されることが分かる.
たとえば正規分布 N(μ, σ2) を仮定すれば,標本平均と標本分散を正規分布の密度関数の平均と分散に代入すれば,
各点 x で有効な密度推定となる.ところが正規性の仮定を緩めると事情は一変し,
密度推定の良さの構造は正規性からのずれによるバイアスと分散とのトレードに問題が還元される.
局所尤度法は, 各点 x ごとにその廻りだけに正規分布の平均と分散を推定した値
(μx ,σ2x) を正規密度関数に代入する.
バンド幅 h の漸近最適構造は真の分布がどれだけ仮定されたモデルと乖離しているかで決定される.言うまでもなく,乖離がゼロならば,バンド幅
h は無限大に取ればよい.   実際には,
乖離に対して統計概モデルのアイデアを適用すると, 統一的な結果が提出される
[p17] , [p24], [rm23]. ここで簡単な多項回帰について見てみよう.モデルの次元と最適バンド幅が密接な関連が想像出来る.
概統計モデルの漸近理論はそのような現象に対して, 統計概モデルの乖離の程度が h無限大漸近性と h無限小漸近性に分岐を起こすことを明らかした.
過去の研究テーマ
情報幾何による統計推測理論
漸近2次有効推定
統計的推定の漸近有効性については多くの理論統計学者が長い歴史の中で様々な観点から,議論してきている.ある意味で統計学の最も純粋な理論的なパートの一つに数えられるだろう.
1975年,Annals of Statisticsに発表されたBradley
Efron の統計曲率はその歴史の中でも記念すべき新たな一ページを開くものであった.Fisher - Rao の定理(最尤推定の2次有効性)を最小コントラスト推定量に
ついて情報幾何の方法により拡張した.これから最尤推定量を含む2次有効な推定のクラスが構成できた.コントラスト汎関数から連想されるReimann計量と双対線形接続を導き,それが統計モデルから作られる情報計量,e-接続とm-接続に一致する時,その推定量は2次有効となる.Fisherのスコアーアルゴリズムについても同様な考察によって効率の回復を明らかにした.([p1, p2, p3, p4, p6, p7, p10, p12], [r1, r2, r3, r4, r6, r15]
射影法による統計推測
統計非線形モデルは多くの場合パラメータの非線型な拘束によって定式化される.一方で線形モデルは最小2乗法が最適に働く(Gauss-Markov定理).
この射影法のキー・アイディアは非線型な拘束を表現される親パラメータを
逆非線型変換を施すことによってモデルを平坦化することにある.([p5],[r5]).
この逆非線型変換のあとは,一般化最小2乗法によって推定量が陽に求まり,漸近有効性が示される.  この平坦化変換の構成は発見的であるが,興味ある適用例として集団遺伝学のHLAの定常性の推測や多変量指数モデルに実現された.
([p8, p9, p11], [r10])
相対エントロピーによる統計解析
Kullback-Leibler (相対エントロピー )が情報幾何の中心の役割をはたしている.この相対エントロピーをデータ解析に直接の適用を試みた.最小2乗法はデータの点とモデルの点とのEuclid距離の2乗を最小化によって定義され,様々な適用の歴史がある.これに対して正値データが得られたときデータの点からモデルの点への相対エントロピーを最小化によってこの方法は提案される.正値なデータが時間的飽和性を持つとき,この方法は特長を発揮すると予想される.特に薬物動態学における,薬の代謝の時間データに適用された.生物成長モデルについて人間の身長・体重のデータや森林環境のデータについて適用が試みられている.
([p13,15],[r8])
方法論から
