今回は,機械学習で使う「確率」のお話です。
確率は,統計的な機械学習のもっとも重要な基礎知識です。とはいえ,確率についてゼロから説明するというのは紙数的にも厳しいため,高校の確率を少し憶えているくらい(期待値や標準偏差など)を前提とし,「高校の確率」と「機械学習の確率」の本質的な相違点について,少し丁寧に見ていく,という形で進めていきます。
機械学習と確率
最初に,機械学習にとって確率はどういう役割なのかを確認しておきましょう。
実のところ,機械学習に確率が必須というわけではありません。ニューラルネットワークやサポートベクターマシンなどの有名な手法も「確率を用いない機械学習」ですし,その他にも数多くの手法があります。しかし,「確率を用いない機械学習」の多くは,「結果のランキングを作りづらい(評価値の大小に意味がない)」「条件が異なる場合の結果を比較できない」などの欠点があります。
一方の「確率を用いる機械学習」では,評価結果や推定されたパラメータが「どれくらい信用できるか(もっともらしいか)」を確率として計算します。確率同士は比較可能なので,計算結果を使ってランキングを作ったり,前提条件が異なっている結果同士を比較したり(よいモデルを探すときによく行われます),ということが自然にできるのです。
また,「確率を通じて他の手法を組み合わせることができる」「モデルによってはデータを生成できるという特徴を持つ(生成モデル)」など,確率を使っているからこそのメリットが数多くあげられます。
こうしたメリットを求めて,もともと確率を用いない手法でも,確率的な手法に拡張されることが多いです。つまり,より効果的な手法を求めるなら,確率を用いた方法に行き着くということですね。
確率変数と確率分布
「確率」について,記号と用語の説明から始めましょう。
確率は p(X) という記号で表します。ここで X は「確率変数」, p(X) は「 X の確率分布」あるいは単に「 X の確率」と言います。X の取り得る値 a に対してその確率を  p(X=a),または簡単に p(a) と書きます。
この記法では,「確率分布の種類は確率変数によって表される」ということに注意する必要があります。別の確率変数 Y の確率分布は,同じ p を使って p(Y) と書きます。2つの関数を表すときの  f(x) と g(x) のような記法とは異なります。
p(X) が確率分布であるための重要な条件が2つあります。
確率の値は0以上1以下
すべての取り得る値の確率の合計は1
厳密には,事象や集合演算なども説明したほうがよいのですが,ここでは省略します。
おなじみのサイコロの例に当てはめて確認してみましょう。X はサイコロをふって出る目の「確率変数」,p(X) がその「確率分布」とします。このとき,X の取り得る値は1から6までの6通りです。すべての目が同じ割合で出るとすると,「確率の合計は1」という条件から,p(X=1) = ... = p(X=6) = 1/6 となります。
ところで,「すべての目が同じ割合で出るとすると」とさらっと言いましたが,本当にそうでしょうか? 確率を初めて勉強したとき,「1の目が出る確率は 1/6 って言うけど,サイコロを振っても1の目がちょうど6回に1回出るとは限らないよなあ」と思ったことはありませんか?
そのもっともな疑問に対し,「無限回繰り返して平均すると,6回に1回出るんだよ」と説明され,一応納得しつつも「いやいや,サイコロを無限回とか振れないから!」と心の中で言い返したことはありませんか?
実は統計的機械学習の最終目的は,まさにそういった「有限回しか試行できない中で,すべての目が同じ確率で出ると言ってもよいか」という問題を工学的に(つまり現実的に)解くことなのです。
高校で確率をやったときにそのあたりの疑問で悩んだことのある人は,機械学習をとても楽しめると思います。
同時確率と条件付き確率
ここまで確率変数は1個でしたが,確率変数は複数個になる場合もあります。特に機械学習では,よほど簡単な例題でも複数の確率変数を持っていますので,複数個の確率変数を扱えるようになっておくことは必須です。
まずは先ほどと同じように,用語をさらっておきます。確率変数の個数は2個で説明しますが,3個以上でも同様です。
2個の確率変数 X, Y に対する確率分布を p(X, Y) と書き,XとYの「同時分布」または「同時確率」と言います。
確率変数 Y に何かある値が与えられているときのXの確率を p(X|Y) と書き,Yが与えられているときのXの「条件付き確率」と言います。
確率変数 Y は気にせずに,確率変数 X のみの確率を考える場合もあります。これは p(X, Y) の X に関する「周辺確率」と言い,単純に p(X) と書きます。
具体例を見ながら,この3つの分布がどういうものか説明します。せっかくなので,機械学習(自然言語処理)で実際に用いられる bag-of-words というモデルのミニミニ版を使いましょう。
bag-of-words では単語の並び方などは考慮せず,文章に単語が含まれているかどうかのみを考えて数値化します(含まれる=1,含まれない=0)。使用回数をカウントするモデル(term-frequency)もありますが,今回は「含まれるか,含まれないか」のみを考えています。
このモデルで表現できる範囲は明らかに限られます。しかし,実現したいことが達成可能ならば,必要以上に複雑なモデルを考えないのは機械学習の鉄則です。このあたりのことは,前回のモデルの話も参照してください。
それでは,モデルの構築は確率変数の設定から始めます。
Xは「文章にプログラムという単語が含まれる」,Yは「文章にアプリケーションという単語が含まれる」を表す確率変数とします。それぞれ,含まれる場合が 1,含まれない場合が 0 という値を取ります。
本物の数字の方が説得力あるでしょうから,gihyo.jpのデベロッパステージの1560記事から各単語の出現確率を調べてみました。
上から「両方の単語が含まれる確率」「プログラムのみ含まれる確率」,「アプリケーションのみ含まれる確率」「両方とも含まれない確率」となります。
扱っている範囲はとても狭いですが,これも立派な「モデル」です。
ここで確率変数 Y はおいておき,確率変数 X のみの確率を考えてみましょう。
p(X=1),つまり「文章にプログラムという単語が含まれる」確率を求めるには,アプリケーションも含む場合と含まない場合の両方を考えればいいので,次の式で求まります。
第2回 確率の初歩:機械学習 はじめよう｜gihyo.jp … 技術評論社
