とりあえず1回目は、前に言っていたNLP Programing TutorialのCapter01(pdf)、1-gram言語モデルの勉強です。
では、スライドに沿って話を進めようと思います。
言語モデルって何?
1-gram言語モデルの前に、そもそも言語モデルって何?
具体例で説明すると。
音声認識システムにて、認識した結果の出力候補としていくつかの単語列があったとしましょう。
W1 = speech recognition system
W2 = speech cognition system
W3 = speck podcast histamine
W4 = スピーチ が 救出 ストン
言語モデルは、これらの単語列に「もっともらしさ」を与えてくれるのです。
一般的には、各単語列に確率を与えてくれます。
上の各単語列は、ある言語モデルに基づくと
P(W1) = 4.021*10-2
P(W2) = 8.932*10-4
P(W3) = 2.432*10-7
P(W4) = 9.124*10-23
のように単語列の生起確率としてとらえることができるようになります。
使う言語モデルによってこの生起確率の値は変わってきます。
この1〜4の単語列の場合、人が見たなら1, 2, 3, 4の順でそれらしい(ありそうな)単語列だと思えるはずです。
つまり、
という確率を出してくれる上の例のような言語モデルは、それっぽい(良い)といえるってことですね。(言語モデルにも善し悪しがあります。)
つまり
言語モデルとは、より自然な、より頻出する文(や単語列)に高い確率を与えてくれるものなのです。
単語列の確率
実際にはどうやって単語列の確率を出してるの?という話
まずは、各単語の同時確率で考えましょう。
同時確率は条件付き確率で考えられるので
P(|W|=3, w1="speech", w2="recoginition", w3="system") =
P(w1="speech") | w0="<s>")
* P(w2="recognition" | w0="<s>", w1="speech")
* P(w3="system" | w0="<s>", w1="speech" , w2="recognition")
* P(w4="</s>" | w0="<s>", w1="speech" , w2="recognition", w3="system")
と表せる。") = 1">*1
(<s>, </s>はそれぞれ文頭記号,文末記号で、1-gramではあまり意味は無いが2-gram以上のN-gramや遷移モデルなどを考えた時には必要になる)
ここで文頭,文末記号が入ってるってことは単語列じゃなくて文だったのかよっていう突っ込みは考えないでおこう
これを一般化すると
これで単語列の確率は表せそうです。
じゃあ、それぞれのP(wi | w0...wi-1)はどうやって求めるのか。
一つの方法として、最尤推定があります。
母集団分布の形が分かっているがその母数が未知であるときに,n個の標本値 x1,x2,...,xnを母集団分布に従う確率変数 X1,X2,...,Xnがとることは最も起こりやすい(maximum likelihood)という条件を用いてその母数を決めようとするものである.
(最尤推定)
起きた現象を、「最も引き起こす確率が高い」場合を考えるのが最尤法です。
(最尤法を言葉で簡潔に説明するには、どのように説明すればいいですか?)
要するに(たぶん)、
起きた事象から、きっとこういう確率だったんだろうなと推測する
ってことだと思います。
で、今回の場合はコーパス*2の単語列を数え上げればいいわけです。(たぶんコーパスが事象で、この世の全ての文章などなどが母集団ってことになるんだと思う。たぶん。いやこの世の文章もサンプルにすぎないのかな、、そしたら母集団は文章を生み出してる言語自体なのかな、よくわからんけどニュアンスは伝わったと思う。。)
で、式はどうなるかというと(c(W) : 単語列Wの総数)
チュートリアル1: 1-gram言語モデル - 自然言語処理の分別奮闘記
