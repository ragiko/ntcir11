N-gramをはじめとする統計的言語モデルの評価尺度として,パープレキシティがこれまで広く用いられてきた.しかし,ドメイン外テキストを併用する言語モデルや混合言語モデルなどの複雑な言語モデルに関しては,認識システムの単語エラー率とパープレキシティとの相関が悪いという結果が近年報告されている.本稿では,n-gram言語モデルに代わりうる評価尺度について検討した結果を報告する.パープレキシティが評価テキストの単語の出現確率のみを用いるのに対して,ここで提案する指標は,評価テキストに出現する単語の言語尤度と,その単語が出現した文脈における最大言語尤度との差に基いている.この尤度差に対してシグモイド状の非線型関数を適用した後,言語毎の平均を算出する.非線型関数を適用することにより,認識結果の改善に寄与しない言語スコアの変動の影響を抑えることができる.音声認シミュレーション実験および実音声認識実験の結果と,ここで提案した指標との相関を調べてみたところ,パープレキシティに比べて高い相関を示すことが確認された.
Perplexity has been widely used as an evaluation metric of stochastic language model. Recently, several papers reported that correlation between perplexity and word error rate was poor when complicated language models were used, such as mixture model. In this paper, a new metric for n-gram language model is proposed, that is intended to substitute perplexity. The major difference of the proposed metric from perplexity is that, while perplexity utilizes probabilities of word occurences in the evaluation text, the proposed metric accumulates differences of linguistic scores between a word in the evaluation text and the maximum score available in that context. A sigmoid-like nonlinear function is applied to the score difference and the average of that values is calculated. Applying the nonlinear function suppresses the effect of language score difference that does not affect word errer rate improvement. Correlation between the proposed metric and word accuracy was investigated for a speech recognition simulator and real speech recognizer. The result proved that the proposed metric had higher correlation between word accuracy than perplexity.

