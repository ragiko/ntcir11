どもです。林岳彦です。息子の3DSにバーチャルコンソールの「ソロモンの鍵」を密かに入れました(まだ3面)。
さて。前回の記事:因果関係がないのに相関関係があらわれる４つのケースをまとめてみたよ（質問テンプレート付き） - Take a Risk：林岳彦の研究メモにつきましては沢山ブクマ等をいただき大変ありがとうございました*1。大変感謝しております。さて。上記記事について、ublftboさんから「相関関係の定義が書かれていないのでは」(相関と因果 - Interdisciplinary)とのご指摘をいただいたきました。ご指摘は確かにごもっともですので、今回は「相関」概念についてと、そのついでに近年に開発された"21世紀の相関(MIC)"の話について私なりに書いてみたいと思います。
(以下、ややマニア向けの話になるかもしれません。あと前回ほどではないですが、それなりに長いです。)
広義の「相関」(より適切には「関連/association」と呼ばれるもの)
まずは、前回の記事の補足的なところから話をはじめたいと思います。前回の記事において「相関」という語を私がどういう意味で用いていたかというと、あまり深くは考えてはいませんでしたが、改めて文字にすると「得られたデータにおける項目Aと項目Bの間に何らかの関連が見られる=相関がある」という意味で用いていたと思います。これはかなりざっくりした用法で、かなり広義の意味での「相関」という語の使い方と言えます(日常言語における「相関」の語のニュアンスの方を強く反映した用法とも言えるかもしれません)。Wikipediaの"correlation"の項を見てみると:
In loose usage, correlation can refer to any departure of two or more random variables from independence, but technically it refers to any of several more specialized types of relationship between mean values. 
という記述がありましたが、この前半の"loose usage"における"any departure of two or more random variables from independence"というところが、前回の記事において私が念頭においていた「相関」の語の意図するところになります。
ここで、"departure of two or more random variables from independence"というところの意味が良く分からない方も多いかもしれないので、「関連/association」と「独立/independence」の関係についてちょっと説明を加えてみます:さて。そもそも「得られたデータにおける項目Aと項目Bの間に何らかの"関連"が見られる/見られない」というのは、統計学的にどう表現されうるでしょうか?一般的には、Aが起こる確率P(A)と、Bが起こる確率P(B)、および、AとBが同時に起こる確率P(A,B)の関係が:
P(A,B) = P(A)P(B)
のときに、AとBは「独立/independence」と呼ばれます。また、AとBが独立であるとき、それらの間には「(統計的に)関連がない」とされます。例えば、2つのコインA, Bを投げたときに、それらがオモテとなる確率がそれぞれ「 P(A=オモテ)=0.5、P(B=オモテ) = 0.5 」であるとしましょう。ここで「AがオモテでBもオモテ」となる確率 P(A=オモテ, B=オモテ)が、「 P(A=オモテ, B=オモテ)= P(A=オモテ) x P(B=オモテ) = 0.5 x 0.5 = 0.25 」であるときには、「コインAを投げてオモテになる確率」と「コインBを投げてオモテになる確率」は「独立」であり「関連がない」ということになります。逆に、 P(A=オモテ, B=オモテ) が0.25 から逸脱する場合には、コインAとコインBのあいだに「何らかの関連性」が推測されることになります。
ちなみに上の式を別の形で書くと:
P(A) = P(A|B)
とも書くことができます。これは、「Bがどうであるか」はAの確率に影響を及ぼさない、ということを示しています。また別の言い方をすると、「Bに関する情報は、Aに関する予測の役に立たない(=何の情報ももたらさない)」という言い方もできます。つまり「AとBが独立である」というのは、AとBの関係が「関連がない」「影響を及ぼさない」「予測の役に立たない」「情報をもたらさない」などなどの意味に対応することになります。
はい。というわけで、「AとBの間に広義の意味で相関がある」は数学的/統計学的にいうと「AとBは独立でない」という状況に対応するものになります。一般には、この意味での「広義の相関」については、「相関correlation」よりも「関連association」という語が使われるのが普通ですので、細かい議論をする際には区別して使うことが望ましいでしょう。(その意味で、前回の私の記事の用語法はあまり良くないと言えます*2)
では次は、ご存知の方も多いと思いますが、狭義(というかより一般的な用法における)「相関」について説明してみたいと思います。
いわゆる「相関」は直線的関係の指標である
一般に、統計学の文脈において「相関」と言った場合には、「ピアソンの相関係数」に基づくものを意味することが殆んどかと思われます。Wikipediaの「相関係数」の項からピアソンの相関係数の数学的定義を引用すると:
となります。さて。では、ピアソンの相関係数の直感的な性質を見てみましょう。(ピアソンの)相関係数の特徴は、データ間の直線的関係のみを見ていることにあります。直線的な関係を見ているということは、AとBの間に「明らかな関連/association」がありそうな場合にも、相関係数(correlation coefficient)は低い値になりうる、ということを意味しています。「百聞は一見にしかず」なので、Wikipediaの図を見てみましょう:
http://ja.wikipedia.org/wiki/%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB:Correlation_examples2.svg より引用
ここでそれぞれの図はデータの散布図を表していて、上の数字はそれぞれの相関係数を表しています。上の図から:
直線的関係でばらつきが全くない場合は相関係数は1(または-1)になる
直線的関係*3の傾きの大きさは相関係数の大きさには関係ない
ただし傾きが完全にフラットのときは相関係数はゼロ*4
直線的関係においてばらつきが大きいと相関係数はその分小さくなる
非直線的な関連を評価したい場合には相関係数はあまり役にたたない
という「相関係数」の特徴がわかると思います。
はい。いわゆる「相関係数」とは、こういうものなんです。
さて。上記の特徴を理解して使えば相関係数は大変便利なものです。しかし、世の中の全てが「直線的関係」だと思うなよ、と言われたらそれはまさしくそうであります。
そこでMICですよ(ドヤ)。
21世紀の"相関":MICとは?
MIC(Maximum Information Coefficient : MIC@Wikipedia)とは、すごくざっくり言うと「どんな形でも対応可能な"相関"係数」です。2011年のScienceで出版されたReshef et al. 2011において発表されたもので、そのときのScience誌の解説文では「A correlation for the 21st century」なんて書かれたりしています(スゴイネ!)。
MICはピアソンの相関係数のようには単純な数式では表せず、コンピュータによってゴリゴリと計算されます。基本的なアルゴリズムとしては、データ散布図を様々な数のグリッド(=解像度)で区切っていきながら、様々な解像度の値において相互情報量が最大(=各グリッド内に含まれるデータ密度のコントラストが最大となるようなイメージ)となるような区切り方を決定し、それらを規格化したのちの最大の情報量をMICの値として選択しているようです*5。ちなみに相互情報量を数式で表すと:
となっており、独立( P(x,y)=P(x)p(y) )のときには相互情報量はゼロとなることがわかります。
MICの特徴は、どんな形のassociationでも定量化できるできるところにあります。例えばこんな感じです:
http://lectures.molgen.mpg.de/algsysbio12/MINEPresentation.pdf より引用・改変
これは、一番左の列のタイプのデータの場合に、MICとピアソンの相関係数がそれぞれどのような値をとるかを示しているものです。ピアソンの相関係数では検出できていないような非線形の場合においても、MICでは高い値を示すことがわかります。(ちなみにMICがとる値の範囲は0から1までになっています)MICの特徴として、データがばらつくに従ってその値が低下することも挙げられます:
http://lectures.molgen.mpg.de/algsysbio12/MINEPresentation.pdf より引用・改変
この辺りの性質は、ピアソンの相関係数の性質を良く受け継いでおり、直感的にも違和感のないものです。
MICはかなりgeneralなものなので、基本的にはどんな対象にでも適応できます。その中でも有望な応用例として、遺伝子発現における「非線形的関連の検出」が挙げられているようです。幸い、最近RでMICを簡単に計算するための"minerva"というパッケージが出たようなので、それを使って計算も試してみたいと思います:
install.packages("minerva")
library(minerva)
data(Spellman)
Spellman ここで"Spellman"というデータセットには、CDC15 Yeast Geneの4382個の転写産物の量を時系列(23 time points)で計測したデータが入っています(詳しくはこちら)。mine関数においてMICの値が計算されており、"res"にはその結果が格納されています。"res"の中身を見て、MICが高い値になっていた2つの転写産物の例をピックアップしてみると以下のようなパターンになっていました:
いずれも、ピアソンの相関係数では低い値となるケースですが、MICでは非線形的な関連が捉えられているようです。正直ちょっと「ほんまかいな」と思わないでもないですが、研究の「とっかかり」を得る分には十分なのかなあとも思います。
"相関"の話&そのついでに"21世紀の相関(MIC)"の話(ややマニア向け) - Take a Risk:林岳彦の研究メモ
