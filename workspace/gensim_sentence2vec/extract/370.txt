 日本語入力におけるN文節最長一致とはなんなのか
Googleの工藤さんとPFIの徳永さんがN文節最長一致法について議論している記事を見つけました.
日本語入力におけるN文節最長一致とはなんなのか
興味深かったので引用しておきます.
Taku Kudo
徳永さんの本のレビューをやりつつ、N文節最長一致について少しコメントを書きました。N文節最長一致についてはさんざんな言われようで、うまくいく原理はよくわからないとか、たまたまうまくいっているみたいな認識を持っている方が多いと思います。Mozcの開発を通じ、その心がわかったし、よくできてるんだなと感心しました。
N文節最長一致は、ユーザの入力単位が文節であるという仮定を強く意識した手法です。換言すると、ユーザは自分の入力が常に1文節になることを期待しながら入力しているという仮定です。実はこれはあながち間違っていなくて、多くのユーザは無意識のうちに文節単位で入力しています。この仮定が常に正しのであれば、1文節最長一致は、すでに最適解です。(もちろん同音異義語や文節内のランキングはありますが。) 最長一致の解が見つからないような長い文が入力となるような場合もあるでしょう。その時には、1文節最長一致であれば、局所的に長いものをグリーディーに見つけていく動作になります。2,3文節最長一致は、局所解に陥らないための、もしくは、できるだけ大局解を得やすくするための工夫です。
本来、「ユーザは自分の入力が1文節になることを期待している」といったユーザの入力単位をIMEの事前知識として入れるべきです。しかし、私の知る限り、統計的手法のIMEでこのような仮定をモデル化しているものはありません。本来は重要な現象であるにもかかわらず、研究分野では「文」単位の評価やモデル化が多いのです。Mozcでも、さんざんこの仮定には悩まされました。文節のプリファレンスを事前知識として入れないと解けない誤変換がいくつかありました。今現在、最小コスト法の枠組みの上に事前知識を入れています。
松本研OBのJ社のTさんも、文節外の言語モデルと文節内のモデルは分割したほうがよいとおっしゃっていました。まさしくその通りです。例えば以下のような定式化が可能でしょう。
文節内言語モデル: 通常のN-gramやクラス言語モデルをつかう。ただし、単語列 w1,w2,w3... が文節になりやすいかも含めた言語モデル。
文節間言語モデル: 特にアイデアはありませんが、文節間のクラス言語モデルなんかになると思います。
この2つのジョイントを考えないといけないのですが、単純なViterbiではデコードできないです。(と思います。)。N文節最長一致のようなスタックデコーダを使えばデコードできます。
古典的なN文節最長一致は、
文節内言語モデル: 文節文法を使った文節の妥当性の判断 (0 or 1) しか返さない値と、自立語部分の言語モデル(単純な頻度など) の掛け算
文節間言語モデル: N文節の文字列長に比例した確率 (Nグラム)
とした場合と考えられます。
Hiroyuki Tokunaga  -  レビューありがとうございます。頂いたコメントを今反映作業中です。
ユーザーの入力単位が1文節であれば1文節最長一致が最強であるというのはこれまでなかった考えで、実データを見ておられるのだな、というのが少し羨ましく感じました。
ただ、N文節最長一致はそれでもまだ不思議なのではないかと思いました。具体的には、入力が文単位のように長くなってきた場合でもN文節最長一致は割合にうまくいく、その点が不思議だと思うのです。2,3文節最長一致だと1文節最長一致より事実として文単位での変換精度が上がる訳ですが、なぜ上がるのかが私にはうまく説明できません。
ただ、自然言語の性質からなんか言えるんじゃないかなぁ、とぼんやりと考えています。人間が言葉を理解する際には聞いた端から単語を認識しているように(自分の挙動を振り返ると)思えるので、そういうやり方でそんなに間違いが起こらないように言語が設計されているから、その結果としてN文節最長一致法で大体うまく行くのだ、みたいな。素人考えですが…。
Taku Kudo  -  N文節最長一致のもうひとつの利点は、辞書引きをサボれることです。最小コスト法は全探索をするので、基本的にすべての単語を辞書引きしている必要がありますが、N文節最長一致はその限りではありません。FDをガリガリルックアップしていた時代はいかにIOを減らすかがキモだったのでN文節最長一致は重宝されたのでしょう。
N文節最長一致(N>=2)がうまくいくことは、文節数最小戦略がうまくいくこととほぼ等価だと思います。文節数最小戦略を辞書引きをさぼりながらできるだけ厳密解を求めようとするのが、N文節最小一致だと思います。N=∞であれば、厳密解は求まりますよね。
文節数最小戦略は、認知的にも自然かなと思います。人間は文節単位で処理しているはずです。(例えば、幼児向けの絵本は文節単位で分かち書きされているとか、文節単位でポーズを置くと話しやすいとか) その仮定が正しいと、いかに処理する文節数を減らし楽にパージングするかということになり、それがまさしく文節数最小戦略になるのではないでしょうか。
ツイートする
Permalink | 10:23
Android版Google日本語入力がリリース
追記: 公式アナウンス出ました。
Google Japan Blog: 思いどおりの日本語入力をモバイルでも。Android 版 Google 日本語入力をリリース。
Android Marketからダウンロードできるようになっているようです。
動画:Android版 Google 日本語入力 beta  提供開始 - Engadget Japanese
ずいぶん前から準備はしてあったと思われますが、それにしてもPC版Google日本語入力のリリースからかなりの時間が経っていますね。それだけAndroid版の開発は難しかったということでしょうか。
ツイートする
Permalink | 08:19
バイドゥ株式会社がアンドロイド用日本語入力システム『Simeji(シメジ)』の事業を取得
これはびっくり.
Baidu(バイドゥ)ニュース - Baidu.jp に関するニュース
たまに聞かれるので答えておきますが,simejiは@adamrocker氏が開発したAndroid向け日本語入力ソフトで,Social IME+JI(字)という由来の通り最初はSocial IMEのAPIをかな漢字変換サーバーとして使っていましたが,比較的早い段階でOpenWnnをデフォルトの変換エンジンとして採用し,現在はSocial IMEはマッシュルームプラグインの1つとしてオプション的な扱いになっています.
今回の買収に関しても私は直接関わっていませんし,adamrocker氏との面識もないのですが,機会があれば話を聞いてみたいですね.
ツイートする
Permalink | 23:32
VLDB 2011とVLDB 2011勉強会の資料
大規模データベースに関する国際学会,VLDB(Very Large Data Base)のスライドが公開されていました.
Detailed Program | VLDB 2011
また,DSIRNLPと同じ日の12月10日にVLDB 2011勉強会が開かれたようです.
No such site.
ここで個人的に気になるのはやはりMapReduce関連の論文ですね.
Session 18: MapReduce and Hadoop
MapReduceには既存のParallel DBの技術が取り入れられつつも,独自の最適化を必要とする部分もあって,大変興味深いフレームワークだと思います.
そういえば,このネタはHadoopアドベントカレンダーに書いても良かったような気がします(でも出し惜しみするのも苦手なので書いてしまいますが).
hadoopアドベントカレンダー2011 on Zusaar
ツイートする
Permalink | 07:24
統計的言語モデルとN-best探索を用いた日本語形態素解析法
今更ながら,NTT永田さんによる形態素解析のためのA*アルゴリズムを使ったN-best論文を読みました.というか,前にも読んで分かった気になっていたのだけど,忘れていたのでメモっておきます.
統計的言語モデルとN-best探索を用いた日本語形態素解析法 
そもそもA*アルゴリズムは最適解探索アルゴリズムであり,なぜこれでN-best探索ができるのか疑問でした.
A* - Wikipedia
論文の5ページ目には「最適解が得られたら,そのノードを取り除き,さらに探索を続けることにより次の最適解が得られる.」と書かれています.しかし,実際に擬似コード(図3)を読むとノードを削除するのではなくclosedリストに移しているだけで,しかもclosedリストに移されたノードは条件によってopenリストに戻される場合がある,というあたりがわかりづらかったです.これはラティス上では最適パスとそれ以下のパスがノードを共有している可能性があるためで,その意味では図4で共有しているノードを分けて書いているのもわかりづらいと感じました.
最後に擬似コードの7行目で「文頭に到達したらSUCCESSを返す」とありますが,それでは1-bestを出しているだけになるので,実際には文頭に到達した回数をカウントしておいて,openリストから探索を再開するノードを探すということを行います.これをN回繰り返すことで,文頭に到達するたびに最適解から順にN-best解が得られるということになります.
ツイートする
Permalink | 23:09
第2回DSIRNLP勉強会に参加しました #dsirnlp
第2回 データ構造と情報検索と言語処理勉強会 #DSIRNLP - PARTAKE
自然言語処理はじめました by @phylloさん
自然言語処理はじめました - Ngramを数え上げまくる
DSIRNLPで発表させていただきました - Negative/Positive Thinking
自己紹介:Negative/Positive Thinking
今日の概要:いろんな方法でN-gram頻度を数える
N-gramとは?
隣り合うN個の塊のこと
単語n-gramや文字n-gramがある
ナイーブな方法
ハッシュに入れて数える
問題:大規模テキストやNを大きくしたら?
N-gramの異なり数はNに対して指数的に爆発する
解決法:N-gramをメモリに保存しない!
Suffix Arrayを使った方法
入力文のSuffix Arrayを使った方法
メモリの節約になってる?:3*N+4byte * N-gramの種類4byte*入力文字数
かなりの節約になる:1GBくらいならこの方法でOK
問題:さらに大規模になったら?:例えば1TBのデータとか
近似カウント法
頻度が多いものに注目しカウント(Counter-based)
Hash関数を使ってカウント(Sketch-based)
今回はLossy Countingを紹介する
Lossy Counting
ストリームデータの基本的なカウント法
何を保持するのか?:要素名,頻度の推定値,最大許容誤差
定期的にいらないデータを捨てる
問題点:近似値でしかない
分散処理を使う方法
近似じゃだめなんだ! 正確な頻度が欲しいんだ!という奇特な人は…
Hadoop: 個人のPCやAmazon EMRなどで利用
まとめ
これでどんなでかいデータが来ても大丈夫!
実際にやってみるといろいろ問題があることが分かる
AIでAI創ってみた(LT) by @uchmikさん
uchiumi log: Ragnarok Online の AI に機械学習を入れてみた
きっかけは入門Luaプログラミングロジスティック回帰書いてみた
Lua書き始めたら簡単に書けたラグナロクオンラインのAIがLuaで書ける
動画:攻撃を教える,ついてくる,スキルを教える
Web+DB Press / Web+DB Press plus
Web+DB pressについて
Webアプリケーション開発のための技術情報誌
読者層:20〜40代,男性95%
Web+DB press plusについて
「〜を支える技術」シリーズなど
近刊:「日本語入力を支える技術」 by @tkngさん(2月発売予定)
お願い:書いて or 買ってください!
Mahoutにパッチを送ってみた by @issayさん
Mahoutにパッチを送ってみた
自己紹介
今日は皆既月食
Mahoutの実装,特に教師あり学習について説明
機械学習の並列分散
NIPSでも大規模学習のワークショップが開催
Scaling up Machine learning
Mahoutとは
実装している手法
リコメンデーション
クラスタリング
分類:SGDの分散に対応していないパッチ送りました!
MapReduce
Map: キー:単語,バリュー:1
Shuffle:キー順にソート,キーごとにグループ化
Reduce:キーごとにバリューを足し合わせる(例:バルス=14059)
Shuffleわかりづらいやっていることは分散ソート!
Mapタスク内のソート:バッファサイズを超えたらソートしてファイルに書きだし,最後にマージ
ナイーブベイズとは
ナイーブベイズとは?:ベイズの定理でP(c|d)を求める
ナイーブベイズ仮定:文書内の単語は独立に生起する
アンダーフロー対策:logを取って足しあわせればOK
ゼロ頻度問題:加算スムージングを行なう
さらなる実装の工夫:Tackling the Poor Assumptions of Naive Bayes Text Classifiers (2003)
Complemental Naive Bayes
TF正規化,文書長正規化,カテゴリ正規化
Mahoutでのナイーブベイズの実装
カウント問題を4回のMapReduceで行なっている
単語の分割はスペース区切りでしか対応していない
BayesFeatureDriver: 特徴量を抽出する
BayesTfIdfDriver: TF*IDFを計算する
BayesWeightSummerDriver
BayesThetaNomalizerDriver
ランダムフォレストとは
複数の決定木の多数決で出力を決定
決定木の構築
学習データを重複を許してサンプリングし,B組のデータセットを作成
枝の分岐:正解カテゴリのエントロピーを計算する
枝を分岐後のエントロピーを計算する
相互情報量:枝を分岐前と分岐後のエントロピーの差
Mahoutでのランダムフォレストの実装
決定木の数だけ分散するのは簡単
ロジスティック回帰とは?
Mahoutでのロジスティック回帰の実装
オンライン学習が実装されているが,MapReduceによる分散には対応していない
L1/L2正則化に対応している
L1正則化はFOBOSとlazy Updateで実装
Adaptive Logistic Regression:ハイパーパラメータが違う学習器をn個用意
送ったパッチの概要
オンライン学習をMapReduceで行なう
Iterative Parameter Mixingという手法を実装
Mapper: 正解データを入力,前回の重みは初期化処理で読み込んで機械学習
Reducer: 受け取った重みの平均を取る
送ろうパッチ
Mahoutはパッチを送ってみるにはいいのかも
Hadoop本体と比べ,完成度は低いし,コード量も少ない
実はHadoop使ってなくてもJavaで書きさえすればいい?
Mahoutの問題点
日本語のテキストを扱いたい
MapReduceの繰り返すようなアルゴリズムは非効率
Hadoop 0.23
MapReduce以外のフレームワークをサポート
MPI, Spark, Graph, Hama..
Introduction to data structure for GraphDB
Introduction data structure for GraphDB
Agenda
Motivation
Introduction
Structure for Node
Create Node
Appendix
Neo4j : an OSS implementation of GraphDB
Graph = Node + Relation
A node data structure
NodeManager: Node cache, Relation cache, Cache manager
Array structure for relationships
Relationship structure
In blocks
Out blocks
Create node
Cache: Strong/Soft/Weak LRU, No cache
Graph Algorithms
Centrality: Betweenness, Closeness
Path: A*, Dijkstart, Shortest Path
announcement
情報処理学会全国大会でパネルディスカッション
冬のLock-Free祭り @kumagiさん
冬のLock free祭り safe
辻Lock-free
CPUの系譜のおさらい
周波数戦争の終焉
マルチコア時代の到来
ポラックの法則
「シングルスレッド性能を2倍にするには,チップ面積が4倍必要になる」
逆にいえば,半分の性能で4つのコアを搭載することができる!
Intel Many Core: 驚きの48コア搭載
マルチコアを使い倒そう!
マルチスレッドは簡単じゃない:ロックはブロッキングする
コアが増えるほどロックが問題になる
ロックフリー!
例:x += 1
CAS (Compare And Swap)命令
コンペア・アンド・スワップ - Wikipedia
今日はロックを用いないデータ構造を紹介します
Lock-free Stack
Headが指している物を指したノードを作ってCAS
Lock-free QUEUE
Dequeueは簡単:
Enque操作
Enqueしたい要素eを用意する
CASで末尾に要素をアトミックに追加
CASでTailポインタを変更
2回のCASが必要2CPUが同時にEnqueすると壊れる
不変条件を考える
Tailが末尾を指さない場合も壊れないように作る
Tailが遅れていたら他のCPUがTailを進めてやる
EnqueとDequeの両方でTailが末尾を指さない場合に対応する
発展話題:boost.lockfreeのqueue
Lock-free List
粗粒度ロック:全体を排他するだけスケールしない
悲観的ロック:ノードごとにロックを用意かなり遅くなる
楽観的ロック:必要になるまでロックを取らない安定性が低すぎ
つまり:単一ロックだとスケールしない
これからロックフリーにします
挿入処理:挿入先のポインタでCASを取る
削除処理:連続したノードを削除しようとするとデータ構造が破壊される
挿入と削除がぶつかっても破壊される
解決策:ノードの削除を「マーキング」「ポインタ繋ぎかえ」の2段階に分ける
ポインタの1ビット目をマーキング対象に
削除マークとポインタを同時に行なうため
削除のときは2回CASを使う
Lock-free Hashmap
「気が狂っていて楽しいデータ構造」
細粒度LockHashmap:全部のバケットにロックを取り付ける遅い
Striped Lock Hashmap:固定数のLockをmoduloでバケットに割り当てる
java.util.concurrent.ConcurrentHashMap:基本的にStriped Lockだけどwait-free
Lockのなくし方を考える
線形リストのLock-freeは上記のアルゴリズムを使う
あらかじめ十分巨大なバケットにしてしまう無理
CASでバケット列を複製一貫性無理
バケットの拡張が困難バケットの中身が必要ない設計
普通はバケットにデータを吊るしてる
こっちはデータの間にバケットの目印を吊るす
メモ:この辺りで脱落した
不変条件:リストが昇順に並んでいるという前提は崩れない
驚異的なスケーラビリティ:JavaのConcurrentHashMapより速い!
Lock-free SkipList
順序関係のあるデータをO(log n)で検索・挿入・削除ができるデータ構造
2分木と違い,リバランスが必要ない
LevelDBの中で使われていてちょっと話題に(並列化しやすいため)
昇順に並べた普通のリストに高レベルなリストを加えたもの
Lock-based Skip List:Lock-free listの楽観的ロックに近い
どうやってロックフリーにするのか?
Lock-free listと同様に,ポインタに削除bitを導入
Lock-free Btree
みんな大好きBtree
普通のBtreeの挿入操作:ノードに余裕がなくなったらSplitする
Lock-free Btreeは簡単すぎて論文にならない
必要なノードを全て複製してしまえばいい
Rootに対するCASでできる超遅い
B-treeがLockFreeになって喜ぶ人は案外少ない
DBの論理的競合解決にB-treeのロックを使ってしまってる
ロックがなくなったら上のレイヤーで競合かいけつが必要
Dynamic Software Transactional Memory
Lock-freeには限界がある
コードを使いまわせない
複数の操作をSerializableな分離レベルで実行
ステータスは,Commit, Abort, Activeのどれかを取る
CommitならNew,AbortならOldが真の値
書き換え方法の説明
衝突した場合の説明
ステータスを共有するのがポイント
The Art of Multiprocessor Programming (TAoMP本)
Transaction on Key Value Store:修士論文(予定)
研究背景
Webサービスを支えるたえにキーバリューストアが広く利用されている
同時に1つのキーバリューペアしかアクセスできない
キーバリューストアでのCAS:getsコマンド(memcachedプロトコル)
ロックで実装クライアントが離脱すると永遠にロックされたまま
提案
キーバリューストア上にトランザクションを実装
サーバー側には手を加えず,クライアントライブラっリとして実現
クライアントが離脱しても大丈夫!
基本方針
Dynamic STMを応用
トランザクショナルキーバリューペア(TKVP)を提案
old, new, statusを追加して4つの値をまとめて扱う
問題点
ゴミが増え続ける
もともとDSTMはGCが前提
参照カウンタを実装する予定
まとめ
Lock-free Stack, Queue, List, Hashmap, Skiplist, B-tree と Dynamic STMのアルゴリズムを解説
分散環境に応用しようと取り組んでいます
Github: kumagi (Hiroki KUMAZAKI) ? GitHub
機械学習と最適化 by @tkngさん
自己紹介:PFI,日本語入力
最適化
目的関数の最小値 or 最大値
大域的最適と局所最適
凸関数なら最適解が1つしか無い
例:2次関数の最小値
微分して極値を求める
機械学習では陽に解けないことが多い
勾配法による解法
機械学習とは最適化である!
線形識別器の紹介
スパムメールの判定:メールから特徴を抜き出す
SVMの損失関数=ヒンジロス+正則化項
勾配法の適用
まとめ:SVMは怖くない!
学習率の話
Lazy Updateの場合,累積正則化項を計算するために,学習率の和が必要になる
学習率の和を予め計算しておけば速くなるはず
LT
機械学習・言語処理技術を用いた誤り検出・訂正 @mamorukさん
誤り訂正の研究はホット!
ACLのError Correctionセッション
Microsoft Speller Challenge
共同研究者募集中
(神の言語を用いた自然言語処理 (あるいは人生宇宙全ての答えについて))
神の言語による自然言語処理
宇宙はLispで書かれている!!!!
Clojureの紹介
…
言語判定へのいざない @shuyoさん
言語判定へのいざない
自己紹介
language-detectionの紹介
Solrに入りました
Twitter言語判定もできます
言語トーク
アゼルバイジャン語
Dutch(オランダ語)はもともとドイツ語という意味
デンマーク語とノルウェー語は似ている
言語判定かわいいよ!
作ろう!簡潔ビットベクトル @echizen_tmさん
Lets Impl Sbv
自己紹介
id:echizen_tm
縁の下の力持ち的な技術に興味がある
簡潔データ構造
データサイズが小さく,高速な操作(検索など)が実現できるデータ構造
ビット列,木構造,文字列などに簡潔データ構造版がある
応用例
Google日本語入力
検索プラットフォームSedue
簡潔ビットベクトル
ビット列の簡潔データ構造
木構造(LOUDS)や文字列(ウェーブレット木)の基礎となるデータ構造
rank/selectという操作を高速に実行できる
ビット列に小さな補助インデックスを加えたもの
rank(i) : i番目より前の立っている(1である)ビットの数
rankの利用例:疎なデータ列の効率的な実装
疑問:0だった場合はどう検出する? rank(n)とrank(n+1)の差を取る?
select(i): i番目に立っているビットの位置
selectの利用例:可変長データ列の効率的な実装
rank/selectは単調非減少関数
rankとselectは逆関数の関係
密なビットベクトルと素なビットベクトル
既存ライブラリの性能
ux-trie, rx-trie, marisa-trieのビットベクトルを取り出して評価
marisa-trieはselectが速いがsizeがやや大きい
実装テクニック集
実装方針: C++ で uint8_t / uint16_t / uint32_t / uint64_t がおすすめ
rankの実装:popcount + rank辞書
popcount: 変数内に立っているビット数を定数時間でカウントする
rank辞書:あらかじめ適当なブロックサイズごとにrankの値を持っておく
iがBの倍数の場合はrank辞書を引くだけ
iがBの倍数でない場合はビットシフトして上位ビットを削ってやれば良い
rank(i) = R[i/B-1] + popcount(V[i/B-1] << (B-i%B))
既存ライブラリのrank実装
ux-trie: B=512
rx-trie: B=32
marisa-trie: B=512,64 (大ブロックと小ブロックの2段階)
ブロックを2段階にすることで,小ブロックのrank辞書のサイズを小さくできる
selectの実装について
selectの実装の基本アイデアはrankを用いた2分探索
まずブロック単位で2分探索を行い,ブロック内で線型探索または2分探索する
既存ライブラリのselect実装
ux-trie: B=512 ブロック単位で2分探索64ビット単位で線型探索64ビット中でpopcountを使った2分探索
rx-trie B=32 基本通りの実装
marisa-trie: B=512,64 512ビットごとにselect辞書を持っておく
popcountの実装
Binary Hacks と 64bit popCount 問題 | TAKESAKO @ Yet another Cybozu Labs
小宮日記
類似文字列検索の仕組み @overlastさん
自己紹介
入門ソーシャルデータを監訳しました
類似文字列検索の使いどころ
応用例:Webサービスの場合
どういう問題か噛み砕く
データの再利用率を上げる
タスクの概要と疑問
類似文字列検索の仕組み
データ構造:トライ,Suffix Array,転置インデックス
良いもの。悪いもの。: C++: 編集距離を求めるアルゴリズム
Apporoの紹介
まとめ:類似文字列検索など余裕!
ツイートする
Permalink | 01:46
Yoh Okunoの日記
