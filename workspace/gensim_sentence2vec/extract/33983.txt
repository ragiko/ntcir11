いずれの質問からも,生のデータを目の前にした観察者が「何」を観察すればいいのかわからずに迷っているようすがうかがえます.確かに,実験なり観察をすればデータは得られます.しかし,データセットさえあれば自動的(受動的)にしかるべき情報が私たちに流れ込んでくるわけではありません.むしろ,観察者たる私たちは自発的(積極的)にデータから情報を読みとろうとする姿勢が必要です.
かつて,科学哲学者カール・R・ポパーはその著書『客観的知識:進化論的アプローチ』(文献2)の中で,科学的知識の獲得に関する「バケツ理論」と「サーチライト理論」とを対置させました.前者の「バケツ理論」とは,人間の五感によって知覚された観察をバケツに貯めこんでいくことが知識を構成するという素朴な考え方です.ポパーは,蓄積された観察さえあればほかは何もいらないという「バケツ理論」に反対し,観察する前に私たちは検証すべき仮説を立てるべきであるとする「サーチライト理論」を提唱しました.ポパーの「サーチライト理論」によれば,
いかなる種類の観察をなすべきか —われわれの注意をどこに向けるべきか,どの点に関心をもつべきか —をわれわれが学びとるのは,もっぱら仮説からだけである(文献2,p.2より引用).
となります.データ解析に当てはめるならば,私たちは単にデータをバケツに受動的に溜め込むのではなく,データセットを能動的に「読む」ためのサーチライトを用意しなければなりません.いったいデータの「何」に光を当てればいいのでしょうか?
§ 分布の位置とばらつきを可視化する
ここで,データは「変動する(ばらつく)」という真理に注目しましょう.たとえば生物を対象とする実験ならば,コントロールされた実験要因はもちろん,遺伝的変動あるいは環境的変動により,観察データにはばらつきが生じます.たとえば,前回用いた事例である栽培土壌条件を変えたときのある作物収量データを再びとり上げましょう.3通りの栽培土壌条件(「clay」 =粘土 / 「loam」 = ローム / 「sand」 = 砂)によってある作物の収量を10標本ずつ調査したデータをインデックス・プロットとして図示すると図1のようになります.
全データを何の手も加えずに可視化した図1をじっと観察するうちに,私たちはこのデータセットがもつ特徴に光を当てる サーチライト がいくつかあることに気づきます.まずはじめに,複数のデータの真ん中を計算することによりデータセットのおおまかな「位置付け」ができます.前回とり上げた箱ひげ図ではこの真ん中を中央値(メディアン)によって示しました.以下では,中央値の代わりに,データセットの平均値(mean)すなわちデータの総和をデータの個数で割り算した値を 真ん中 の指標としましょう.図1の上に30標本から計算した平均値(=11.9)を横線(太実線)で記入すると次の図2が得られます.
このデータセット全体の位置を示す総平均(grand mean)を真ん中を示す基準として,次にそれぞれのデータが総平均から見てどれほど大きなばらつきをもつかが可視化できます.各データと総平均とのこの差を全偏差(total deviation)と定義します.この全偏差は,データが総平均よりも大きければ正の値をもち,逆に平均値を下回れば負の値をもちます.図2に全偏差を書き加えれば次の図3になります.
以上で,データセット全体の挙動を知るために真ん中すなわち総平均とばらつきすなわち全偏差という2つのサーチライトによって光を当てました.
さて,この実験では栽培土壌を三水準で変化させてその効果を調べています.観察された収量データの値がばらつく原因は実験処理水準の結果でしょうか,それとも偶然誤差に起因したのでしょうか.総平均および全偏差という2つのサーチライトだけではこの問いに答えることはできません.水準ごとに限定してデータの挙動をより詳細に探査するためには,水準ごとに計算された処理平均(treatment mean:ある水準のデータ総和÷反復数)というサーチライトが必要になります.実際に水準ごとに処理平均を計算すると下記のようになります:「sand」=9.9,「clay」=11.5,「loam」=14.3.総平均と処理平均との差は処理偏差(treatment deviation)とよばれます.この処理平均を図2に記入すると図4になります.
太実線で示された総平均がデータセット全体の真ん中を示す基準値であるのに対し,細実線で示された処理平均は各水準に限定された真ん中を示す基準値といえます.同じサーチライトではあっても,総平均と処理平均ではその射程の広さに違いがあります.
処理平均によって各水準ごとの真ん中が確定したならば,各水準内のデータのばらつきを誤差偏差(error deviation:水準内データ−処理平均)として表示できます(図5).
このように,元のデータセットに対して真ん中を示す平均とばらつき示す偏差という2つのサーチライトを導入することで,データセットのふるまいに関する「可視化」がきわめて直感的に実現されることが理解できるでしょう.データのもつばらつきは数値のままでは可視化できません.しかし,上で示したように一つひとつグラフ化することによって,私たちはばらつきのもつ構造を直感的に理解することができるようになります.次回以降に登場することになる水準間の差の検定あるいは分散分析(analysis of variance)という方法の基礎には,データのばらつきこそ情報ソースなのだという信念があるのです.
意外なことに統計学の歴史を振り返ると,データセットの真ん中の指標として「平均」を用いるという考え方は17世紀以前にはまったく見当たらないと統計学史家イアン・ハッキングは指摘しています:
平均化という概念自体が新しいものであり,1650年以前には人々は平均をとらなかったので,平均を観察できた人はほとんどなかったのである (文献1,p.155より引用).
直感的に理解しやすい平均でさえ歴史的に新しい概念であるとしたら,平均を踏まえたデータのばらつきの指標を考えつくのがさらに遅れたとしても不思議ではありません.
指標としての平均と偏差は数値的なデータ解析のスタートラインをも与えます.直感的なデータ解析から定量的な統計分析への移行に複雑難解な数学は必要ありません.私たちに求められているのは,自らの目でしっかりデータを見る姿勢にほかならないのです.
§ 統計的推論はアブダクションである
全数調査のように母集団をすべて調べ尽くす状況では,データセット(=母集団)は集められた全情報の要約という記述統計としての意味をもちます.一方,母集団からの有限個のサンプル抽出を考える推測統計の場合には,ばらつきのある,すなわち変動のあるデータに基いて,母集団に関する未知パラメーター(真の平均や分散の値)を推論するという状況が生じます.通常のデータ解析は有限個の標本データに基づく推測統計を意味しているので,既知の知見から未知のものを推論するという過程が必ず含まれます.
データ解析の第一段階であるデータの可視化はその後に続く統計的推論の方向づけをするきわめて重要な意義を担っています.実験や観察によって得られたデータはそのまま鵜呑みにはできません.データに含まれているかもしれないさまざまな間違いやノイズ,ばらつきや偏りは,データを蓄積しさえすればいつの間にか真実に到達できるというバケツ理論の素朴な実証主義とは相容れません.むしろ,観察されたデータを批判的に吟味することにより,必要に応じてサーチライトを照射しながらデータのふるまいを調べるスタンスが必要でしょう.
データ解析を踏まえた統計学的推論「アブダクション(abduction)」という推論形式にしたがっています.推論様式としてのアブダクションは,伝統的な帰納や演繹とは異なり,データを説明するために立てられた仮説の真偽を問いません.同一のデータを説明しようと競合する複数の仮説の間で,データを証拠とする相対的なランキングを与え,それを踏まえてもっともよい仮説を選び出します.
歴史学者カルロ・ギンズブルグはデータがもつ情報的価値について次のように述べました:
資料は実証主義者たちが信じているように開かれた窓でもなければ,懐疑論者たちが主張するような視界をさまたげる壁でもない.いってみれば,それらは歪んだガラスにたとえることができるのだ (文献3,p.48より引用).
ギンズブルグはデータを鵜呑みにしたり頭から拒否することなく目の前のデータ(資料)を批判的に検討する態度が必要だと強調しました.データが仮説に対してもつ証拠として価値を認めるギンズブルグの結論は統計学の立場からも吟味する価値があります:
ひとは証拠を逆撫でしながら,それをつくりだした者たちの意図にさからって,読むすべを学ばなければならない (文献3,p.46より引用).
データという 歪んだガラス を通して見るということは,データと仮説のいずれに対しても 「真偽」 を問うことなく,もっと弱い論理的関係を両者の間に置くことです.それはまた,目の前にある観察データをそれぞれの対立仮説がどれほどうまく説明できるかを数値化し,その善し悪しによってランキングするという意味でもあります.証拠としてのデータが仮説に与える経験的支持は,演繹や帰納が含意する論理的真偽に比べればはるかに弱い関係ですが,それでもなおデータによる仮説の選択力は失われてはいません.われわれは証拠によってより強く支持される仮説を選ぶという基準を置くことができるからです.
データを十分に「逆撫で」したうえで最良の仮説へのアブダクションをすることが統計学の最終目標です.そのためには,何の熟慮もなく単に「計算」するのではなく,前もってデータをよく 「見る」 心構えが私たちには求められています.統計解析に先立つデータ処理の核心は「視覚化」にあります.生のデータの挙動がよく見えるようなグラフを描くこと,そしていろいろなグラフを併用して視点を変えてデータを見つめることは,私たちの直感的な統計センスと生得的な認知的能力のもつ利点を積極的に活用したデータ解析の第一歩となります.
では,データの視覚化に続く次なる一歩とは何か.次回は確率分布とパラメトリック統計学に関する話題に移ることにしましょう.
文献
Ian Hacking:The Emegence of Probability: A Philosophical Study of Early Ideas about Probability, Induction and Statistical Inference, Second Edition, 2006
『確率の出現』(イアン・ハッキング/著 広田すみれ,森元良太/訳),慶應義塾大学出版会,2013
Karl R. Popper:Objective Knowledge: An Evolutionary Approach. Clarendon Press, 1972
『客観的知識:進化論的アプローチ』(カール・R・ポパー/著 森博/訳),木鐸社,1974
Carlo Ginzburg:Rapporti di forza: storia, retorica, prova. Giangiacomo Feltrinelli Editore, 2000
『歴史・レトリック・立証』(カルロ・ギンズブルグ/著 上村忠男/訳),みすず書房,2001
[SHARE]
ツイート
Prev2Next
TOP
本記事の掲載号
第2回 データの位置とばらつきを可視化しよう｜2014年4月号｜統計の落とし穴と蜘蛛の糸｜羊土社:実験医学online
