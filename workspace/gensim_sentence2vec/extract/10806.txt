
9月12日(木)
夜中に何度か目覚める。3時ごろ起き出して、メールを読む。今日くるはずの面倒な仕事が来ていたので、それをしばらくやっていた。途中腹が減ったので、買っておい
たカップ麺を食べる。
6時ごろ疲れたのでもう一度寝て、7時半に起きる。睡眠障害で具合が悪いが、仕方ない。
8時ちょっと前に朝食をとりにロビーへ。混んでいたのでいつもとは別な部屋。
8時すぎに会場入り。8時半からキーノート。
Weighted Transducers in Speech and Language Processing (Michael Riley,
Google)
WFSTで有名なMichel Relayの講演。すごい早口。
・最初にWFSTについて。WFSTの基礎。
・WFSTの基本演算。合成、決定化、最小化。最短距離パスの探索。
・応用。
・Pushdown automatonへの拡張。pdt.openfst.org から入手可能。
・WPDTのアルゴリズム。WFSTと大体同じ。
・応用:parsing, machine translation
プッシュダウンオートマトンへの拡張は今回のInterspeechでも別の発表があった。全体的にはあまり新しい情報はない講演だった。
コーヒーブレイク。同じケーキが大量にサーブされている。大量にあったので何個か食べた。
午前後半は、ポスター会場をざっと眺めた後、会場の端でずっと雑用をこなす。残念だがしょうがない。
11時半頃、PCのバッテリーが無くなってきたのと、眠くて具合が悪くなってきたので、いったんホテルに戻る。眠い上に胃腸の調子も悪い。バッテリーを充電しなが
ら1時間ほど仮眠。
13時に起きて、(あまり具合が良くないが)再び会場へ。
Language Modeling: New Models and Features
Paraphrastic Language Models (Cambridge U)
同じ意味に対する多様な言い回しを直接モデル化する試み。Paraphrase
pairを探すために、同じ左右コンテキスト(各3単語)で出現しやすい単語列を探し、その共起確率をパラフレーズの確率として使うようだ。計算にはWFSTを使う。単語
4gramから0.5ポイントぐらい改善。平滑化はないようなので、何か決まった言い回しがうまくはまったときだけ効果がある気がする。パラフレーズの例とし
て"I'd like to"みたいな定型句の変種が示されていた。機械翻訳とかには効くんじゃないかな。
Efficient Structured Language Modeling for Speech
Recognition    (JHU)
その昔(2000年)提案されたStructured Languge
Modelの改良。時間の半分以上はSLMの説明。今回の発表は、SLMが大きすぎるのを何とかしたいという話らしい。Stolcke
pruningと似たような方法を使うが、SLMの平滑化は短いコンテキストの確率との線形結合によるので、モデルを縮小するたびに結合係数を推定し直す必要があり、大変
そうだ。N-best rescoreによるモデル適用。4-gramに比べて0.6ポイント改善。
Towards Recurrent Neural Networks Language Models with Linguistic
and Contextual Features (Delft U of Tech)
2010年に提案されたRNN言語モデルに、単語以外の特徴を組み込む。組み込む特徴は、品詞・語根(lemma)の他に、話者の状況(砕けた対話、インタ
ビュー、電話会話、ディベート、ニュース音声、授業、など)を特徴とするところがおもしろい。評価はパープレキシティと単語予測精度。RNNLM単体で
Kneser-Ney
5-gramの半分のパープレキシティになるところがすごい。いろいろ入れた特徴のうち、品詞が一番効果があり、その他の特徴を全部入れるとちょっとだけ向上する。
Conversion of Recurrent Neural Network Language Models to Weighted
Finite State Transducers for Automatic Speech Recognition (IDIAP)
従来のRNN言語モデルはN-bestリスコアでしか使えなかったので、最初のデコーディングから使えるように、RNNLMをWFSTに変換した。RNNでは、コ
ンテキスト情報は多次元連続空間上のベクトルなので、量子化が必要。ここではk-meansを使ってベクトル量子化をした。また、Stolcke
pruningと似た方法によって状態の枝刈を行う。状態の量子化の影響は結構大きくて、PPでみると量子化の影響によってRNNの性能は5-gramより悪くなる。ま
た、枝刈を入れるとコードベクトルを増やしてもPPが下がらなくなる。実際にWFSTによる認識をしてみると(2パス:bigramまたはRNNLMで認識した
後、4gramまたは離散化しないRNNLMでリスコア)、最初にbigramでデコードした後でRNNLMでリスコアしたものがもっともよかった
(bigram+4gramと比べて0.3ポイント改善)。
Large Scale Hierarchical Neural Network Language Models (IBM/Nuance)
こちらはリカレントでないNN言語モデル。大量データでNN言語モデルを学習するため、クラス言語モデルをNNで表現する。(これがHierarchical
NNLMらしい)通常のNNLMでは、もっともよい条件で4-gramよりも0.7ポイント改善したが、学習に50日ぐらいかかる。クラスモデル化することで、性能を落と
さずに2日で学習できるようになった。さらに学習データを増やすと、クラスモデルで1ヶ月かけて作ったモデルでさらに0.3ポイント性能が上がる。n-
gram+NNLMにさらにModel Mを加えるとさらにちょっと上がる。しかしNNLMよりもRNNLMのほうがさらに性能がちょっと高いのであった。
A Sparse Plus Low Rank Maximum Entropy Language Model (U of
Washington)
MaxEnt言語モデルの改良。学習データが少ないときの性能を上げるのが目標。単語と履歴を特徴関数に入れるのではなくて、単語と履歴を多次元空間に射影する関
数を用意して、それらの重み付き内積をとる。このときの重み行列のランクを低減するために特異値分解を使うと、履歴と単語をそれぞれ低次元の連続空間に射影するよ
うな定式化になる。さらに重み行列に正則化項を入れて、重みが低ランクの重みとスパースな重み(例外的な単語連鎖を表現)の和になるように定式化する。
全体的に、言語モデルはいろいろ工夫するけど改善はちょっぴり、という状況が続いている。新しい技術で比較的注目できるのはRNNLMとModel Mかなあ。
この後、ポスター会場をざっと眺めて、いくつか発表を効いた後で早めにホテルに戻る。
ホテルに戻ったところで、仕事の続き。3時間ほど作業してようやく一段落。おなかの調子はよくないが、買っておいた食材が残っているので、今日はサーディンのトマ
トソーススパゲティ。ソースを薄めすぎてちょっと水っぽくなった。ガーリックがほしかったなあ。
食べ終わって再び仕事。Word文書の比較ができなくて苦労する。(本物のWordがあれば問題ないのだが、いま互換品しか使ってないので、その機能がないのだ)
いろいろやってうまくいかず、10時頃寝る。
前の日へ 目
次へ 次の日へ
9/12
