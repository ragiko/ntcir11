本報告では,大域的な文脈をモデル化する確率的LSA(Probabilistic Latent Semantic Analysis: PLSA)を利用した統計的言語モデルに注目し,このモデルを未知の文脈に適応させう方法を検討する.従来の適応方法は,モデルを作成するときと同じ最尤推定(EMアルゴリズム)をそのまま使うものであるが,未知の文脈に動的に適応させる場合は使える文脈は少量であり,過適応を起しやすい.本報告では一般に過適応しにくいと言われているベイズ学習(変分ベイズ学習)を用いた適応手法を検討し,unigramとtrigramモデルのtest-set perplexityを使って比較評価した.結果として,PLSAが得意とする中頻度語彙に対しては,特に適応に使える文脈の量が少ない場合,ベイズ学習を用いた適応がEM適応よりも安定して高性能であることを確認した.高い出現頻度を持つ語彙を含む場合は,EM適応の方が高い混合教のときunigramモデルで優位であったが,trigramモデルではベイズ適応が優位であった.
This paper describes a context adaptation method using variational Bayesian learning for a statistical language model based on PLSA (Probabilistic Latent Semantic Analysis) which models global context. Gildea and Hofmann (1999) proposed an original training and adaptation method for PLSA which is based on EM algorithm. However, the EM adaptation method tends to over fit to a context, because the context which can be used for dynamic adaptation is so smaller than that for training. To avoid over-fitting, we use a variational Bayesian learning method for the adaptation which could be tolerant to the over-fitting problem. We compare two methods in test-set perplexity of unigram and trigram models. The experiments show a stable high performance of the Bayesian adaptation for small contexts made up of medium frequency words in perplexity compared to the EM adaptation. For contexts made up of high and medium frequency words, a unigram perplexity of the EM adaptation is comparable or lower than that of the Bayesian adaptation, but the Bayesian adaptation is better in perplexity of trigram models.

