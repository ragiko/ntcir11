[機械学習] 第5回 全脳アーキテクチャ勉強会 参加メモ
「Deep Learning とベイジアンネットと強化学習を組み合わせた機構による、前頭前野周辺の計算論的モデルの構想」(産総研 一杉裕志氏)
前頭前野周辺に関する知見
大脳皮質 - 基底核ループは強化学習に関与
4つの平行したループ
運動野、前頭眼野、前頭前野、前帯状皮質
大脳皮質 - 線条体 - 黒質網様体・淡蒼球内節 - 視床
幼児の思考はグリーディーサーチ?モンテカルロ?
前頭前野周辺のアーキテクチャを推定するためのヒント
前頭前野周辺の4つの平行した大脳皮質 - 基底核ループは、階層型強化学習を行っているのではないか。
思考は最適意思決定の近似計算ではないか?
近似計算の方法そのものも経験から学習するのではないか?
「BDI —モデル、アーキテクチャ、論理—」(奈良女子大 新出尚之氏)
意図の理論
Bratmanが提唱 (1987)
BDI: 信念・願望・意図 (Belief-Desire-Intention) の3つの心的状態
BDIモデル
Rao, Georgeffらが提唱 (1991〜)
意図の理論に基づく、自律・合理的エージェントのモデル
BDIインタプリタ
option-generator
deliberate
update-intentions
execute
get-new-external-events
drop-{unsuccessful, impossible}-attributes
BDIモデルに元々ない部分
知能の獲得と利用(i.e. 学習)
社会性(社会的義務など)
プランニング(プランを動的に作成)
非合理的な行為(感情, etc)
BDIモデルは、人間の行為を決めるメカニズムのうち主に「頭で考える」部分を取り上げたもの
BDI logic
BDIエージェントを形式的に記述する様相論理体系
他分野との融合
機械学習との融合
BDI・・・熟考の部分
機械学習(特に強化学習)・・・技能の獲得と利用の部分
人間は行為にあたって両者を併用している
学習も「力任せ」ではなくある程度考えながら行っているかも
確率推論との融合
不確実な行為
確率的な因果関係
ベイジアンネットワークなどと関連?
プランナとの融合
プランライブラリにないプランの合成
人間が学習で得たスキルを他の問題にも応用することに似てる?
「強化学習から見た意思決定の階層」(グーグルジャパン 牧野貴樹氏)
強化学習
自律エージェントの意思決定メカニズム
未知の環境の中を探索しながら期待報酬和を最大化するためのエージェントの行動原理
データを収集するプロセスを含めて考えるという点でいわゆるビッグデータ(教師あり学習、教師なし学習)とは大きく異なる
試行錯誤しながら最適化(利用と探索のトレードオフ)
「不確かな時は楽観的に」原理
Playing Atari with Deep Learning + RL (Mnih+, 2013)
Deep Learning + RL でゲームを解く
Multi-armed Bandit
「状態」がない
Greedy algorithms
ε-Greedy algorithm
UCB1 algorithm (Auer+, 2002)
Thompson Sampling (Thompson, 1933) 
最近 Regret O(log t) が証明された (Kaufmann+, 2012)
TD-Learning
行動反復法を確率更新で求める手法
モデルフリー学習
ヒトの脳を考えるために
TDは学習するまでに多くの試行錯誤を必要とする
複雑な環境に対応できない
複雑な環境に対応するために
モデルベースアプローチ
階層的強化学習
モデルベース強化学習
モデルフリー強化学習は、実装はシンプルだが学習コストは大きい
試行錯誤の結果から、環境のモデルを構成することで、学習コストを減らすことができる
-> モデルベース強化学習
そのかわり計算コストがかかる
モデルベース強化学習のアプローチ
モデルをもとにDPで解く
モデルをサンプリングしてDPで解く
サンプリングにより、適度な探索が実現できる
モデルで発展をサンプリングして評価値を求める
近似計算
発展サンプリングの例
囲碁の盤面評価 (Gelly & Wang, 2006)
盤面の評価値をモンテカルロ法で計算(プレイアウト)
有望そうな手に対して多くのプレイアウトを割り当てたい
-> Multi-armed Bandit を使う
脳科学との対応
モデルベースとモデルフリー強化学習の融合(Dayan & Daw, 2008; Daw, 2011)
強化学習における問題の抽象化
複数の行動をまとめたものを「抽象化された行動」ととらえると問題が簡単になる
RLでは "Option" と呼ぶ
階層的強化学習のアルゴリズム
MAXQ (Dietterich, 2000)
サブゴールの選択を含めた強化学習問題(Semi-DMP)を定義
サブゴールの設定を人間が与える
HEXQ (Hengst, 2002/2008)
状態をいくつかの状態変数の組としてとらえる
状態変数の数だけ階層を作る(もっとも変化しやすい状態が最下層)
内発的動機づけによる学習
目新しい新奇な刺激を体験することに対し、外部からではない内的な報酬を与える
刺激のsaliencyに対応してoptionを作り、noveltyに対して報酬を与える
しばらく同じ刺激を得続けていると報酬がゼロになる
今後の方向性
Relational Reinforcement Learning
状態・行動が関係の形で定義される強化学習
状態空間の上で価値関数を近似する手法、シンボリック空間で直接DPを解こうとする手法
Permalink | コメント(0) | トラックバック(0) | 21:37
第5回 全脳アーキテクチャ勉強会 参加メモ - katz's adversaria
