最新アルバム「Ghosts」のリリースを終えたばかりのRobert Henkeに、自身のレコーディング・プロセス、インストゥルメントに対するユニークなアプローチ、バーチャルスペースのデザインなどについて話を聞きました。
このアルバム制作において、サウンドクオリティにはどのようなアプローチを採りましたか? またそれは作曲プロセスと作品にどのような影響をもたらしましたか?「サウンドクオリティ」という言葉は少し曖昧です。話を進める前にそれが何を意味するのかをはっきりさせておいた方がいいでしょう。 音楽では、結果とは常に主観的なものであり、歪みやアーチファクトというものを「信号」から切り離すことはしばしば不可能です。 ベースラインの歪みは技術的な問題なのか、それとも故意のものなのか? ここでの私の音楽についての話では「クオリティ」という語は使用せず、「サウンド」について話すことにしましょう。高めのサンプリングレートを使用するようになったのは、20kHzを超えるとどのような面白い効果があるのか、そして大幅にトランスポーズダウンするときにその効果がサウンドに反映されるのかどうかに興味があったからです。 20kHzを超える信号をキャプチャできる性能のいいマイクをいくつか持っているので、ありとあらゆる音源を96kHzサンプリングレートでレコーディングし始めました。 それをLiveのSamplerで再生し、2オクターブ下にトランスポーズしました。 96kHzのサンプルレートで可能な最高値は40kHzあたりです。 それを10Khzで2オクターブ下にトランスポーズし、5から10kHzの間に「隠しオクターブ」が現れるようにします。 44.1kHzのサンプルレートで録音されたサンプルを同じだけトランスポーズダウンすると、Samplerの補間設定(［Interpol］)が［Normal］か［No］に設定されている場合はダウンサウンプリング特有のアーチファクトが生じ、［Good］に設定されている場合はサウンドがぼやけます。 96kHzのレコーディングでは、「ヘッドルーム」と「空間感」が生まれます。 素材をトランスポーズダウンしても、サウンドが人工的になることはもうありません。 しかし今のところ、わくわくするような「隠された信号」にはまだ出会えていません。 この作業は、JPEGでなくRAWで写真を撮影するのに少し似ています。 ほとんどの場合どちらでもいいけれど、場合、あるいは処理によっては、違いが大きく影響するのです。より複雑なテクスチャを作成するために、独自の素材をリサンプリングするということを私はよくやります。 リサンプリング処理のルールはレコーディングと同じですから、私は新アルバムをまるごと96kHzで制作することに決めました。 これが可能になったのは、現在使用しているコンピュータの処理速度で、このサンプルレートのプロジェクトに対応できることが分かったからです。変化するさまざまなサンプルレートでのLiveを使った制作は少し難易度の高いもので、若干の訓練が必要でした。 Liveは、あらゆるサンプルレートのファイルを、サウンドカードの現在のサンプルレートで再生します。 だから、間違ったサンプルレートでプロジェクトを作業していることに気づきにくいのです。 サウンドカードの現在のサンプルレートをモニタし、レンダリング設定に注意を払うことが必須のタスクとなります。 個人的に、サンプルレート変換により生じるアーチファクトはあまり気になりません。ほとんどの場合聞こえないし、それが分析器にスペクトルとして表示されていたとしても、それは私にとってはどうでもいいことです。 大きな違いを生むのは、インストゥルメントとエフェクトの効果です。 私のドラムのほとんどはOperatorで作成されており、FM合成の使用ではエイリアシングとトランケーションエラーが重要な部分を占めています。 Operatorで作成した44.1KHzのハイハットのプリセットはすべて、別のサンプルレートではまったく異なったサウンドになります。 なので、「Metal_Hihat_03_sr44', 'Zoink_sr96」というふうに、プリセット名にサンプルレートを書き込むことが習慣になりました。 あらゆる種類のビットクラッシング、サンプルレートリダクションプラグイン、その他のディストーションアルゴリズムでのサウンドの違いも明らかです。 場合によっては、サウンドを維持するために特定のトラックを44.1kHzで再生してからトラックをフリーズし、サンプルレートを96kHzに変更しました。 これこそLiveが優れていると思う理由のひとつです。ほぼすべての問題になんらかの解決策を見出すことができるんですよ。かといって、解決策がいつも簡単に見つかるというわけではありませんが。アルバムのミキシングとマスタリングの方法について、当初のアイデアを変更したと話していましたね。 これについて詳しくお話しいただけますか?私は自宅で作業するのが好きなんです。 プロ仕様に近いスタジオを所有していたこともあったんですが、建物が売りに出されてしまい、引っ越さなくてはならなくなりました。 それ以来、音響状態のよい小さな部屋で仕事をしていますが、スタジオ仕様の部屋ではありません。 「Ghosts」の制作中は、ラフミックスまでを自宅で制作し、最終的なミックスダウンは「本物の」スタジオでやろうと思っていたんです。作品を聴き込んでいない外部のエンジニアを使ってね。 それは、マスタリング/レコーディングスタジオを経営する友人のDaniel Dettwilerに会いにバーゼルを訪れたときに起こったんです。 彼が所有していたスイスの小さな会社Straussが製作している素晴らしいスピーカーに度肝を抜かれてしまいました。 この会社は、私の部屋に(ついでに私の予算にも)収まり切らない巨大なモニターも扱っていますが、小型のニアフィールドモニターも扱っていました。 ある日、その小さなスピーカーを私のスタジオでテストする機会を得ることができたんです。 セッションを試聴した後、すぐに購入しました。その後数週間使ってみて、自分のミキシングに確信が持てたので、ミックスダウンを外で行わず、マスタリングだけをバーゼルのDanielと行うと決めたんです。 制作はすべてLiveで行いました。バーゼルでは、セットをラップトップから再生し、Pro Toolsのステレオミックスに転送してマスタリングしました。プロセス中はずっと96kHz/24ビットのままです。
Robert Henkeのホームスタジオ。 モニター: Strauss SE-NF3とGenelec 8020Aアルバムをダウンロード用に24ビット/96kHzでもリリースしましたね。 制作を高解像度で行うことは理解できるのですが、今回のこのリリースは、CDの44.1kHzより優れたサウンドだと考えてのことですか?人間の聴力で聞き分けられるのは、20kHzがせいぜいです。 それ以上は聞き分けられないと私も考えています。 しかし、2つのバージョンを切り替えながら聴いてみるというテストをDanielとしたんですが、2人とも違いをはっきりと感じることができたのです。とくに空間感が違っていました。 これについてはある理論を思いついたんですが、専門家と詳しく議論する時間はまだ持てていません。ただ、2つ以上のスピーカーを使用する場合の96kHzの単一のサンプルで位相の違いを聞き分けられる能力が人間にはあるのではないか、と漠然と考えています。「Ghosts」は外部機器を使用せずLiveのみでミキシングされていますね。 スタジオでミックスするのとは違うと思いますが、その点はいかがでしょうか。そうですね。 重要な決断でした。 Live、または他のDAWだけを使用した制作とミキシングと、ミキシングデスクや外部機器を使用した制作とミキシングには、どちらにもそれぞれの利点があります。 ただ、利点と問題については多くの混乱があるような気がします。 これについては、私もここでコメントしておくべきかもしれません。Abletonフォーラムで、Liveの「サウンドクオリティ」について長い間議論してきたこともありますし。 まず、外部ミキシングを使用しないでLiveだけで制作を完了させる場合、まずなによりもコストを大きく節約することができます。これは重要な点です。 クオリティの高い16チャンネル以上のD/A(デジタル/アナログ)コンバーター、アナログコンソール、その他の外部機器はとても高額です。 このような機器を備えたスタジオのレンタル費用も高価です。そんなお金があれば、優れたスピーカーや、処理能力の高いコンピュータや、音楽レッスンに使いたいと思うかもしれませんよね。2つ目のポイントは、スペースと作業のエルゴノミクスです。 寝室を兼ねるような小さなスタジオでは、大型ミキサーを置くスペースはありません。 またリスニング位置も、ラップトップ前かミキサー前かのどちらかになります。 「スイートスポット」と呼ばれるリスニングに最適な位置に座って作業しなければ、制作やミキシングでは意味がありません。 また、たとえこれらを上手く配置できたとしても、もう1つ私にとって非常に重要な要件がありました。ミックスと共にプロジェクト全体をアーカイブとして記録したいということ、それと同時に別のプロジェクトの作業も進めたいということです。 外部エフェクトやミキシングを使用する場合、これはほぼ不可能です。多数のスタジオ・アシスタントを抱えているのなら別ですが…。 「Ghosts」の制作中は、各楽曲の作業を同時進行で行っていました。これもソフトウェア単体ですべての作業を行う場合の利点でしょう。ソフトウェア単体でのミキシングの利点を並べてきましたが、それでは今度は別の視点から見て行きましょう。振り返ってみると、外部ミキシング・ボードや楽曲を聴き込んでいないミキシング・エンジニアを使ってアルバムをミキシングすることには一定の利点がありました。 技術的なサウンドクオリティという点ではありません。仕事の流れ上、DAW内でミキシングを行うと集中力を失いやすくなるのです。 クリエイターは、制作中幾度となく同じミックスを聴いています。そのため、ミックスダウン時に、外部エンジニアなら聞き逃さないようなことに気が付きにくくなるのです。例を挙げましょう。ある楽曲のMIDIトラックのトラックミキサーを-40dBに設定してしまっていたことがありました。 どこでどうしてこうなってしまったのか分かりませんが、トラックのファイナライズ時にも気付きませんでした。 アルバムをステージ演奏の素材にするためにステムを作成して、初めてそれに気付いたのです。 ドラムセットにいいサウンドを加えたはずのトラックが、まったく意味のないものになってしまいました。 セット全体を外部ボードにレイアウトし、各フェーダにラベルを張り、各チャンネルを確認するという手順があれば、このようなことは起きなかったでしょう。 それに、EQへのコントロールも重要です。 この件に関する現在の私の見解をまとめれば、次のようなものになるでしょう。次回もソフトウェアだけで制作し、作品を完成させるつもりで作業する。その後、スタジオに入ってミックスにさらに手を加える。 こうすれば、ミックスが上手くいかなかったとしてもLiveで作成したバージョンがあります。ミックスが上手くいけばしめたものです。どのような状況であっても自分でやろうとは思わないプロセスが、マスタリングです。 音のバランスを判断し、最終的に正しい判断を下すための助言をくれる外部の人間が私にはどうしても必要です。 マスタリングセッションには必ず同席します。マスタリングエンジニアと私で議論を重ね、多くの場合、私が「このミッドレンジを除去して欲しい」といった最終的な決断を下します。 それでも、私が間違った方向に進みそうになったとき、それを修正してくれる経験の豊富な外部の人間がいるというのは大切なことです。「Ghosts」は素晴らしい空間感を感じさせます。それは至る所で感じられますが、要所では抗いがたいほどパワフルに表現されています。 このようなダイナミクスを得るためにどのようなアプローチをとったのでしょうか。空間というのは、特にエレクトロニック音楽では興味深いトピックです。 アコースティック楽器のサウンドでは、空間感がすでに存在しています。さまざまな帯域がさまざまな強度でいろいろな方向へと放射され、室内で反響します。 いろんなことが起こっているわけです。 エレクトロニック音楽ではこのような現象は自然には生じませんから、現象を構築してやる必要があるわけです。 私たちは皆、自分自身の世界を設計する空間デザイナーであり、音響設計者なのです。 リバーブやダイナミクスを使用するのが好きです。サウンドのディケイフェイズに耳を傾け、2つのスピーカーの間で何かが開けていくような感覚を感じるのが大好きなんです。 特別なアプローチはありません。ただ集中してこれらの側面に耳を澄まします。 Liveに搭載されているReverbに満足しているのはそのためです。「優れた」「リアルな」サウンドを生み出すからではなく、自分の欲しい音響パレットを得るためにどう操作すればいいかよく知っているからです。 それに、何か他のリバーブを使用したいなら、替わりとなるものはたくさんあります。他のリバーブプラグイン、コンボリューションリバーブ、ハードウェアリバーブなどや、信号を外部にフィードする方法など(私は小さなアナログミキサーを使用しています)です。 とはいえ、今となっては、部屋を暖めたいというのでもなければハードウェアの電源を入れることはほとんどありません。このアルバムでは、いくつか印象的なサウンドデザインを聞くことができます。 どのAbletonインストゥルメントを使用したのですか?プリセットの「コンテンツ」というものが大嫌いなので、新しいシンセを手に入れたら、まずOperatorのインスタンスにトラックを入れて、デフォルトプリセットからベースドラムかベースラインのサウンドを作成します。 ファクトリープリセットを削除するんです。 マシンを自分の手で検証していくのが好きなので。 プリセットを確認することは、後の段階でしか行いません。何か基本的なことを見逃していないかどうかを確かめる時だけです。 その後、自作のプリセットに戻ります。 これができるのは、私にはサウンドについてあれこれ指図する人間がいないからです。コマーシャル音楽を作成する時みたいに、「誰々の曲みたいにもっとヒップに」とか注文を付けられることはありません。 私にとって、サウンド作成はエレクトロニクスにおける楽しみの大部分を占めています。自分の音色を定義することができるのがいいんです。Operatorが私の動力です。 今考えれば興味深いのですが、2004年にOperatorをデザインした当初の目的は、どちらかというとベーシックなFMシンセをより効果的な方法でカバーする「エントリーレベル」のインストゥルメントを作成することでした。 当時、スタジオではより「ビッグ」なハードウェアFMシンセを使用していたので、Liveユーザが手軽に使用できるようなFMが欲しいと思っていたんです。 その後Operatorは成長を続け、優れたディテールが追加されていきました。 2012年の今、ハードウェアFMシンセに触れることはほとんどなくなりました。ドラム、パッド、ベースラインなど、いまでは全サウンドの70パーセントにOperatorを使用しています。
Henkeのシンセの数々: PPG Wave 2.3とNED Synclavier II(左)、Sequential Prophet VS、Oberheim Xpander、Yamaha TG-77およびDX27(右)「特定の何か」を加えたい時は、他のインストゥルメントも使用します。 すでにいいグルーヴがあって、スタジオで少し実験してみたいという段階になったら、80年代の巨大なデジタルハードウェアやソフトウェアインストゥルメントを演奏してみます。 この時点で探しているのは、楽曲のカラーを決める極めて特定なサウンドです。 レコーディング、シンセ、エフェクトなどからこのサウンドを探します。「Ghosts」では、Tension、Corpus、Liveのワープエンジン、Prophet VSハードウェアシンセ、金属製の物体をレコーディングしたものを多数使用しました。 Tensionは猛獣のようなインストゥルメントで、強烈な個性を持っています。 Tensionについては、ギターやバイオリンを指弾きした時のリアルなサウンドを求めているわけではなく、 弦楽器を演奏する時に生じるいくつかの演算をシミュレートする複雑なアルゴリズム、という風にとらえています。 何が起こっているのかだいたいのことは理解できるけれど、何かクールなサウンドが生まれるまで設定を変更していろんな実験ができる、余白のあるインストゥルメントです。
「Phenomenon」という曲で使用した弦を弾くような「ミューン」というサウンドは、Tensionのインスタンスをいくつか使用して作成しました。 サウンドデザインの大部分にはMultiband Dynamicsデバイスを使用し、モデルのエラーすべてを浮き立たせています。 「ミューン」というサウンドは、Max for Liveデバイスでいくつかのパラメータをモジュレートした結果です。 サウンド自体の音量は非常に低いのですが、強力なコンプレッションにより強い印象が与えられています。エラーが一種のテーマになっているのです。 モジュレーション、そして意図的なモジュレーションの欠如、それが私の音楽の重要なポイントです。 「Discontinuity」というトラックを特徴付けるサウンドは、Corpusデバイスを駆動する短いインパルスです。ピッチをモジュレートするLFOを切り替えています。 トラックを形作っているのは、モジュレートされたサウンドとスタティックなサウンドのコントラストです。私にとってLiveは、クラシックなDAWというよりもモジュラーシンセです。 オーディオを内部でルーティングしMaxを統合することができ、ワープエンジンも面白い。Liveの「楽器」としてのクオリティこそ、私がLiveを好む理由です。 特にワーピングについて強調しておきたいです。なぜならこれも、元の目的とは異なる使用によって、面白い音楽的結果が生み出されるテクノロジーだからです。 テクスチャのサンプルをリサンプルし、ピッチシフトやタイムストレッチを加えて音響特性を変化させるのが好きです。別のワープモードや設定で再生して、さらに結果をリサンプリングし、フィルターし、またリサンプリングし、リバーブ、リバース、ワープなどと続けます。 可能性は無限です。 実は、テンポマッチのためにワープを使用したことはまだありません。 オーディオファイルは、タイムストレッチすると酷いサウンドになりますからね。 時間を引き伸ばすことはできないんです。どんなアルゴリズムを使用しても、アーチファクトなしには不可能です。 私の話し声をコンピュータで20%遅くすれば、酔っ払っているように聞こえてしまうでしょう。グラニュラー合成用の優れたMax for Liveインストゥルメント、Monolake Granulatorをデザインされましたね。 「Ghosts」ではどこに使用されていますか?パッドサウンドのいくつかはGranulatorを使用して作成しています。 「Unstable Matter」にその多くを聞くことができます。 「Aligning the Daemon」のオルガンのようなサウンドとコーラスボイスもグラニュラーがベースです。サラウンドサウンドのセットアップを使用してライブパフォーマンスを行っていますね。 このセットアップではLiveをどのように使用しているのですか?昔は、ライブショーのセットはゆっくりと進化するミュータントのような感じでした。 サウンドチェックのたび、またはホテルの部屋で休むたびに、何かを加えたり変更したりしていました。 結果としてセットはかなり乱雑なものとなり、ごくわずかの制約(チャンネル数など)がこのカオスを制御していました。 「Ghosts in Surround」ツアーでは、初めからかなり違ったことをしたいと思っていました。アルバムにタイトにつながったライブショーを作り、毎夜大きな変更を加えることなく演奏するという方法です。まず、ライブで演奏したいトラックの順序を決めることから始めました。細かいことを決める前に、ショーの基本的な構成をまず頭に描きました。 その後、アルバムの全トラックのアレンジのスクリーンショットを取り、長い時間をかけて検討しました。 次の3つの事柄についてアイデアを模索しました。 まず、楽器編成という観点から、トラックをどう整理するか。 次に類似点と相違点について。 各トラックの構造を確認し、ブレイク、ループ、パートなどがどこにあるかに注目しました。 そして最後に、これらのトラックをマルチチャンネルサラウンドのセットアップでどのように表現するかです。 Liveでの作業を始める前に、紙にいろんなことを書きとめました。 そのあとで、要所となるいくつかのポイントについて決めていきました。 Liveセットのトラック数は?…12でいこう。 各トラックの主要な役割は? …「ベースドラム」、「ベースライン」、「コード」、「スネア1」という感じかな? サラウンドでどの要素をパンしようか?スタティックなままにしておくのはどの要素か、それとも全てのトラックか?…ベースドラムはスタティックで、すべてのチャンネルから常に聞こえるようにしよう…といった感じです。その後、Liveで各楽曲をもう一度開き、ステムをエクスポートしました。 次に、ステムを使用してオリジナルの楽曲を再構築し、上手くいくかどうか確認しました。 ここでかなりの簡素化を行いました、素材をより簡単に演奏できるようにね。ライブパフォーマンス中にその場の雰囲気に合わせてエフェクトを加えられるよう、ステムのレンダリングの前にエフェクトを取り除きました。 これが終わったら、各クリップ内でステムをカットしてみたり、セッションビューで再アレンジしてみたりと、いろいろなことを試しました。 結果として、アルバムの全曲を含む、たくさんのクリップが連なった1つの長いセットが完成しました。 この状態なら、上から下へナビゲートしていくこともできますし、少し冒険してみたくなったらジャンプして移動することもできます。 センドにエフェクトを追加したり、ミックスを変化させたり、パートをスキップしたり長めに再生させたりもできます。 作成には4週間かかりました。大変な作業ですが、その甲斐はありましたね。ここまで終わってから、MIDIトラックを追加しました。「Ghosts」ツアーでビジュアル面を担当してくれたTarik Barriが動かすビデオレンダリングソフトウェアに、Max for LiveとOSC経由でキュー信号を送信するMIDIトラックです。その後、使用するハードウェアコントローラを決めました。 最終的に、16MIDIフェーダ(Doepfer Pocket Fader)と、LemurをインストールしたiPadの2ユニットを使用することに決めました。これで30以上のフェーダと多数のボタンが使用できます。 今後、iPadをもう1台追加して、さらにコントロールを拡張するつもりです。多くのパラメータをコントロールできるので、MIDIトラックを追加して、ステム内の素材の一部をMIDIクリップとシンセに置き換え、シンセパラメータを使ってリアルタイムで演奏できるようにしました。 ライブではベースラインのサウンドを主に演奏しています。 ウォブルエフェクトも使用できます。 備えあれば…ですからね。(これで完了ではないですが)最後にサラウンドコントロールを行いました。 会場はそれぞれ大きく異なっており、さまざまなスピーカーセットアップに適応しなければならなかったので、フレキシブルかつ安定性の高いスキームが必要でした。 私のセットアップを、私は「4.2.2」と呼んでいます。 4つのメインチャンネル(フロントに2、バックに2)と、状況に応じて置き場所を変える2つの追加チャンネル(6つのスピーカーで円を描くため、またはステージセンターや天井にスピーカーを追加するため)、それにサブ用の2チャンネルです。vLiveのセンドを使ってそれぞれのチャンネルをフィードし、Max for Liveパッチでセンドを動的にコントロールしました。 かなりの数の「パンニングトラック」を設定して、これらのトラックごとにM4Lデバイスを置いてセンドをコントロールします。 これらのデバイスはすべてマスタートラックの「Surround Master」デバイスからコントロールされます。このデバイスでは、パンニングチャンネル上でサウンドを同期させてまたは自由にコントロールすることができます。 いまのところ、円・らせん・ジャンプなどの動きとスタティックなルーティングを用意していますが、まだまだ改良の余地はあります。 らせんの動きにはかなりの数を用意しましたが、クラブやコンサートでこれをどう機能させるかはまだ研究段階です。 インストゥルメントは完成したので、次はその機能と操作を学ばなければなりません。この空間的な側面が単なるランダムな効果として発揮されるのではなく、他の要素と上手く調和するようになればいいなと思いますが、まだそこには至っていませんね。—Monolakeについてさらに詳しく見る

