C言語を勉強するとき,文法や標準関数をマスターすることに並ぶくらい重要なのが,デバッガを使いこなせるようになることです.C言語を習いたての頃は,書いたコードが予期せぬ動作をしたり,ポインタの使い方を間違えてプロセスを落とすこと(セグメンテーション違反)なんてザラです.そういう失敗をたくさん経験して,自分の書いたコードの間違いに気づき,修正していくことでコーディング力が養われていきます.しかし,自分の書いたプログラムの何処が悪いのか,皆目見当もつかないような状況に陥ってしまうと,プログラミングが嫌いになってしまいます.
デバッガを文字通りに解釈すると「バグを取る」ためのツールで,プログラマにとってはコードの間違いを発見するための検証手段として欠かせません.これに加えて,正しいコードをデバッガ上で動かしてみることも,C言語の理解促進のために重要であると私は考えています.ループや条件分岐でプログラムの流れがどのように変わるのか,ポインタは具体的にどんな値を取っているのか,main関数のargcやargvにはどんな値がセットされているのか,関数呼び出しでどのようにスタックフレームができるのか,auto変数・static変数でメモリ空間がどのように使われているのか等,デバッガを使ってプログラムの動きを追ってみることで,C言語やコンピュータ・アーキテクチャの勉強になります.
かくいう私も20数年前,Microsoft Quick C 2.0という開発環境でC言語を勉強しました.20年以上前の製品ですが,Quick CはC言語の初心者に優しい逸品でした.MS-DOS上で動作するコンソール・アプリケーションながら開発統合環境として動作し,エディタ,コンパイラ,デバッガの機能が一通り揃っていました.特にデバッガが秀逸で,ブレークポイント,ステップ実行,ウォッチ変数などがエディタ上から操作できました.Quick Cのデバッガが使いやすかったお蔭で,挫折せずにC言語の勉強が出来たと言っても過言ではありません.この頃に習得したデバッグの技(操作体系)から離れられず,C/C++のコーディングにはMicrosoft Visual Studioを用いてきました.研究用のC言語のプログラムは,Windows上で開発してからlinux上に移植しています.
今回,東北大学のプログラミング演習Aという講義でデバッガの使い方を教えようと思い立ち,色々調べていくと,emacsとgdbでMicrosoft Visual Studioに近いデバッグ環境が実現できることを知りました.元々emacs+gcc使いの人には常識なのかもしれませんが,”emacs gdb”で検索して上位に出てくるようなページを見るだけでは,いまいちピンと来ていませんでした.あまりにも感動したので(本当は講義のためですが),emacsでC言語のプログラムをデバッグする例を紹介するムービーを作っちゃいました.サクサクとデバッグできる感じを伝えたかったので,ゆっくりと説明するような作りになっておらず,字幕を読むのはキツイかもしれません.そんなときは,ムービーを止めながら観て頂ければと思います.文字が滲んで見づらい場合は画質を上げて下さい(オリジナルは1080pのHD画質).
このムービーで使っているバグ入りコード(sum.c)とemacsの設定ファイル(gdb.el)です.この設定ファイルでは,gdb-many-window,gdb-use-separate-io-buffer,gud-tooltip-modeと,以下のようなショートカットキーを採用し,快適なデバッグ環境を実現しています.
[F1]: カーソル位置(もしくはリージョン)の変数の内容を表示(gud-print)
[Shift]+[F1]: カーソル位置の変数をウォッチに追加(gud-watch)
[F5]: プログラムの実行を再開(gud-cont)
[Shift]+[F5]: デバッガを終了
[F6]: カーソル行まで実行(gud-until)
[F9]: カーソル行のブレークポイントを設定/解除(gud-breakもしくはgud-remove)
[Shift]+[F9]: main関数の先頭行にブレークポイントを設定(gdbに”break main”と入力するのと等価)
[F10]: ステップオーバー(現在行を実行,関数の中には入らない)(gud-next)
[F11]: ステップイン(現在行を実行,関数の中にも入る)(gud-step)
[Shift]+[F11]: ステップアウト(現在の関数を抜けるまで実行)(gud-finish)
(GUDのデフォルトでは,ステップオーバーやステップインのために3ストロークが必要のようですが,それって疲れないのかなぁ)
昨日,松島マラソンの10kmの部に出てきました.朝7時に車で家を出て,県道8号線(利府バイパス)で松島に向かいました.8時には会場付近に着き,近くの小学校に車を停めました.この時点で,どの駐車場も満車に近く,もう少し遅かったら車を停められなかったかもしれません.
会場で受け付けを済ませ,ゼッケンを受け取ります.ゼッケンの裏にはRFIDタグが付いていて,タイム計測が完全自動化されています.10kmの部のスタートまで時間があるので,KHB東日本放送のマスコット「ぐりり」と写真を撮ったりして過ごします.「ぐりり」はネコ目イタチ科のオコジョだったのか～.
スタートは目標タイム順に並ぶようにできていて,私は目標タイム65分以上の最も遅いレーンに並びました.3,000人弱が一緒にスタートするので,自分の所からスタートラインを通過するまで2分くらいかかりました.コースはアップダウンがあり,特にコース中で6回くらいある登りがしんどかったです.不本意ながら,登りで歩くことになってしまい,そのままペースが作れない展開となってしまいました.公式記録は1:02:52.スタートゲート通過からフィニッシュライン通過までの参考記録は,1:00:46でした.10kmを長いとは感じませんでしたが,体がついていけず,不完全燃焼に終わりました.
娘はマラソンの概念を理解できないようで,スタート後に私がいなくなったと思って,機嫌が悪かったようです.ゴール直後に抱っこしろとせがむので,ヘロヘロになりながら抱っこ.完走者に配られるTシャツを着て,記念撮影.
松島は今年の2月に行ったことがあったのですが,その時と殆ど変わらない町並みに驚きました.連休ということで人出も多く,どこも大賑わいでした.本当は観光をしたかったのですが,駐車場がどこも満車で,昼食後に娘が車の中で寝てしまったので,あきらめて帰ってきました.
今日は足全体が筋肉痛.このコースよりもさらにアップダウンが激しい青葉山の駅伝は,一から鍛えなおさないと本当にやばいなぁと痛感しました.
昨日プレスリリースの通り,JST戦略的創造研究推進事業「さきがけ」の研究領域「情報環境と人」に,自分が申請していた研究課題が採択されました.研究課題名は「知識の自動獲得・構造化に基づく情報の論理構造とリスクの分析」です.研究の概要は次の通りです.
ウェブやソーシャルメディアなどの新しい情報環境により、情報の流通が加速する一方、偏った情報やデマなどの拡散による社会の混乱や不安が増大しています。本研究では、ネット上で言及されている物・事態に関する知識を計算機がロバストに獲得・活用する言語処理技術を基盤として、流通している情報の背後にある論理構造を解析し、その整合性を分析することで、安全・危険に関する多角的な判断材料を人や社会に提供します。
研究総括の石田亨先生の総評にもありますが,この研究は東日本大震災を強く意識したものになっています.世の中の状況に左右されず,研究者は自身の研究に邁進すべきという考え方もありますが,自分の研究を社会にどう役立てるか,高い意識を常に保ちたいと考えています.私も震災復興に関するシンポジウムやミーティング等に参加し,言語処理(さらには情報科学)をどのように役立てるか考えてきました.これらの場で,人命の救助や医療,建物や水道などの生活インフラの復旧,災害に強い情報通信インフラの整備,ソーラーやバイオマス発電などのエネルギー施策,ロボットによる原発対応などの報告を伺う度に,言語処理を役立てることの難しさを感じます(その点,GoogleのPerson Finderや自動車通行実績情報マップ,ANPI NLPは,スピード感があってすごく良いプロジェクトだと思います).
ただ,震災後の日本の混乱を見ていると,言語処理の重要性はむしろ増しているように思います.震災ではウェブやソーシャルメディアなどの情報環境が大活躍しましたが,一方で偏った情報の拡散による社会の混乱・不安を招いています.情報技術がどんなに進歩しても,人間の情報処理能力は向上しませんので,大量の情報を有効に活用できません.社会は高度に専門化されているため,そもそも情報の内容を理解できない場合もあります.このような状況では,各人が情報の内容を理解を伴なわず,情報の発信者や怖さといった周辺的情報に基づいて意思決定をしてしまうことになります.さらに悪いことに,Twitterなどで誰もが気軽に情報を発信できるようになると,周辺的情報のみに依存した反射的な情報伝搬が増え,デマや偏見・差別が助長される要因となります.
どの情報が正しいのか人間ですら分からない状況のなか,計算機に情報の正しさを検証してもらうのは不可能です.しかも,最終的に情報を活用して意思決定を行うのは人間です.私も震災後に身の回りの人の説得に失敗した経験があり,人間の意思決定のやり方を変えてもらうことの難しさを痛感しました.このような動機により,意思決定に参考になりそうな情報を自動的,もしくはインタラクティブに提示することで,ユーザの情報処理能力を高めるための支援をしてみよう,というのが本研究のテーマです.
具体的には,ネット上のコンテンツを自動的に分析し,人間が意思決定を行う際に参考となるリスク情報(目的の達成度合い,安全度,危険度)を多角的に集約することを目標としています.このとき,ネット上から自動的に獲得したエンティティに関する知識,イベント及びイベント間(含意関係や因果関係など)の知識を総動員することで,流通している情報の論理構造(例えば二つの文がだいたい同じ事を言っているとか,参考情報としての筋の良さ)を明らかに出来るかどうかが,言語処理研究としての課題となります.
震災後の社会の混乱を目の当たりにしつつ,乾さんの言論マップの現状などを聞きながら,研究の構想を練りました.今まで私が進めてきた研究をキーワードで表現すると,文書自動要約(～D2あたりまで),生命・医学文献からの知識獲得(NaCTeM以降)なので,獲得した知識をきちんと活用し,言語理解に一歩近づいた文書自動要約に挑戦することになるのだと思います.今の自分にはちょっと背伸びした研究テーマであるため,上手くいくのかどうか,不安とワクワク感でいっぱいです.
Não Aqui!
