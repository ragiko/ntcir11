
パターンには、音声、画像、図形、文字などがある。これらが何であるかを認識することをパターン認識と呼び、音声認識の応用は音声入力装置に、画像認識の応用は顔や指紋の照合、画像ファイルの検索などに使われる。文字認識は、大別して、手書き文字の認識と、印刷文字の認識に分けられる。ここで紹介するのは印刷文字認識の一種で、ゴシックのアルファベットを認識する。
本文の内容と類似のもの(アルファベットではなく、自動車等のナンバープレートに用いられる数字を対象)が、「ニューラルネットワークを用いたパターン認識」と題して、すでにCodeZine上に寄稿してあり、こちらに原理とプログラムを詳述してあるので、参照されたい。
ニューロンとニューラルネットワーク
ニューロンは脳の神経細胞のことであり、人には数100億個のニューロンがあり、それらが網目のように結合してニューラルネットを形成している。
ニューラルネットワークは、それをコンピュータ上でシミュレーションするものである。
原理は非常に古く、多くの研究がある。最近のAdaBoostブームもあり、やや影が薄いが、捨てたものでも無い。
バックプロパゲーション(Backpropagation, 誤差逆伝播)法
ニューラルネットワークの主要なアルゴリズムであるバックプロパゲーション法は、学習させるべき教師信号と出力結果を比較し、その誤差をフィードバックして、重み係数やしきい値を修正する。
バックプロパゲーション法の欠点は、
1)学習速度を上げようとしてフィードバックを大きくすると、振動が起きて収束しなくなる。
2)ローカルミニマ(local minima、真の最小ではなく、それより大きい局所的な窪み)に陥って、それ以上学習が進まなくなってしまうことがある。
である。1)に対しては、後述する係数のALPHAとBETAを試行錯誤で決めて用い、2)に対しては、必要に応じて「再学習」させることによって、ローカルミニマからの脱出を図る。
ここで用いるニューラルネットワーク
認識すべきパターンを受付ける入力層(7 x 11 = 77)、認識結果を表示する出力層(10)、その中間の隠れ層(ここでは16を使用)から成る三層構造を使用する。入力層と隠れ層の間には、それぞれの入力に対する「重み」があり、さらに、判断に対する「しきい値」がある。この結果に対して、シグモイド関数と呼ばれ、出力結果を0から1の間に補正する処理が行われて、最終的な隠れ層の出力となる。
隠れ層と出力層との間にも、同様の関係がある。
順方向演算
これは、学習のために入力パターンを与え、教師信号と比較する過程と、学習後の認識の過程で使用する。
入力層から隠れ層への変換
入力層の値に重みを掛けて、しきい値を引いたものを求め、それをシグモイド関数に当てはめたものが隠れ層の値となる。
inputを入力層の値、weight_ihを入力層と隠れ層の間の重み、threshold_hを隠れ層のしきい値とし、入力層、隠れ層のインデックスをiおよびjとして、隠れ層の値hiddenを下式により求める。
ただし、シグモイド関数は、下式であらわされ、 x=0 のとき sigmoid(x)=0.5、x=∞のとき
sigmoid(x)=1.0、x=-∞ のとき sigmoid()=0 である。ここでは、試行錯誤の結果、BETA=1.2を用いている。
隠れ層から出力層への変換
同様に、隠れ層の値に重みを掛けて、しきい値を引いたものを求め、それをシグモイド関数に当てはめたものが出力層の値となる。
weight_hoは隠れ層と出力層の間の重み、threshold_oは出力層のしきい値である。出力層のインデックスをkとする。outputは出力層の値である。
出力層の誤差計算
学習のための入力に対する結果outputと教師信号teachとを比較して、出力層の誤差error_oを計算する。
隠れ層の誤差計算
さらに、出力層の誤差error_o、隠れ層と出力層の間の重みweight_ho、隠れ層の出力hiddenからさかのぼって、隠れ層の誤差error_hを計算する。
逆方向演算(誤差逆伝搬)
各誤差を用いてフィードバックし、重みを修正する。
同様に、しきい値を修正する。
ALPHAは、逆方向演算(バックプロパゲーション)に用いられ、重みとしきい値に対する修正量の係数(ここでは、試行錯誤の結果1.2を使用している)である。これを大きくするとフィードバックが強まるので、学習時間が短縮されるが、大き過ぎると不安定になる。
学習の回数
あらかじめ分かっているサンプルをニューラルネットワークの入力層に与え、その出力層に現れる結果と、期待されるべき出力(教師信号)とを比較し、相違(誤差)がある場合は、それに応じてフィードバックすることを学習と言う。
学習は、誤差が一定以下になるまで、次のようにして繰り返して行われる(ここで紹介するんのは、A〜Jの認識)。
パターンAの学習をm回  パターンBの学習をm回  ・・・ パターンJの学習をm回
ここで、同一パターンの繰り返し学習を内部サイクルと呼び、この例では内部サイクル数がmである。内部サイクルをすべてのパターンについて順次行うと、再び最初のパターンから繰り返す。この繰り返しを外部サイクルと呼び、n回繰り返す。結局、合計の学習回数は、m x nになる。
アプレットの使い方
最初は「学習モード」になっていて、上部には学習するパターンが表示され、内部サイクル数と外部サイクル数を設定できるようになっている。いずれも、最初は20になっているが、50と100に変更できる。
これらのサイクル数は、小さいと学習時間が短くて済むが、学習が十分でない。それぞれ20のままで、「学習/再学習」ボタンをクリックすると、学習は行われるが、「学習結果の確認」欄に「?」が並ぶ。実行中の各サイクルの回数と二乗誤差の合計が表示される。
内部サイクル数や外部サイクル数を増やして、「学習/再学習」ボタンのクリックを繰り返し、最終的に、「学習結果の確認」欄に、斜めに赤の「YES」が全部並び、「?」マークが無くなったら、学習に成功したので、「入力/再入力」ボタンをクリックして、「認識モード」に移る。
「入力/再入力」ボタンをクリックすると、左下に、マウスで手書き入力できる枠が現れる。これに適当に入力して「認識」ボタンをクリックすると、認識結果が右に表示される。ただし、確率は目安であり、合計しても1にならないことがある。別のパターンを認識させたい場合は、再び「入力/再入力」ボタンをクリックする。
これは、手書き文字認識として作られていないので、手書きの崩れた入力には誤りが大きくなる。
「Visual C++ の勉強部屋」(目次)へ
ニューロンによるパターン認識の確認
