「機械学習はじめよう」というタイトルの連載なのですが,実は今まで機械学習そのものの話がほとんどありませんでした……。今回からようやく機械学習がはじまります。
連載の第1回では,機械学習とは「解決したい問題」を数値化する「モデル」と,モデルのパラメータをデータから決める「学習」からなることを紹介しました。しかし,これだけ聞いて「なるほど,わかった」という人はまずいないでしょう。やはりもう少し具体的な説明が欲しいところですね。
そこで今回は,数ある機械学習の中でもっとも歴史のある手法を紹介します。他の新しい手法に比べてもずっとシンプルですが,そこにはちゃんと機械学習のエッセンスが詰まっています。そこから機械学習というものをより具体的に理解できるはずです。
2つの変数の関係を見つけよう
まずは例題として,2つの変数間の関係を調べてみましょう。「2つの変数」には,「気温と湿度」のようにいかにも関係ありそうな組み合わせだけではなく,「株価と犯罪発生率」のように関係があるかわからないような組み合わせもアリです(意味があるかどうかはまた別の話です)。
ここでは,どんな問題にも当てはめられるよう,2つの変数をxとyという2つの文字で表しましょう。そして,サンプルデータとして,以下の組み合わせを用意してみました。
「ん〜,だいたいこんな感じ?」でいいのであれば,このグラフを見せれば事足りるかもしれません。しかし,この直線の式 y=ax+b が欲しいという話になった場合にはどうしましょう。「グラフから判断すると,bはきっと5くらいだな」では多分許してくれません。
最小二乗法
こういうときに使える有名な手法が「最小二乗法」です。聞いたことある,使ったことあるという人もきっと多いでしょう。まずは,復習を兼ねて「最小二乗法」を簡単に試してみましょう。
求めたい直線の式を y=ax+b とします。この直線はサンプルデータの最初の点 (x,y)=(4,7) の近くを通るはずです。つまり,x=4 のときの値 y=4a+b と y=7 との差 4a+b-7 はできるだけ小さくしたいです。同様にその次の点 (x,y)=(8,10) での差 8a+b-10 も小さくしたいですし,その次も,次も……。どれか1つなら簡単ですが,全体的に小さくしたい。それでは,差の合計が小さく(0に近く)なるようにしたらどうでしょう。
すぐわかりますが,これではダメです。なぜなら,それぞれの差が負の値になることもあるため,大きい正の値と大きい負の値がキャンセルしあってしまいます。そこで,それぞれの差の二乗をとってから足しあわせることにしましょう。これを「二乗誤差」と言います。
第8回 線形回帰［前編］:機械学習 はじめよう｜gihyo.jp … 技術評論社
