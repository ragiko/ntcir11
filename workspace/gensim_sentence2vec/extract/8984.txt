 ゲーム技術において重要視されているのはいうまでもなくグラフィックスだろう。グラフィックス処理には潤沢なCPU性能やメモリ,そして強力な演算性能を持つGPUといったリソースが割り当てられている。それだけ人間にとって目から入ってくる情報が重要ということだが,その次あたりに重要なのが耳から入る情報,つまり音だと筆者は思っている。
というわけで本稿では,CEDEC 2014初日に行われた「当たって砕けろッ!プロシージャルオーディオ制作」と題されたセッションの内容を紹介してみたい。
CEDEC 2014 記事一覧
物理エンジンとサウンドエンジンをつなげるプロシージャルオーディオ
ゲームには昔から効果音が利用されてきた。ゲーム内で起こり得るイベントに合った効果音を録音ないし合成したサウンドデータとして用意しておき,ゲームプログラム中で適切なサウンドを選んで再生する,というのが古典的な効果音の手法だ。シチュエーションに応じてエフェクトを適用するなどすれば,作り置きしたサウンドでも効果音に変化を付けることができる。
シディーク・サジャード氏(スクウェア・エニックス テクノロジー推進部 音声研究者)
だが,最近のゲームでは,この古典的な手法に限界が生じるようになってきた。たとえば,ゲーム内で起こり得るイベントが膨大な量になって,イベントに合った音を作り置きするコストがかかり過ぎたり,サウンドデータの容量が膨れ上がったりといった問題が生じるのだ。
さらに物理エンジンがゲームで利用されるようになると,物理シミュレーションによって描き出されるシーンの展開を予測することができないため,サウンドを作り置きするという手法そのものが通用しなくなってしまう。
そこで,作り置きした音を再生するのではなく,音をプログラムで作り出す「プロシージャルオーディオ」(Procedual Audio)の技術を使って,物理シミュレーションの結果に合わせて音を生成してはどうか,というのが,今回のセッションのテーマとなる。
セッションを担当したのは,スクウェア・エニックスでサウンドの製作に携わっている音声研究者 シディーク・サジャード氏,サウンドプログラマー 谷山 輝氏,そしてサウンドデザイナー 廣瀬裕貴氏の3人だ。
左から谷山 輝氏(スクウェア・エニックス サウンド部サウンドプログラマー),廣瀬裕貴氏(スクウェア・エニックス サウンド部 サウンドデザイナー)
セッションではまず,サウンドデザイナーの廣瀬氏が,なぜプロシージャルオーディオが必要なのかという点について,デモを交えて解説した。氏いわく,「グラフィックスの表現は物理エンジンの恩恵を受けて多彩な表現が可能になった。だがサウンドは必ずしもそれに合わせていなかった」。そこでスクウェア・エニックスでは,物理エンジンに対して厳密にサウンドを合わせるということを考えたのだそうだ。
廣瀬氏が披露したデモの1つ。上からブロックが落ちてきて壊れるという,物理エンジンを使ったデモだが,音がうまく合わない
［CEDEC 2014］物理シミュレーションの結果に合わせて音を生成する方法とは。スクエニの開発者が語るプロシージャルオーディオの応用例 - 4Gamer.net
