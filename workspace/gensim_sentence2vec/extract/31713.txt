10:20-12:00 音声ドキュメント処理Automatic Speech Recognition and Machine Translation System for MIT English Lectures using MIT and TED CorpusVeri Ferdiansyah, Seiichi Nakagawa(Toyohashi Univ. of Tech.)MITコースウェアに字幕を付けるため,音声の自動書き起こしと翻訳を行う.MIT OpenCourseWareの音声で適応(30人,5時間).適応手法,言語モデルの学習データ(WSJ, MIT OpenCourseWare)を比較.翻訳モデルでは,日本語側でCSJ, TED Talks, JENAADを組み合わせて利用.認識ではWSJに対してMAP推定でMITコーパスを適応させたものがよかった.翻訳の学習データとしては,TEDだけ使った場合が最良.WERが32%ぐらい,BLEUで7.16ぐらい(重要文のみ).チャレンジングなタスクだが,結果はまだまだな感じ.音声認識結果の有用性の自動判定に基づく講義のリアルタイム字幕付与システム桑原暢弘, 秋田 祐哉, 河原 達也(京大)音声認識人手修正の字幕作成を効率化するため,認識結果に人手修正が必要なのかどうかを識別して,その結果を修正者に提示する.認識結果の正誤よりも「字幕として修正が必要かどうか」を基準にするところが特徴.文節単位で「有効」「無効(フィラーなど)」「要チェック」の3クラス分類.認識は単語単位なので,CSJから学習したSVMで単語系列を文節にまとめる.まとめたものが文法的な文節構造になっていないものは要チェック.それ以外のものはCRFで3クラス分類.CRFの素性としては,文節そのものと読み,品詞,信頼度を使った場合が最高性能で約80%.「要チェック」の再現率は6割.「無効」の判定はうまくいかない(データが少ないため).また,字幕作成方法(認識修正提示,認識提示修正を上書き,それに自動分類を入れたもの)を比較.聴覚障碍者によるプリファレンスは従来法と同程度だが,字幕作成者の負荷が下がるので有効という主張.聴覚障碍者へのリアル講義での字幕提示にプロンプターを使っているのが面白い.Normalized web distanceを用いた音声認識誤り訂正法エンフボロル ビャムバヒシグ, 田中克幸, 滝口哲也, 有木康雄(神戸大)音声認識結果からConfusion Networkを作って,各位置の1位の単語についてCRFで正誤判定.誤っていると判定されたら2位を取ってくる.CRFの素性として従来法ではLSAのスコアなどを使っていたが,ここではNormalized Web Score(検索エンジンのヒット数に基づく単語共起の強さ)を使う.また,CNのヌル遷移をそのまま使った場合と使わなかった場合の2つのCRFを併用する.LSAを利用してCRFを1つしか使わないものと比べて3.45ポイント改善.第三話者読み上げコーパスによる音声ドキュメントの活用小泉悠馬, 塩出萌子, 伊藤克亘(法政大)ひさしぶりに伊藤先生を生で見たが,あいかわらずで安心した.アニメで声優が変わった時に,新声優の声を旧声優の声に近づける話者変換.演技はそれぞれ妥当だと思われるので声質だけ変換する.パラレルコーパスがとりにくいので,第3話者を経由して2段階で変換を行う.中間の話者は,できるだけ対象話者・目標話者のまねをして話す.主観評価による結果はあまりよくない.対象がドラえもんなので,「だみ声」などがうまく変換できていないのではないかと分析.13:00-18:00 「中川聖一先生退職記念講演・討論会」「音声処理における人間とコンピュータの能力の違い」中川聖一(豊橋技術科学大学)・人間対コンピュータでコンピュータが勝ってきた(そしてまだ人間が勝っている)歴史 チェス,クイズ,将棋 vs. 囲碁,入試,翻訳,対話 それぞれの探索空間の大きさ・音声に含まれる情報量(1発話3秒あたり) 位置:5bit 感情:3bit 話者:7bit 言語:42～bit・音源分離能力 PASCAL CHiME challange 最高性能のシステムは人間より1～4%程度の劣化・音源同定(発声方向,距離) 2つのマイクロホンアレイを使うと人間の方向同定性能を超える 距離の同定性能は機械が人間を超える・感情認識 人間が8種類の識別60% vs. 機械が4種類の識別70%・話者認識 人間<機械,人間の結果統合≒同じ内容の機械識別,人間の結果統合<違い内容の機械識別 位相情報の利用,MFCCと位相の併用・音声認識 機械は人間の能力に全く及ばない(特に雑音・残響環境下).ロバスト性が違う 前後のコンテキスト付与による単音節単語の聴取率:音響環境で20ポイント程度,前後の単語でさらに10～20% 音韻認識率・パープレキシティと単語の認識率の関係 相互情報量と言語重み最後に中川先生の4月からの身の振り方を紹介して終了.伊藤克亘先生から「ここで言っている人間のレベルはどの程度のものなのか?」という話が.「音声・画像処理の共通点と統合・変換処理について」有木康雄(神戸大学)・音声と画像の認識対象 どちらも似ている(Who,How,Where,When,What)が,Why, Contextなどについての研究(意図理解,状況理解)は少ない・生成 音声の生成モデル,ケプストラム分析          室伝達関数 画像の生成モデル(入射光束と反射光束),画像信号のケプストラム          撮像モデル,点広がり関数(PSF)・特徴量 画像にしかないSIFT特徴量(回転・拡大縮小・輝度変化に不変)・検出 AdaBoostによる顔検出,HOG特徴量・追跡 パーティクルフィルタ・認識 HMM,GMM,SVM, Convolutional NN
ImageNetの認識コンテストILSVRC
BoF 特定物体認識(検出),一般物体認識・変換 声質変換 GMMによるHOGの視覚化(HOGから元画像を推定する)・合成 AAMによる顔合成・認識(顔の3次元形状の主成分分析・フィッティング)・音声画像処理の相互流用 GMM話者変換GMM超解像へ AdaBoostによる顔検出要求発話検出 言語文法映像文法・今後 機能的パターン認識 シンボルグラウンディング 状況の理解,意図理解"Performance Comparision of the Front-End Noise Reduction Methods with Parallel Model Combination Approach"Hyun-Yeol Chung(嶺南大学)鄭先生ひさしぶり.最初に韓国の音声認識の現状.・韓国政府のICT戦略:Smart Software, IoT, BigData/Cloud, UI/UX for Mobile・Google, Nuanceなどの台頭により韓国での研究は急速に縮小・Google音声検索,Siriの成功により研究動向に大きな影響 言語モデル,対話管理,NLPなどの重要性,ビッグデータ,自発音声認識・研究機関:メーカー(Samsung, LG, Pantech)ポータル(Naver, Daum, Google) 通信(SK Telecom) 研究機関(ETRI, KIST) 大学(KAIST, SNU, Postech)など.産学連携も・モバイルのための連続音声認識に注力・研究課題:データ収集,言語モデル,対話処理 データ収集:一般に利用可能なコーパスがない(商用のみ,会社は自分で集めている) 言語モデル:データベースがないので推定ができない,Large-scale distributed LM 対話管理・企業の研究の現状 サムスン:自前でエンジンを開発(S Voice) Siriのようなアシスタント サムスンのスマホに入っている      (2011年にVlingoから買ったもの) LG:長い歴史,自前で開発 (Q Voice)   韓国語エンジンWernicke Naver:認識エンジン Link 13言語をサポート Daum:ベンチャー会社Dialoidを買収,音声アプリ開発 ETRI:多言語自動翻訳 GenieTalk・Siri(Apple), S Voice(Samsung), Q Voice(LG), Smart Voice(Pantech) の比較ビデオ(韓国のTV番組) 4つのスマホに一斉に「おなかすいた」と呼びかけて反応を比較,など.Smart Voiceはできない子・自前の技術が必要,NLPと対話管理が必要,データを集めなければならない後半は鄭先生の研究紹介で,PCMによるフロントエンド雑音除去.・さまざまなフロントエンド・バックエンドの雑音対策を比較・試した中ではTwo-stage Mel-warped Wiener Filter (TMWF)がよかった・RASTAとARMA filteringの比較・PMCとの組み合わせ さまざまな雑音に対して,どの手法の組み合わせがよいか評価・フロントエンドでは ERN-CMVN-RASTA PMCの組み合わせではTMWF-PMCがよかった「音声情報処理の近未来像ー音声・音響・言語処理の最前線からみる音声言語処理の今後の展開ー」司会: 中川聖一パネラー: 伊藤彰則(東北大), 篠田浩一(東工大), 西村雅史(IBM東京基礎研究所), 峯松信明(東大)出演したのでメモがとれず.
3月14日 音声ドキュメント処理ワークショップ&中川聖一先生退職記念講演・討論会 | aitoの日記 | スラッシュドット・ジャパン
