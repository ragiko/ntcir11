ビッグデータのプラットフォームを握る事業者は、こういうことができてしまうんだなというのを、改めて教えられるニュースだった。
6月17日付けの科学専門誌「米科学アカデミー紀要(PNAS)」に掲載されたこの論文だ。
「ソーシャルネットワークを通じた大規模情動(感情)伝染の実験的証明」
フェイスブックが、ニュースフィードのアルゴリズムをいじって、ユーザーの感情をコントロールできることを証明した、と言っている論文だ。しかも、利用規約上は全く問題ない、と。
当のフェイスブックのデータサイエンティスト、アダム・D・I・クレイマーさんがそう書いている。
ジャーナリストブロガーの草分けでアリゾナ州立大学ジャーナリズムスクール教授、私の友人でもあるダン・ギルモアさんは、ツイッターでこうコメントしている。
フェイスブックの感情コントロール実験をした研究者は、自分たちのことを恥じるべきだ。
The researchers in the Facebook emotion-manipulation experiment should be ashamed of themselves, too: http://t.co/L2XJ9WlSbE
&mdash; Dan Gillmor (@dangillmor) June 29, 2014
界隈は、かなり騒然としている。
感情をコントロールする実験
感情は伝染する——楽しそうな人と接していると、その感情が伝染して自分も楽しい気持ちになり、悲しそうなと人と接していると自分も悲しい気持ちになる、という感情の共有現象は、「情動伝染(感染)」(emotional contagion)と呼ばれる。
従来の研究では、リアルで接する社会ネットワークの中で、表情などの非言語シグナルも影響して感情は伝染していく、とされてきた。
それに対して今回の論文では、直接顔を合わせるのではないネット上のテキストベースの感情表現でも、情動伝染が証明できるのか、という実験を行っている。
論文の著者は、クレイマーさんとコーネル大学教授のジェフ・ハンコックさんと、カリフォルニア大学サンフランシスコ校の研究員、ジェイミー・ギロリーさんだ。
実験対象は、12億3000万人の月間アクティブユーザーを抱える世界最大規模のソーシャルメディア「フェイスブック」だ。
実験が行われたのは2012年1月11日から18日までの1週間で、被験者(実験の対象者)は、フェイスブックを英語で使っているユーザーの中から選ばれた689,003人。
言語の頻出度調査などで用いられるソフト「LIWC」を使って、フェイスブックの投稿に含まれる感情表現を「ポジティブ」と「ネガティブ」に分類。
そして、フェイスブックのニュースフィードに投稿を表示させるアルゴリズムを変更。「ポジティブ」と「ネガティブ」それぞれについて、友人たちからの投稿の表示数を、10%から90%の範囲で減らすことで、ユーザー本人の投稿の感情表現にどんな影響が出るかを見た(被験者のニュースフィードに表示されなくなった投稿も、投稿した本人のウォールに行けば見ることはできたようだ)。
分析した投稿数は300万件、1億2200万語以上でうち400万語が「ポジティブ」(3.6%)、180万語が「ネガティブ」(1.6%)だったという。
フェイスブックでの感情の伝染
実験結果を見ると、「ポジティブ」な投稿の表示を減らした場合、被験者自身の投稿では「ネガティブ」な表現は0.1%減だったのに対し、「ポジティブ」な表現は0.04%増。逆に「ネガティブ」な投票の表示を減らした場合は、「ネガティブ」0.07%減、「ポジティブ」0.06%増になったという。
そして、こう述べている。
これらの結果は、オンラインのソーシャルネットワークを通じた友人による感情表現は、私たち自身の感情に影響を及ぼすことを示している。
この実験により、顔の表情などの非言語シグナルがなくても、ネット上のテキストだけで感情は伝染することがわかった、としている。さらに、ユーザーが友人からのニュースをコメントつきで共有する場合、それがいいニュースか悪いニュースかという内容の良し悪しよりも、友人たちがそれに対してどんなコメントをしていたかの方が、感情の伝染につよい影響を及ぼす、としている。
さらに、最後にこうまとめている。
これらのデータは、感情がネットを通して広がり得るという、論議を呼ぶ主張を支える、我々の知る限り初めての実験的証拠となっている。だが、この実験の効果量(effect sizes)は小さい(d=0.001)。(中略)とは言え、フェイスブックの規模における効果サイズ0.001は無視できない: 2013年初めの時点で、これは1日あたり、数十万件の投稿における感情表現に匹敵するのだ。
知らぬ間に感情が変わる
実験結果としては、面白い内容だ。
面白くないのは、70万人近いユーザーが、自分の知らない間に勝手に被験者にされ、フェイスブックによって感情をコントロールされていた、という点だ。
フェイスブックにしろ、グーグルにしろ、ネットユーザーが目にする情報が、決して知らされることのないアルゴリズムによって決められ、その仕組みに依存せざるを得ない危険性は、これまでの繰り返し指摘されてきた。
ただ、今回はそれが学術論文という形で、あからさまに公言されたのだ。
論文の中では、今回のデータの扱いについて、こう述べている。
これはフェイスブックの〝データの使用に関するポリシー〟に則したものだ。このポリシーは、全ユーザーがフェイスブックでアカウントをつくるに先だって同意しており、本研究に対するインフォームドコンセント(事前同意)を構成している。
確かに「データの使用に関するポリシー」には「Facebookが受け取る情報の用途」として、この項目がある。
トラブルシューティング、データ分析、テスト、調査、サービスの向上等の内部運用。
「調査」は英文では「research」。つまり研究目的のデータ利用に、ユーザーは同意している、という主張だ。
フォーブスのライター、カシミール・ヒルさんの取材に対し、フェイスブック広報はこう回答したという。
この研究は2012年に1週間だけ実施されたもので、使用されたいかなるデータも、特定個人のフェイスブックアカウントとは関連づけられていない。我々はサービスの改善と、人々がフェイスブック上で目にするコンテンツをできる限り適切で興味を引くものにするために、研究を行っている。特に重要なのが、異なるタイプのコンテンツへの、人々の反応の仕方を理解することだ。書きぶりがポジティブかネガティブか、友人からのニュースか、フォローしているページからの情報か。我々は自分たちが何の研究をするのか、慎重に検討しているし、厳格な内部審査手続きもある。これらの調査研究では、不必要な個人データの収集は行っていないし、すべてのデータは安全に保管されている。
学内審査委員会の対応
では共同執筆者に名を連ねている2人の大学(コーネル大学とカリフォルニア大学サンフランシスコ校)で、学内の倫理審査委員会(IRB)は、この実験に対してどのような対応を取っているのか。
前述のフォーブスの記事によるとPNASの担当編集者で、プリンストン大学教授のスーザン・フィスケさんは、@Zleeilyさんという大学院生からの問い合わせに対し、こう回答しているようだ。
倫理的な問題については私も気になったが、著者たちは、フェイスブックがユーザーのニュースフィードにフィルターをかけることについて、IRBはユーザーの同意を条件に実験を認めた、と述べていた。IRBの委員長を10年にわたって務め、被験者の研究倫理についての著作もある経験から、私はIRBの扱いについて疑問を差し挟む必要はないと判断した。
ただ、ネット上での騒ぎを受けて、後悔も頭をもたげているという。アトランティックのエディター、エイドリエン・ラフランスさんの取材には、こう答えている。
規制の観点からは倫理上はOKなのだが、倫理とはある種、社会的決定の側面もある。絶対的な正解はない。だからこそ、広がり始めている怒りのレベルから考えると、この実験は行われるべきではなかったと言うことかも知れない...私はなおこれについて考えていて、少々後味の悪い思いをしている。
オンラインマガジン「スレート」のケイティ・ウォルドマンさんは、論文が言う「インフォームドコンセント」は、そもそもその条件を満たしているのか、と疑問を投げかける。
これは、大半の社会科学者が定義するインフォームドコンセントとは別物だ。
A/Bテストとの違い
公共放送「NPR」のブライアン・ボイヤーさんのツイッターでの反応は明解だ。
ファック・フェイスブック。
フェイスブックがユーザー689,003人の感情をコントロールする(更新あり) | 新聞紙学的
