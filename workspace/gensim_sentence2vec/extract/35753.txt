[論文メモ]点予測による形態素解析
NL198 2010中田陽介、NEUBIG Graham、森信介、河原達也SVMベースの形態素解析単語分割と品詞推定を別でやるところに特徴がある。従来手法では、分を形態素列とみなし、形態素解析を単語分割と品詞推定を同時に行う系列ラベリングの問題として扱っている。大きく、2つの系統に分けられ、ヒューリスティックスに基づく手法と、コーパスから規則などを学習する手法がある。ヒューリスティックスに基づく手法はJUMANがある。品詞間での連接コストや単語の生起コストを人手で与え、微妙なコスト調整が必要であるため、人的コストが高い。コーパスに基づく手法は、連接コストや単語の生起コストをHMMのパラメータとみなす。他にはCRFがある。※最近調べた論文ではME (Maximum Entropy) なんかもそうだろう。どちらにせよ、分野適用性が低く、標準文書(新聞)以外の文書については、解析精度が著しく低下することが知られている。提案手法は点予測。点予測による単語分割単語分割の考え方は、「現在注目している文字境界が分割ポイントになるか否か」というものである。素性は、文字N-gram:窓幅"m"と長さ"n"を与えたとき、まず境界位置"i"の前後"i-m"から"i+m"までの部分文字列をとり、"n"以下の長さの全ての部分文字列をとる。文字種N-gram:文字を文字種に変換した列であり、基本的な考え方は文字N-gramと一緒。文字種は、漢字(K)、カタカナ(k)、ひらがな(H)、ローマ字(R)、数字(N)、その他(O)の6つ。単語辞書素性:位置iを始点とする単語、終点とする単語、内包する単語があるか否かのフラグと、その単語の長さ。点予測による品詞推定単語分割後に推定開始。従来手法は、単語列を入力とする。提案手法は、推定対象の単語と前後の文字列。つまり、周囲の単語や品詞の推定結果を一切利用しない。品詞の推定方法は4種類1. 学習コーパスに品詞候補が複数出現する単語は、分類器で多値分類2. 学習コーパスに品詞候補が1つの場合は、その品詞3. 学習コーパスに品詞候補がないが辞書にはある場合、辞書の最初の品詞4. 学習コーパスにも辞書にもない場合、名詞素性は、文字N-gram:単語境界のときとほぼ一緒文字種N-gram:単語境界のときとほぼ一緒単語ごとに多値分類one v.s. rest評価線形SVM、"n=3"、"m=3"精度はMecab-0.98とほぼ同程度なかなか興味深い。品詞推定方法に疑問はあるが、それでも精度はかなりよい。学習コーパスがかなり充実しているのではないか?分野適用性の柔軟さがあるという主張だが、そこまでではない印象。
ツイートする
Permalink | コメント(0) | トラックバック(0) | 23:07
[論文メモ]A survey of named entity recognition and classification
David Nadeau, Satoshi Sekineサーベイ論文。分野を知らない人にとっては、サーベイ論文ほど助かる論文はない。もっと定期的にサーベイ論文が出るといいですね。1991年から2006年をまとめた論文。1991〜1995年まではヒューリスティックス中心。1996年にワークショップ(的なイベント)開催。MUC-06。それから、HUB-4、MET-2、IREX(知ってる!)、CONLL(知ってる!)、HAREM、LREC(知ってる!)と開催。タスクの説明は省略2006年以降のトレンドは教師あり、半教師あり学習。教師あり学習は、Hidden Markov Models (HMM) から始まり、Decision TreesやMaximum Entropy Models、Support Vector Machines、そしてConditional Random Fieldsとすすむ。※現在はConditional Random FieldsがMajorな手法半教師あり学習が流行ったのは、ひとえにアノテーション作業の手間ゆえ。Bootstrappingでアノテーションデータを増やすことが大きな目的。教師なし学習は、クラスタリング。これは、「同じ品詞を持つ単語は周辺の単語が似てるでしょ」って仮定のもとになりたっている。学習素性(NEの場合)単語レベルの特徴系先頭文字がCapitalizationかどうかすべてUppercaseかどうかUpeercaseとLowercaseのMixかどうかピリオドでおわるかピリオドが間に入るかアポストロフィが入るかハイフンかアンパーサンドがあるか数字のパターン(日付形式、などなど)Cardinal NumberOrdinal NumberRoman Number数字が含まれる単語かどうかpossessive mark(所有形ってことかな)一人称代名詞Greek lettersprefixsuffix単数形stemCommon ending (ishとかistとか)proper nameverbnounforeign wordAlpha(フィルタ、アルファベットだけ通過)non-alpha(フィルタ、アルファベット以外通過)n-gramlowercaseuppercase versionpattern(文字種に変換、大文字or小文字or数字or記号orなどなど)summarized pattern(文字種に変換、ただし同じ文字種はまとめる)文字長フレーズ長辞書参照系辞書 (gazetteer?とかlexiconとかontologyとか)ストップワードCapitalized noun (Januaryとか)Common abbreviationEntity(Organizationとか)Entityの特徴 organizationでよく用いられる語 person title, name prefix, post-nominal letters locationでよく用いられる語, cardinal point文書やコーパスの特徴系テキスト中の別の単語(正規化みたいなもん)UppercasedとLowercasedの共起AnaphoraCoreferrenceとaliasEnumerationAppositionセンテンスでの場所、段落での場所、文書での場所URIEmail headerXML section箇条書きかどうか番号付きリストかどうか表図単語またはフレーズの頻度共起複合語(選別する)あとは精度についてチラホラ。かなり訳が大雑把になってしまった。素性について詳細に書いてあるのは助かる。それも大雑把に書いたから、実際に論文を閲覧してチェックしなおす必要がある。
ツイートする
Permalink | コメント(0) | トラックバック(0) | 00:01
[論文メモ]Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
ACL2011Kevin Gimpel, Nathan Schneider, Brendan O'Connor, Dispanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, Noah A. Smith共著者多すぎ。Twitterを対象としたPOS Tagging。精度は90%。データセット公開してるらしい。http://www.ark.cs.cmu.edu/TweetNLPPOS大系はPenn Treebank。追加でURLやHashtag、@ユーザ名、感情(たぶん顔文字)、非公式RT英語圏のツイート限定でデータを集め、Twitter TokenizerとStanford Taggerとヒューリスティックスによりある程度自動でタグをつけておく。それから17人の研究者でタグ付け開始。2人ペアでつけたタグを比較し、agreement rate 92.2%、Cohen's k value 0.914。Tag SetHashTagは#tagnameで表されるのだが、いくつかの例で#がなくてもHashTagに分類できるものがある。調査の結果、148個の#なしHashTagが見つかった。なかにはMulti Wordも。※なかなかに定義の難しいことしてるな・・・あと、省略形。たとえば"ima"は"i'm gonna"。これはこれで新しくTagを設計。{nominal, proper nou} x {verb, possessive}の組み合わせで表される4つのタグ。タグは全部で25種類。24種類で抑え切れない、分類できないものはGというタグをつけた。識別はCRF。素性はword type、 数値かハイフンを含むかどうか、suffixの文字(3字まで)、Capitalization pattern。※たぶん、従来で使われる素性ももちろん入れてるとおもわれる。追加で、at-mention、hashtagなどTwitter specificな素性よくcapitalizedされる単語かどうか従来使われるタグ辞書音素的な正規化精度は89%前後。Stanford Taggerが85%前後。課題としてはproper noun。やはりCapitalizationは重要な素性で、Twitterでは文法的に正しい使われ方をしていないことがあり、うまく取れない。あとはmiscellanneous category (G) に分類したもの。※結果見る限りでは、proper noun + possessive (Z) も相当悪い。少し読み飛ばしたが、素性自体は比較的普通。でも、アノテーションがおもしろい。アノテーションつけるのはかなり難しいと思うけど、一致率高いな。もしかしたら一致したところはStanford TaggerがつけたPOSかも。失敗するようなところってのはStanford Taggerでも2割程度。人の手で修正して一致するのはその内の1割。残りの1割は人でも判断できない難しい問題なのかも。そこをうまく大系づけることが今後のミッションかな。言葉って明確に分類できなかったりして難しい。
ツイートする
Permalink | コメント(0) | トラックバック(0) | 11:33
[論文メモ]Named Entity Recognition in Tweets: An Experimental Study
ACL2011Alan Ritter, Sam Clark, Mausam and Oren EtzioniTwitterを対象としたNER。POS TaggingとChunking込み。Twitterを解析する上で大きく2つ課題がある。1点目 データスパースネス企業名や商品名、バンド、映画などなど、新語はたくさんでるけど、コーパス内に頻出するとは限らない。2点目 140字制限短い。故に色々欠落してる。背景知識とか。提案手法POS Tagging従来手法でもけっこー高い精度がでる。Brownコーパスで0.9くらい。同様の設定のTwitterデータで0.76。精度の下がる原因は、構文的な問題というよりOut Of Vocabularyが大きい。spellのvaliationによるところも多いが、NNPがOOVの1/3を占める(らしい)。Stanford TaggerだとTwitterデータで0.8くらい。Stanford TaggerはPenn Treebankで学習しているらしい。TwitterだとCapitalizationが強調のために用いられていたり、主語なしでつぶやかれていたり、などなど。提案手法では、Twitterデータに対してPenn TreeBankと同様の仕様のアノテーションをした。追加で、ユーザ名、ハッシュタグ、URL、RT。OOV対策として、Jclusterでprefixの4,8,12bitでクラスタリングした結果を使った。※おもしろそうPOS TaggerはCRF。上記の実装で26% error reductionできた。IRC chat dataにもアノテーションを施し、学習を補強。Penn TreeBankのデータも使うと、最終的に41% error reduction、精度は0.883。改善幅が大きいのがinterjectionとverb。Stanford Taggerではnounに分類されていた。※やっぱりアノテーション済みデータがあればよくなる。他に工夫しようはないのか?Chunkingやっぱりアノテーション。CoNLL shared taskの仕様に準拠。追加でCoNLLのデータも学習に追加※というかほとんどCoNLLデータになってしまった。実装はCRF。これで22% error reductionできた。CapitalizationNamed Entity Recognitionで強いキーになっていた要素。Twitterでは強調の意図で使われることも多いため、素性として微妙になってしまった。そこで、InfomativeなCapitalizationを見つける識別器を作った。文頭かどうかとか、他にCapitalizeされたEntityがあるかとかをチェック。素性にする。識別器はSVM。素性は、Capitarizeされた単語の一部、辞書でlowercase/capitalizedで登録されているが実際にlowercase/capitalizedじゃない単語の一部、Iが何回lowercaseで現れたか、文頭でかつcapitalizedされているかどうか。※他にもあるかもしれない。あと、訳が間違ってるかも。これらを総合的に組み合わせたCRFが最終的な成果物。Stanford NERがF値0.44、提案手法はF値0.67※もっと!もっと精度がほしい!Infrequent WordはそれでもNERに失敗する。そこで、FreebaseのOntologyを使い、未知語の前後の単語を集めて、品詞を推定する。LabeledLDAを使う。素性はBag of Words。アルゴリズムは省略。数学的なのでめんどう。Named Entity Clasificationの精度は、DL-CotrainがF値0.53、提案手法がF値0.66※最初のCRFで作ったNERとの関係は?作ったプログラムは下記で入手可能。http://github.com/aritter/twitter_nlpおもしろい部分もあった。いい論文。でも、やっぱりアノテーションをつけたCRFがいいのか。それじゃつまんないんだよな。
ツイートする
Permalink | コメント(0) | トラックバック(0) | 01:07
[論文メモ]形態論的制約を用いたオンライン未知語獲得
村脇有吾, 黒橋禎夫JUMANの未知語対応。JUMANは、いくつかの崩れた表現にも対応している。オンライン学習の考え方に近い形で実現。(オンラインって言葉には、いつも違和感を感じる)つまり、未知語を見つけたら逐次辞書を更新する、というもの。未知語の同定は、ヒューリスティクス。※カタカナ語に特化している気がする。未知語を同定し、未知語前後の文字列により品詞を同定する。※「nグラム統計によるコーパスからの未知語抽出」に近い逐次辞書を更新するため、かなりPrecision重視にチューニングする必要がある。終了条件(品詞同定)は2つ。前方境界の妥当性チェック。これは、前方に句読点などの境界マーカーが出現するかどうかをスコアリングし、設定した閾値以上が条件。活用型の異なり数チェック。これは、特定のパターンだけ頻出するわけではなく、品詞にあった活用型をある程度網羅しているかをチェック。単語によっては、例えば「楽し-い」と「楽し-む」のような品詞の衝突を起こす場合がある。しかし、その衝突が同一テキストで起こるということは稀だと考え、特に考慮しない。提案手法はテキストを逐次的に解析するため、同じ文字列で複数の品詞が逐次的に登録される。当然、獲得済みの形態素が獲得した形態素によって分割できる場合がある。チェックは、まず既存形態素辞書のうち部分文字列で新形態素を含むものを見つける。ある場合、当該形態素を辞書から削除し、新形態素を辞書登録した上で、当該形態素を形態素解析する。新形態素によって分割が起こらなければ、当該形態素も辞書に残す。※少し、違和感がある。ほぼ分割されうるのではないか?課題字種が混在する形態素は難しい。(「ドジっ娘」「シャ乱Q」など)登録済みの形態素の異表記もあった(「すごい」⇔「スゴい」、「解かる」⇔」「解る」など)過分割(「アブラハム」「アブラ」と「ハム」)ひらがな表記の未知語は曖昧性が高く、困難。これも中々に直感的。ただ、課題も多い。ひらがな表記の未知語が困難だ、という論文が続いたが、どれくらい出現するんだろう。
ツイートする
Permalink | コメント(0) | トラックバック(0) | 22:21
PineApple Inc.
