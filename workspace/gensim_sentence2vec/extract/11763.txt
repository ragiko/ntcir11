[SSL][機械学習][妄想]半教師あり学習とは結局のところ何なのか?
今週ゼミで半教師あり学習のイントロダクション的なものをやる予定なので資料を作っている。
Self Training
Generative Models
S3VMs
Graph-Based Algorithms
Multiview Algorithms
などなどの手法を簡単に紹介する予定(Blogにもあげるかもしれない)。で、資料を作っていて、「半教師あり学習とは、つまりラベルありデータだけではdata sparsenessに負けてしまうところをラベルなしデータによって補う方法ではないか」というところに行きついた気がする。
半教師あり学習の本質はどこにあるのか?
Semi-Supervised Learning Tutorialの資料からいくつか例で見てみよう。テキストの分類問題を解いているとして、BOWの表が以下のようになったとする。
このとき学習器を作ったとしても、overlapしている部分がないため(たぶん)うまく行かない。しかし、これにラベルなしデータがあると話は違ってくる。こんな具合だ。
(グラフベースの手法で言えば)ラベルを「伝播」させていけるわけで、overlapのところをdataがsparseであると読み変えればラベルなしデータでsparsenessに対抗しているとも読み取れる(自分はそう読み取った)。
もう一つTutorialから例を。以下は文字認識で使われるような画像。oneとかが付いているのがラベルありの部分である。
ラベルありのデータだけではデータ空間全体に対してすかすかであるが、似たような画像を近くに置くことによってもうすこしdenseな感じになっている。これもsparsenessな部分をラベルなしデータで補っていると考えられる。
というわけで、「ラベルありデータを集めるにはcostがかかる」とかそういうのはもうどうでもよくって(よくないが)、半教師あり学習の本質は「ラベルなしデータを使って、dataがsparsenessな部分を補ってあげることで学習の精度をよくする手法である」と言えるのではないかと思う、いや言いたい、いやきっと言える(ぉ。
現在の半教師あり学習の悪い(?)ところ
半教師あり学習の論文の実験では、ラベルありデータの数が少ないときは(ラベルありデータのみから構築した)SVMなどに比べ、性能がよいことが多い。しかし、ラベルありデータの数を増やしていくと半教師あり学習は(ラベルなしデータのせいで)性能が悪化することがある。ここに載っている結果などはその一例だ。
データサイズが大きくなっても動かせる、という意味で半教師あり学習がscaleするように工夫した手法はいくつかある。しかし、ラベルありデータを増やしていっても半教師あり学習の性能が落ちない(悪化させない)という意味でのscaleする(って言っていいのか?)研究はあんまりない気がする。
半教師あり学習の本質が上に書いたように「ラベルなしデータを使って、dataがsparsenessな部分を補ってあげることで学習の精度をよくする手法である」であるとすると、ラベルありデータが増えてくるというのはdataのsparsenessが徐々に解消されていくということだ。ある程度denseになったにも関わらずラベルなしデータがいすわっていては精度が落ちてしまうのもある意味仕方がない。だからと言ってラベルありデータが多い場合にラベルなしデータが役に立たないかと言われるとそうではないと思う。言語モデルとかはdata sparsenessとの戦いのような感じでもあると思う(自分は)ので、きっと役に立つ。Self TrainingやCo-trainingのようなもので考えると、「dataがsparseness」かつ「predictionがconfident」であるようなものを追加していくようにすればラベルありデータが増えてもうまくいくようになるんじゃないかと思う。
その他
「dataがsparseness」かっていうのを調べるためには密度推定のようなことをやらないといけないが、分類問題を解きたいっていうのに、その中で密度推定をやるっていうのはかえって問題が複雑になっている気がする。
あと、ラベルありデータが十分にあるんだったらもうラベルなしデータのこととか面倒だから忘れちゃおうぜ、っていうのもあながち間違ってはいないような気がする(欲しい十分な精度が出てるんだったら)。
ツイートする
Permalink | コメント(0) | トラックバック(0) | 07:03   
半教師あり学習とは結局のところ何なのか? - Seeking for my unique color.
