ここで音声認識の歴史をちょっと振り返ってみよう。
人間の音声を人工的に合成しようという試みは18世紀にまで遡るが、人間の話す音声を機械が認識できるようになるまでにはコンピュータの登場を待たなければならなかった。
1950年代の米国では、人の発話の様子をX線撮影し、声道(口から声帯まで続く、声を出すための空洞)の構造を調べる研究が盛んに行われていた。人間の声道は複雑な形状をした管がいくつも連なってできている。これらが音を発する時にどう変化するのかを確かめ、それを数学的に記述する。このモデルに従って音を合成すれば音声合成になり、入力した音声がどのモデル(どのような声道の形)に近いかを調べれば音声認識になると考えたのだ。
このような研究に取り組んだのは、米国ベル研究所の音響物理学者達で、1970年代には人間の音声をすっきりとした数式モデルで表せるようになった。「あ」という音は、何Hzと何Hzの音が強いという特徴があるとか、「い」は何Hzと何Hzとかいう具合だ。
こうした考え方に従えば、人間の音声はとても簡潔に記述できる。十数個の数式を組み合わせることで、人間の音声に近い音を合成できるようになったのだ。数式モデルで音声を表現することを音声符号化という。
音声符号化ができたのだから、あとは入力した音声をこのモデルに当てはめて近似パターンを検出すれば、音声認識の出来上がり……と研究者の多くは考えた。
人間の音声はサイコロで決まるのか?
確かに、数式モデルで音声認識の技術が進歩したのは確かだ。
1960年代にはIBMが「Shoebox」という音声認識機器を発表し、日本でも京都大学が単音節の音声を認識する「音声タイプライター」を開発している。
[写真]  世界初の数式モデルの音声認識IBM Shoebox
さらに、1970年代には日本とロシアで、DPマッチング法という音の伸縮特性を考慮した方式が開発され、単語を連続で認識することもできるようになった。
数式モデルをより精密に突き詰めていけば、音声認識は完成するはずと思われた。だが、こうしたモデルを元に作られた機器は、静かな場所で話者がクリアに話すという条件が必要だった。非常に限られた条件下でなければ適切な認識ができないというのでは、実用的な製品にならない。
人間の話し方は、とてもいい加減である。日常会話において、アナウンサーのように一音一音を明瞭に発音している人などいない。数式モデルで理想的な「あ」を表現できても、実際に発音される「あ」は大きく崩れることも多い。前後の音に応じて、口の形は滑らかに変化していくが、数式モデルではこうした「調音結合」をうまく表現することができなかったのだ。
ちなみに、音声符号化の技術は通信分野に応用され、大きな成果を上げている。代表的な音声符号化であるCELPでは、人の音声を数式モデルにできる限り当てはめて表現する。その結果、音声波形をそのままデータ化するよりはるかに少ないデータ量で、元の音声を再現することが可能になった。通信速度の遅かった携帯電話でも、きちんと音声通話ができるのは数式モデルを使った音声符号化によるところが大きい。
音声認識技術はどこに向かうのか? (2/5) | Telescope Magazine
