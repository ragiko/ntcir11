Representation Learning: A Review and NewPerspectivesY. Bengio, A. Courville and P, Vincent, 20122012/12/14(金) Deep Learning 勉強会東京大学大学院 工学系研究科 技術経営戦略学専攻大澤 昇平
ADGENDA   窶「 7. Direct Encoding: Learning a Parametric Map from    Input to Representation   窶「 8. Representation Learning as Manifold Learning   窶「 9. Connections between Probabilistic and Direct    Encoding Models   窶「 10. Global Training of Deep Models   窶「 11. Building-in Invariance   窶「 12. Conclusion                                                          1
ADGENDA   窶「 7. Direct Encoding: Learning a Parametric Map from    Input to Representation   窶「 8. Representation Learning as Manifold Learning   窶「 9. Connections between Probabilistic and Direct    Encoding Models   窶「 10. Global Training of Deep Models   窶「 11. Building-in Invariance   窶「 12. Conclusion                                                          2
7. Direct Encoding: Learning a Parametric Map from Input to RepresentationNon-probabilistic feature learning paradigm     Section 6 では、学習された表現は、ある潜在変数の事後確率に基づくものであった(probabilistic feature learning)          潜在変数の事後確率は、それ自体では特徴ベクトルにはならない                 特徴ベクトルベースの分類器(e.g. SVM)に入れられない          実際の特徴ベクトルは分布から導出される(期待値、marginal probability、最尤値)          このような事後確率は、モデルが相互に接続された層を持つ場合、しばしば複雑になりうる          そのため、サンプリングや近似推論の技術が用いられる     最終的に得られるアウトプットが特徴ベクトルなのであれば、事後確率を導出する手順を省略することで、より効率的な計算が可能     なのではないか           Non-probabilistic feature learning (e.g. auto-encoder)    Probabilistic Feature Learning (e.g. sparse coding)        Non-probabilistic Feature Learning (e.g. auto-encoder)    Observed                                                   Observed    Latent                                                     Latent                                              0.257                            0.257                                           Feature Vector                    Feature Vector             Prior (complicated)                                                                                                                3
7. Direct Encoding: Learning a Parametric Map from Input to RepresentationAuto-encoder    Auto-encoder framework [Lecaum 1987][Bourland 1988][Hinton 1994] : unsupervised feature    construction method の一つ。       auto-: 「自己の」 auto-encoder を直訳すると自己符号器       encoder, decoder, reconstruction error の 3 つの要素から構成。       encoder と decoder の合成写像が入力値を再現するような学習を行う。       学習は入力値と出力値の誤差(reconstruction error)を最小化することで行われる。       この操作によって、入力値をより適切な表現に写像する auto-encoder が得られる。                                                                                                                                                                                                                                                                        = argmin  DAE ( ) = argmin                   ,                                                                                                              Reconstruction                                                         =1    t-th Input                                                                                             Output    Vector             Encoder               Representation Vector      Decoder                            Vector          ( )                                            ( )                                                    ( )                                              Reconstruction Error                                                                                                                                                                                  4
7. Direct Encoding: Learning a Parametric Map from Input to RepresentationAn instance of auto-encoder: identical function    恒等写像は最も trivial な auto-encoder                Input                              Representation                 Output                               Encoder                                Decoder                    (1)                                     (1)                       (1)                    (2)                                     (2)                       (2)                                     id                                      id                    ( )                                     ( )                       ( )                                               Reconstruction Error                                                                                                                    =0            Representation が入力ベクトルと同じなため、入力ベクトルの適切な表現を得るという目的は果たせていない                                                                                            5
7. Direct Encoding: Learning a Parametric Map from Input to RepresentationAn instance of auto-encoder: affine mapping    エンコーダと デコーダをアフィン写像(  ,   , * ,  '+) + 活性関数(    ,     )によって構築    入力空間が非有界の場合           =     = id,    ,   =       2 主成分分析と等価    入力空間が [0,1] の場合           =     = sigmoid,    ,   =       2    入力空間が {0,1} の場合           =     = sigmoid,    ,   =       log    + 1      log(1      ) (交差エントロピー)                Input                                                                  Output                                   Encoder         Representation            Decoder                    (1)                                                                    (1)                                                           (1)                    (2)                                    (2)                             (2)                                                           (   )                    (    )                                                                 (    )                                                 Bottleneck:     <                                                     Overcomplete:     >                                                   Reconstruction Error                                                                                                                                                                6
Deep Learning 勉強会 (Chapter 7-12) - SSSSLIDE
