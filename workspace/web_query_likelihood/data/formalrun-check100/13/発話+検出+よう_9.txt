実走行車内環境は騒音が多く,音声信号のみによって音声認識の前処理である発話区間検出を自動的に行うことが困難である.本稿では,筆者らが開発を進めてきた実走行車内環境で利用可能な画像情報と音声情報を統合した自動的な発話区間検出手法において,新たな画像発話特徴量,音声発話特徴量,統合手法,発話と非発話を判別するしきい値設定を導入することにより発話区間検出精度の向上を図る.本手法では,ドライバの口唇周辺画像における低輝度領域の面積変化を画像発話特徴量として,また,車載マイクロフォンより得られた音声の対数パワー変化を音声発話特徴量として抽出する.音声発話特徴量に対し,しきい値を用いて発話フレームと非発話フレームを判別し,発話中と判定されたフレームの前後にマージンを付加し検出区間を決定する.その際,画像発話特徴量に応じて音声発話特徴量にオフセットを加えることで画像情報と音声情報を統合し,誤検出の抑制を図る.実走行車内において収録されたカメラ映像及び音声を用いた実験の結果,提案手法における発話区間検出成功率が従来手法と比較して約10%改善されたことが確認された.
We propose a method for improving end point detection using driver's facial image and audio sequences taken in vehicles. In the proposed method, the number of low-intensity pixels inside the driver's mouth in each frame is used as a feature for detecting the lip movement. The logarithm power of sound captured by an in-vehicle microphone is used as a feature for extracting driver's utterance. End point of each utterance is detected using the sound feature enhanced by the image feature. The experimental results showed that the proposed method could reduce false detection and could improve the end point detection accuracy.

