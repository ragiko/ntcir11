 Google Analyticsによるアクセス解析
Twitterで聞かれたので、Google Analyticsによる1月のアクセス解析の結果も公開しておきます。
Google アナリティクス公式サイト - ウェブ解析とレポート機能 ? Google アナリティクス    
1月のPVは3万ちょっとでした。
Google Analyticsではユーザの属性もある程度までわかるようになっていて、たとえば地域は当然日本ばかりだけど少しだけアメリカもいるとか。
ブラウザはFirefoxが多いけど、Mac使いではChromeが一番多いとか。やっぱりうちのブログに来る層は偏ってますね。
あとリファラーははてなブックマークとTwitterが強いけど、検索ではかなり古い記事に来る人も多いとかが分かります。
そうそう、TwitterクライアントのHootSuiteの存在はアクセス解析で初めて知りました。
Social Media Management Dashboard - Hootsuite
面白いですね。
アクセス解析もまたアフィリエイトと同様に何かを自動的に処理してシステムを作れるほどのデータ量ではないのですが、解析した結果をサイト運営者が見てまたサイトを改善する、というサイクルは機械学習を使ってなにかシステムを作るというのとはまた違ったやり方がありそうです。ここまで書いてid:doryokujinさんのエントリを思い出しました、というところでこのエントリを締めさせていただきたいと思います。
解析者として僕が大事にしていること - doryokujin's blog
ツイートする
Permalink | 10:42
アフィリエイトはじめました
このブログをRSSではなく直接見ている人は気づいていたかもしれませんが、12月から左のサイドバーにAmazonのインスタントストアウィジェットを貼りつけています。単独のインスタントストアページはこちらにあって、自然言語処理や機械学習のお気に入りの本をまとめています。
自然言語処理の書籍 - nokunoの本棚
しかし、このウィジェットを貼りつけるとChromeで見たときに表示が崩れるのですよね…なんとかなりませんか?>はてなダイアリー Firefoxで見るとウィジェットに画像も表示されているのですがChromeだと表示されていなかったり、人気エントリのモジュールが動作していなかったり、はてなスターが消えていたりします。
それはともかく、1ヶ月たったので1月の戦績を晒してみます。
昨年末から毎日更新も維持していることもあって、滑り出しとしては上出来かなと思います。アクセス解析の結果についても後日公開したいと思います。
id:mamorukさんも以前書いていましたが、自分のブログを読んだ人がどんな商品を買っているのかわかるのがいいですね。直接掲載している商品だけでなく、idのついた状態で購入した商品が加算されるので買う側としては怖いかも。見る側としては、データ量が少ないので推薦システムが作れるほどではないのですが、見ていて面白かったり意外な発見があったりします。注文レポートから自分の知らなかった商品を知って、うっかりすると買ってしまったこともあります(笑)。
ツイートする
Permalink | 12:07
Hadoopのコマンドをエイリアスで簡単操作
Hadoopのコマンドって長いですよね。今まで覚えるために手で打っていたのですが、ついカッとなってエイリアスを作ってしまいました。
# Hadoop Alias
alias hls='hadoop fs -ls'
alias hlsr='hadoop fs -lsr'
alias hcat='hadoop fs -cat'
alias htext='hadoop fs -text'
alias htail='hadoop fs -tail'
alias hmv='hadoop fs -mv'
alias hcp='hadoop fs -cp'
alias hrm='hadoop fs -rm'
alias hrmr='hadoop fs -rmr'
alias hmv='hadoop fs -mv'
alias hget='hadoop fs -get'
alias hgetmerge='hadoop fs -getmerge'
alias hput='hadoop fs -put'
alias hmkdir='hadoop fs -mkdir'
alias hdu='hadoop fs -du'
alias hdus='hadoop fs -dus'
alias hsetrep='hadoop fs -setrep'
alias htouchz='hadoop fs -touchz'
alias htest='hadoop fs -test'
alias hcount='hadoop fs -count'
alias hstat='hadoop fs -stat'
alias hchown='hadoop fs -chown'
alias hchmod='hadoop fs -chmod'
alias hchgrp='hadoop fs -chgrp'
alias hhelp='hadoop fs -help'
alias hstream="hadoop jar $HADOOP/hadoop-streaming.jar"
$HADOOPはHadoopがインストールされたディレクトリです。hcd(カレントディレクトリを記憶しておいて、hlsやhcatのときに反映する)も作りたかったのですが、デバッグが面倒になりそうなのでやめました。良いやり方ないでしょうか?
私はあまり環境をいじり過ぎたくないタイプなのですが、よく使うものはさすがに耐え切れなくなってしまうようで、だんだんと増えていきます。
ちなみに他に使っているエイリアスはこのあたりです。
# Alias
alias ls='ls -F --color=auto'
alias ll='ls -lh'
alias la='ls -alh'
alias lt='ls -lht'
alias sort='LC_ALL=C sort -S 1000000'
alias mecab='mecab -b 1000000'
sortのときにLC_ALL=Cをつけ忘れないようにしておくと便利です。
ツイートする
Permalink | 07:50
ヒープと優先度付きキュー
アリ本ネタの続きです。今日はp.69のデータ構造の章のうち、ヒープと優先度付きキューについて。
プログラミングコンテストチャレンジブック
ヒープは次の性質を満たす木構造(通常は二分木)の一種です。
親ノードの値が2つの子ノードの値より小さい
木は左上から順に詰まっており欠けているノートがない
一般に後者の性質を満たす木には、配列で表すことができるという性質があります。このとき添字がnのノードの親ノードの添字は(i-1)/2と、子ノードの添字はn*2(左の子ノード)とn*2+1(右の子ノード)となります。この性質を利用してヒープを配列で実装すると、以下のようになります。
#include <iostream>
#include <stdlib.h>
using namespace std;
#define MAX_N 10
int heap[MAX_N], sz = 0;
void push(int x) {
int i = sz++;
while (i > 0) {
int p = (i - 1) / 2;
if (heap[p] <= x) break;
// swap
heap[i] = heap[p];
i = p;
}
heap[i] = x;
}
int pop() {
int ret = heap[0];
int x = heap[--sz];
int i = 0;
while (i * 2 + 1 < sz) {
int a = i * 2 + 1, b = i * 2 + 2;
if ( b < sz && heap[b] < heap[a]) a = b;
if (heap[a] >= x) break;
heap[i] = heap[a];
i = a;
}
heap[i] = x;
return ret;
}
int main() {
for (int i = 0; i < 10; i++) {
push(rand() % 10);
}
while (sz > 0) {
cout << pop() << endl;
}
}
ここで、ヒープへのノードの追加と削除はO(log n)で実行できます。
ヒープはSTLでpriority_queueとして実装されています。
#include <queue>
#include <iostream>
#include <stdlib.h>
using namespace std;
int main() {
priority_queue<int> pque;
for (int i = 0; i < 10; i++) {
pque.push(rand() % 10);
}
while (!pque.empty()) {
cout << pque.top() << endl;
pque.pop();
}
}
値が小さなものや、大きなものから順に取り出すことのできるデータ構造が必要な場合、priority_queueを使うと効率よく実装できます。単純にソート済みの配列やリストでは、追加や削除にO(n)のオーダーで時間がかかりますが、O(log n)で行えることは大きなメリットです。
ツイートする
Permalink | 10:43
トップダウンとボトムアップ
物事の進め方について考えていて、トップダウンとボトムアップについていくつかの気付きがありましたので、シェアしたいと思います。
トップダウン
トップダウンアプローチは、あるゴールを決めてそのゴールから逆算した道筋を辿るという方法論のことを指します。トップダウンにはゴールが見えているので進めやすくリターンも分かりやすいというメリットがあります。短期的なゴールには道筋が立てやすいのですが、長期的なゴールには道筋が立てづらく、近道をしようとして失敗しやすいというデメリットがあります。また組織においてトップダウン型のアプローチを取ると、末端の構成員には居心地の悪い場所になりがちです。
トップダウンアプローチの向いているケースとして、プロダクトがはっきりしているサービスやソフトウェアの開発、材料は揃っていてそれを組み合わせるだけでよい場合などが挙げられます。材料を作るところから始めるのには向いていないというイメージです。
ボトムアップ
ボトムアップアプローチは、必要な土台を下から積み上げることで目的を達成するという方法です。積み上げている段階でははっきりした到達点が見えているわけではなく、将来役立ちそうな基礎を固めて行くところに特徴があります。ボトムアップのメリットは、長期的な視野に立った行動を取れたり、ときには思わぬところで役に立つことがあったりすることです。デメリットは手段が目的化して役に立たなくなったり、すぐに効果が現れないためモチベーションの維持が難しいことなどです。組織の中では研究系の部門が担当する事が多いです。
ボトムアップアプローチには投資の側面があります。自主的な努力が必要なので、やりたいことがあって強いモチベーションを持った人が向いています。
どちらがいいのか?
トップダウンとボトムアップの関係は、大域最適と局所最適の関係にも似ています。どちらがいいとは一概には決められませんが、個人的にこの二つは同時には両立しないと思っているので、自分の中でフェーズを決めてトップダウンに進めるときとボトムアップに進めるときを長めのスパンで使い分けるようにしています。バランスを取るのは難しいので、せめて交互にやり方を変えるようにしています。
ツイートする
Permalink | 10:38
【ネタバレ】映画 ソーシャル・ネットワークを見てきました
というわけで、劇場版ザッカーバーグこと映画ソーシャル・ネットワークを観てきました。
フェイスブックの創業者、マーク・ザッカーバーグがフェイスブックを作りだす過程を描く。プログラマやハッカーとして天才的な能力を持つマークが、凡人離れした情熱を傾けてサイトを作っていく様子を、ジェシー・アイゼンバーグは独特のマシンガントークや不審な挙動などで、見事に表現している。フェイスブックの共同創始者でもあり、マークの友人として、フェイスブックを愛していたはずが、最終的にマークを訴えるエドゥアルドを演じたアンドリュー・ガーフィールドの抑えた演技も素晴らしい。監督のデヴィッド・フィンチャーは独創的な天才と、彼を理解できない凡人たちの織りなす古典的な悲劇を、極めて現代的な題材を通して見事に描き出した。
ソーシャル・ネットワーク | goo映画 
映画全体としては創業者ザッカーバーグに焦点が当てられているヒューマンドラマになっているので、登場人物は多いのですが話の筋はつかみやすかったと思います。ストーリーは訴訟を起こした側のインタビューを元に構成されているので、若干視点に偏りがあるように感じられました。映画なので脚色も多い(ハッカーをクラッカーの意味で使っていたりとか)と思いますが、事前に聞いていた情報でもあるのでまあ許容範囲内かなと感じました。原作である小説を読めばまた印象が変わるのかもしれません。
Web業界の人なら楽しめる小ネタもあって、Apacheのサーバからwgetするというセリフがあって思わずニヤリとしました。また映画のパンフレットの目次はPHPのコードになっていて、はっきり言って誰得としか言いようがなかったしべつにどうでもいいと思いました。翻訳はいくつか気になったところがあって、ザッカーバーグが女の子の採点サイトを作るために写真のクローラをPerlスクリプトで書いていたのですが、そのとき英語ではemacsでと言っていたのに字幕では削られていました。他にもSNSといえばMySpaceかFriendStarかというときにFriendStarが削られていたり、Palo Altoがシリコンバレーと訳されていました。英語の勉強を始めてから映画を見るのは初めてだったのですが、映画の字幕って意外と日本人に分かりやすいように意訳されてるんだな、ということがわかるようになっていたのに気づいたのが個人的な収穫でした。
訴訟の話が中心になっていく後半と比べて、前半部分は天才的なプログラマーに特有の万能感や、急成長中のベンチャーらしい熱気が描かれていてわりと気に入りました。サーバーがダウンしたときのパニック状態や、初めて収益が上がったときの興奮などを思い出して、Webサービスを作って運営したことのある人には共感できるのではないでしょうか。
そうそう、自分にとっては3月にFacebookのオフィスを見学しにいく計画を立てていたこともあってちょうどタイムリーな映画でした。根底の部分でコンピュータとアメリカ文化や法律の両方の知識が必要なのでどちらか片方がないと難しく感じるかもしれませんが、Web業界でエンジニアをやっている人は見ておいて損はないのではないでしょうか。
ツイートする
Permalink | 09:51
Yoh Okunoの日記
