本論文では,計算領域を確保するために低頻度語を削除したコーパス(文書集合)における語彙数とパープレキシティの関係を考える.コーパスがZipf則に従うという仮定のもとで理論的解析を行い,k-グラムモデルとトピックモデルのパープレキシティが特定の条件下では削減後の語彙数に関するべき乗則に近似的に従うことを証明する.この結果は,低頻度語は統計モデルの学習結果に大きな影響を及ぼさないという我々の直感に理論的根拠を与える.得られた結果について人工コーパス上の実験を行い理論の正しさを確認し,実コーパス上の実験により理論値と実測値の差を議論する.
This paper studies a relationship between perplexity and vocabulary size on a corpus (or documents), which is reduced to improve computational performance. We prove that perplexity of κ-gram models and topic models approximately follows a power law with respect to reduced vocabulary size under some condition, when a corpus follows Zipf's law. This gives a theoretical evidence for our intuition that low-frequency words may not make a large contribution to the performance of statistical models. We verify the correctness of our theory on synthetic corpora and examine a gap between theory and practice on real corpora.

