【課題】認識されたキーワードの信頼度にフレーム間での影響の相違を反映させ得る音声認識方法を提供する。【解決手段】受信された音声信号からフレーム別に特徴ベクトルが抽出される。各フレームの特徴ベクトルに対してキーワードモデルの尤度とフィラーモデルの尤度とが算出される。算出された二種類の尤度の比からフレーム別の信頼度点数(c1,…,cn)が算出される。各フレームの信頼度点数に基づき、各フレームの特徴ベクトルの属するクラスターが選択される。クラスター別に異なる変換関数(fcluster1,…,fclusterM)を用いてフレーム別の信頼度点数が変換される(C'1,…,C'n)。変換されたフレーム別の信頼度点数の平均値としてキーワードモデルの信頼度が算出される。
【特許請求の範囲】
【請求項1】  一連の音声信号からフレーム別に抽出された特徴ベクトルのそれぞれに対し、キーワードモデルの尤度を算出するキーワードモデル部;  前記特徴ベクトルのそれぞれに対し、フィラーモデルの尤度を算出するフィラーモデル部;及び、  前記キーワードモデルの尤度と前記フィラーモデルの尤度との間の比率(以下、尤度比という)に基づいて前記キーワードモデルの信頼度を前記フレーム別に重み付けして評価する信頼度測定部;を備える音声認識装置。【請求項2】  前記信頼度測定部が、    前記キーワードモデル部と前記フィラーモデル部との各出力に基づいて前記尤度比を計算するフレーム別尤度比計算部;    特徴ベクトルの分布範囲を複数のクラスターに分類し、前記フレーム別に抽出された特徴ベクトルの属するクラスターを判別するクラスター選択部;    前記クラスターごとに異なる変換関数を用いて前記尤度比を変換し、変換された前記尤度比を前記一連の音声信号の全体で平均し、その平均値を前記信頼度として採用する変換部;及び、    前記信頼度に基づき、前記一連の音声信号の表すワードとして前記キーワードモデルを採用するか否かを決定する決定部;を含む、請求項1に記載の音声認識装置。【請求項3】  前記クラスター選択部が、前記フレーム別に抽出された特徴ベクトルの属するクラスターを前記尤度比に基づいて判別する、請求項2に記載の音声認識装置。【請求項4】  前記決定部が、前記一連の音声信号の全体での前記尤度比の分布について歪度を算出し、前記歪度を用いて前記信頼度を補正する、請求項1に記載の音声認識装置。【請求項5】  前記フレームの一部が次のフレームと重なっている、請求項1に記載の音声認識装置。【請求項6】  前記フレームがサブワードより細かい、請求項1に記載の音声認識装置。【請求項7】  前記一連の音声信号を検出し、前記フレーム別に前記特徴ベクトルを抽出する特徴抽出部、を前記音声認識装置がさらに備え、  前記キーワードモデル部または前記フィラーモデル部が前記特徴抽出部により抽出された前記特徴ベクトルを利用する、請求項1に記載の音声認識装置。【請求項8】  前記特徴ベクトルが、MFCC、LPCケプストラム、PLPケプストラム、またはフィルタ係数のいずれかで表現される、請求項1に記載の音声認識装置。【請求項9】  前記キーワードモデル部がビタビデコーダを含む、請求項1に記載の音声認識装置。【請求項10】(a)一連の音声信号からフレーム別に抽出された特徴ベクトルのそれぞれに対し、キーワードモデルの尤度とフィラーモデルの尤度とを算出する段階;及び、(b)算出された前記キーワードモデルの尤度と前記フィラーモデルの尤度との間の比率(以下、尤度比という)に基づいて前記キーワードモデルの信頼度を前記フレーム別に重み付けして評価する段階;を有する音声認識方法。【請求項11】  段階(b)が、    特徴ベクトルの分布範囲を複数のクラスターに分類し、前記フレーム別に抽出された特徴ベクトルの属するクラスターを判別する段階;    前記クラスターごとに異なる変換関数を用いて前記尤度比を変換し、変換された前記尤度比を前記一連の音声信号の全体で平均し、その平均値を前記信頼度として採用する段階;及び、    前記信頼度に基づき、前記一連の音声信号の表すワードとして前記キーワードモデルを採用するか否かを決定する段階;を含む、請求項10に記載の音声認識方法。【請求項12】  前記クラスターを判別する段階では、前記フレーム別に抽出された特徴ベクトルの属するクラスターが前記尤度比に基づいて判別される、請求項11に記載の音声認識方法。【請求項13】  前記一連の音声信号の全体での前記尤度比の分布について歪度を算出し、前記歪度を用いて前記信頼度を補正する段階、を更に有する、請求項10に記載の音声認識方法。【請求項14】  前記フレームの一部が次のフレームと重なっている、請求項10に記載の音声認識方法。【請求項15】  前記フレームがサブワードより細かい、請求項10に記載の音声認識方法。【請求項16】  段階(a)以前に、前記一連の音声信号を検出し、前記フレーム別に前記特徴ベクトルを抽出する段階、を更に有する、請求項10に記載の音声認識方法。【請求項17】  前記特徴ベクトルが、MFCC、LPCケプストラム、PLPケプストラム、またはフィルタ係数のいずれかで表現される、請求項10に記載の音声認識方法。【請求項18】  段階(a)では前記キーワードモデルの尤度がビタビデコーダを用いて算出される、請求項10に記載の音声認識方法。
【発明の詳細な説明】【技術分野】【0001】  本発明は音声認識方法、及びその方法を用いた装置に関する。【背景技術】【0002】  近年、家電製品、カーナビゲーションシステム、又はゲーム機等で音声制御が一般化しつつある。更に、ホームネットワークやホームオートメーションに対する一般的な関心の高まりに伴い、音声制御、特にVUI(Voice  User  Interface)の開発/研究が産業全般で活発化している。特に一般的な家庭環境下での音声制御では、話者にとって自然な言語(自由発話)の中から制御に必要な単語(キーワード)が自動的に認識されねばならない。その場合、認識されたキーワードの信頼度が的確に評価されることで、キーワードの認識能力が向上する。【0003】  従来の自動音声認識では、1つのキーワードあるいは1つのサブワード(音素)ごとに信頼度が計算される(例えば、特許文献1参照)。特に特許文献1では、認識されたキーワードの信頼度が、そのキーワードを構成するサブワード別に重み付けられて評価されている。それにより、キーワードの信頼度に対するサブワード間での影響の相違がサブワード間での重みの相違として反映されるので、キーワードの信頼度の的確性が高い。【特許文献1】米国特許第6,539,353号明細書【発明の開示】【発明が解決しようとする課題】【0004】  音声認識では、検出された一連の音声信号が一般に一定長ずつ、複数のフレームに分割され、各フレームで音声信号の特徴が抽出される。更に、一般に複数のフレームから抽出された特徴に基づき、一つのサブワードが認識される。従って、認識された複数のサブワードから構成される一つのキーワードの信頼度に対しては、各サブワードに留まらず、各フレームが一般に異なる影響を与える。それ故、キーワードの信頼度の的確性を更に向上させるには、フレーム間での影響の相違が信頼度の計算に反映されねばならない。しかし、特許文献1のような従来の音声認識では、キーワードの信頼度に対し、そのキーワードを構成するフレーム間での影響の相違が反映されていない。  本発明の技術的課題は、認識されたキーワードの信頼度に対し、そのキーワードを構成するフレーム間での影響の相違を反映させ得る音声認識方法、及びその方法を利用する装置の提供にある。【課題を解決するための手段】【0005】  本発明による音声認識装置は、  一連の音声信号からフレーム別に抽出された特徴ベクトルのそれぞれに対し、キーワードモデルの尤度を算出するキーワードモデル部、  上記の特徴ベクトルのそれぞれに対し、フィラーモデルの尤度を算出するフィラーモデル部、及び、  キーワードモデルの尤度とフィラーモデルの尤度との間の比率(以下、尤度比という)に基づいてキーワードモデルの信頼度をフレーム別に重み付けして評価する信頼度測定部、を備える。特に信頼度測定部が、好ましくは、    特徴ベクトルの分布範囲を複数のクラスターに分類し、フレーム別に抽出された特徴ベクトルの属するクラスターを判別するクラスター選択部、及び、    クラスターごとに異なる変換関数を用いて尤度比を変換し、変換された尤度比を一連の音声信号の全体で平均し、その平均値をキーワードモデルの信頼度として採用する変換部、を含む。更に好ましくは、クラスター選択部が、フレーム別に抽出された特徴ベクトルの属するクラスターを尤度比に基づいて判別する。【0006】  本発明による音声認識方法は、(a)一連の音声信号からフレーム別に抽出された特徴ベクトルのそれぞれに対し、キーワードモデルの尤度とフィラーモデルの尤度とを算出する段階;及び、(b)算出されたキーワードモデルの尤度とフィラーモデルの尤度との間の尤度比に基づいてキーワードモデルの信頼度をフレーム別に重み付けして評価する段階;を有する。特に、段階(b)が、好ましくは、    特徴ベクトルの分布範囲を複数のクラスターに分類し、フレーム別に抽出された特徴ベクトルの属するクラスターを判別する段階、及び、    クラスターごとに異なる変換関数を用いて尤度比を変換し、変換された尤度比を一連の音声信号の全体で平均し、その平均値をキーワードモデルの信頼度として採用する段階、を含む。更に好ましくは、クラスターを判別する段階では、フレーム別に抽出された特徴ベクトルの属するクラスターが尤度比に基づいて判別される。【発明の効果】【0007】  本発明によれば、認識されたキーワードの信頼度が、そのキーワードを構成するフレーム別に重み付けされて評価される。従って、信頼度による評価の的確性が更に向上し、特に音声認識のエラー率が更に低減する。更に、認識されたキーワードを構成するフレーム間での尤度比の分布に基づいてそのキーワードの信頼度を補正することで、音声認識のエラー率を更に低減可能である。【発明を実施するための最良の形態】【0008】  まず、本明細書で使用される用語の意味を簡略に説明する。尚、以下の用語の説明は本明細書の理解を助けるためのものであり、本発明の技術的思想を限定する意味に解釈されるべきではない。【0009】−帰無仮説(Null  Hypothesis)と対立仮説(Alternative  Hypothesis)  帰無仮説は、「確信できるほどの強い反証がなければ受け入れる」仮説を意味する。対立仮説は、「帰無仮説を反証できるほどの強い証拠がある場合に受け入れる」仮説を意味する。帰無仮説と対立仮説とは相反するように設定される。以下、本明細書では、帰無仮説をH0と表し、対立仮説をH1と表す。【0010】−フィラーモデル(filler model)  非核心語(キーワード(核心語)以外の単語(ワード)や音素(サブワード))を探すためのモデルを意味する。非核心語の各々をモデル化する方法や、非核心語全体をモデル化する方法が存在する。フィラーモデルに基づいて測定された尤度から、検出された音声信号がフィラーモデルに該当するか否かが判断される。更に、フィラーモデルの尤度とキーワードモデルの尤度とを利用し、検出された音声信号から認識された単語がキーワードであるか否かが判別される。【0011】−HMM(Hidden  Markov  Model)  音声認識装置に最も良く使われる方法である。音声認識装置は、検出される音声信号をマルコフモデル(Markov  model)であると仮定する。、音声認識装置はまず学習段階で、モデルのパラメータを推定する。その後、音声認識装置は、推定されたパラメータを用いて未知の音声信号に最も適したモデルを探し出す。ここで、その音声認識では、音素または単語ごとにモデルが使用される。更に、使用されるマルコフモデルは、毎時間状態を変える有限状態機械(FSM:Finite  State  Machine)である。【0012】−尤度(likelihood)  尤度は、認識されたフレーム、ワード、またはサブワードが、対比されたキーワードモデルまたはフィラーモデルに実際に該当する確率を表す。例えば、認識された単語がキーワードモデルに属している特定の単語である確率が、そのキーワードモデルの尤度である。キーワードモデルがサブワードで構成されている場合、個々のサブワードの尤度がまず求められ、その後、それらの尤度から、キーワードモデルの尤度やフィラーモデルの尤度が計算される。その他に、サブワードが更に、状態やフレームに細分され、個々の状態やフレームごとに尤度が計算され、それらの尤度の集計結果から、認識された音声全体の尤度が計算されても良い。以下に説明される本発明の実施形態では、サブワードを構成するフレームまたは状態ごとに尤度が計算される。フィラーモデルに対しても、フィラーモデルを構成する様々なフレームや状態に基づいて尤度が求められる。【0013】  本発明の一実施形態による音声認識装置の構成は、音声認識部100と信頼度測定部200とに大別される(図1参照)。音声認識部100は、特徴抽出部110、キーワードモデル部120、及びフィラーモデル部130を含む。信頼度測定部200は、フレーム別尤度比計算部210、クラスター選択部220、変換部230、及び決定部240を含む。ここで、各部は好ましくは、専用のソフトウェアを共通のプロセッサで実行することにより機能する。その他に、各部が、FPGAやASIC等、専用のハードウェアで個別に構成されても良い。【0014】  特徴抽出部110は、好ましくはマイク(図示せず)の出力から一連の音声信号を検出し、その音声信号の周波数特性をフレーム別に計算し、各フレームから特徴ベクトルを一つずつ生成する。ここで、特徴ベクトルは好ましくは、MFCC(Mel  Frequency  Cepstral  Coefficient)で表現される。更に好ましくは、特徴ベクトルが、13個のケプストラム、13個のΔケプストラム(delta  cepstrum)、及び13個のΔΔケプストラム(delta  delta  cepstrum)で構成される。その他に、特徴ベクトルが、LPC(Linear  Prediction  Coefficient)ケプストラム、PLP(Perceptual  Linear  Predictive)ケプストラム、フィルタ係数等で表現されても良い。【0015】  特徴ベクトルをMFCCで表現する場合、特徴抽出部110はまず、検出された一連の音声信号をアンチエリアシングフィルタ(anti−aliasing  filter)に通し、その後、A/D(Analog/Digital)変換により音声信号をデジタル信号に変換する。特徴抽出部110は更に、デジタル音声信号をデジタルプリエンファシスフィルタに通す。このデジタルプリエンファシスフィルタの周波数特性H(z)は好ましくは次式(1)で表される:【0016】    H(z)=1−az-1。                                                (1)【0017】ここで、係数aは0.95〜0.98である。特に、この高域通過特性H(z)は好ましくは、人間の外耳/中耳の周波数特性に近似する。それにより、音声の唇での減衰(20dB/decade)が補償されるので、デジタルプリエンファシスフィルタを通過したデジタル音声信号には、対応する音声を発した人間の声道特性(フォルマント)だけが良好に反映される。更に、デジタル音声信号をデジタルプリエンファシスフィルタに通すことで、「人間の聴覚が1kHz以上の周波数帯域に対して敏感である」という事実が、音声認識に対してある程度反映される。【0018】  特徴抽出部110は、デジタルプリエンファシスフィルタを通過したデジタル音声信号に対してハミングウィンドウをかけ、デジタル音声信号を複数のフレームに分ける。ここで、各フレームの長さは一定であり、好ましくは20ms〜30msである。更に、各フレームの一部は次のフレームに重なる。好ましくは、フレーム間の重なりが10〜20msである。特徴抽出部110は続いてデジタル音声信号の各フレームに対し、FFT(Fast  Fourier  Transformation)を施す。ここで、FFTの他に、DFT(Discrete  Fourier  Transformation)が適用されても良い。特徴抽出部110は更に、各フレームのデジタル音声信号の周波数帯域を複数のバンドに分け、バンドごとにエネルギーを求める。ここで、バンドの形状とその中心周波数とが、人間の聴覚の周波数特性(すなわち、かたつむり管の周波数特性)を考慮して決定される。特徴抽出部110は最後に、各バンドのエネルギーの対数値に対してDCT(Discrete  Cosine  Transformation)を施し、MFCCを決定する。好ましくは、そのとき決定されるMFCCが12個である。各フレームの特徴ベクトルは、決定された12個のMFCCに加え、そのフレーム全体でのエネルギーの対数値を含む。特徴ベクトルはこれら13個のパラメータに加え、各パラメータの時間による一階微分係数(Δケプストラム)と二階微分係数(ΔΔケプストラム)とを含む。こうして、特徴ベクトルは計39個のパラメータを含み、すなわち、39次元空間のベクトルとして構成される。【0019】  特徴ベクトルがLPCケプストラムで表現される場合、デジタル音声信号のサンプルが過去のサンプルと組み合わされ、両者の差から計算された予測誤差がLPCとして決定される。特徴ベクトルがPLPケプストラムで表現される場合、デジタル音声信号が人間の聴覚の周波数特性に基づいてフィルタリングされる。その周波数特性のモデルとしては好ましくはラウドネス曲線(equal−loudness  curve)が利用される。フィルタリングされたデジタル音声信号は自己相関係数に変換され、更にケプストラム係数に変換される。PLPケプストラムによる表現では、「人間の聴覚が特徴ベクトルの時間的変化に敏感である」という事実が反映される。【0020】  キーワードモデル部120は好ましくは、ビタビデコーダ121とキーワードデータベース122とを含む(図2参照)。キーワードデータベース122は、HMM(Hidden  Markov  Model)を用いてモデル化された複数のキーワードモデル(例えば、普通名詞(「父」や「母」)、固有名詞(「チョンサンベ」や「ベクスングォン」)等、特定の単語(キーワード)を表す音声信号のモデル)を含む。ビタビデコーダ121は、好ましくはHMMのビタビ(Viterbi)アルゴリズムを用い、特徴抽出部110により一連の音声信号から抽出された特徴ベクトルのグループを、キーワードデータベース122に含まれている各キーワードモデルと比較し、キーワードモデルごとにワード全体に関する尤度を算出する。キーワードモデル部120は更に、キーワードデータベース122に含まれているキーワードモデル間で尤度を比較し、最も高い尤度を示したキーワードモデルを選択する。例えば図2に示されているように、「ベクスンチョン」というワードを表す一連の音声信号が特徴抽出部110へ入力されるとき、「ベクスングォン」というキーワードモデル全体の尤度11が127点であり、キーワードデータベース122に含まれているキーワードモデルの尤度の中では最も高い。従って、キーワードモデル部120は「ベクスングォン」というキーワードモデルを選択する。キーワードモデル部120は続いて、選択されたキーワードモデル(例えば「ベクスングォン」)の尤度を特徴ベクトル別に、すなわちフレーム別に算出し、信頼度測定部200に出力する。【0021】  フィラーモデル部130は、特徴抽出部110により一連の音声信号から抽出された特徴ベクトルのグループをフィラーモデルと比較し、そのフィラーモデルの尤度を特徴ベクトルごとに算出する。ここで、フィラーモデル部130により利用されるフィラーモデル(フィラーネットワーク)は、各音素がその音素に含まれている特徴ベクトルのみから同定されるモノフォンフィラーネットワーク、同定対象の音素とその前若しくは後に存在する音素との間の関係が考慮されるバイフォンフィラーネットワーク、又は同定対象の音素とその前後に存在する二つの音素との間の関係が考慮されるトリフォンフィラーネットワーク、のいずれであっても良い。フィラーモデル部130は特に、特徴抽出部110により抽出された特徴ベクトルのグループに対し、音素(例えば、「あ」、「い」、…、等の五十音)の様々な組み合わせ(フィラーモデル)ごとにその組み合わせ全体の尤度を算定する。フィラーモデル部130は更に、フィラーモデル間で尤度を比較し、最も高い尤度を示したフィラーモデルを選択する。例えば図2に示されているように「ベクスンチョン」というワードを表す一連の音声信号が特徴抽出部110へ入力されるとき、「ベクスンチョン」という音素の組み合わせ(フィラーモデル)の尤度12が150点であり、フィラーモデル部130により構成されたフィラーモデルの尤度の中では最も高い。従って、フィラーモデル部130は「ベクスンチョン」というフィラーモデルを選択する。フィラーモデル部130は続いて、選択されたフィラーモデル(例えば「ベクスンチョン」)の尤度を特徴ベクトル別に、すなわちフレーム別に算出し、信頼度測定部200に出力する。【0022】  図2は、キーワードモデル部とフィラーモデル部とが、検出された音声信号に対してキーワードモデルの尤度とフィラーモデルの尤度とをそれぞれ点数化する過程を例示する。例えば、「ベクスンチョン」という音声信号が入力されるとき、特徴抽出部110は、検出された音声信号からフレーム別に特徴ベクトルを抽出し、キーワードモデル部120とフィラーモデル部130とに提供する。ここで、各フレームには状態が割り当てられる。その状態は好ましくは、HMMのような有限状態機械(FSM)での状態を意味する。一つの状態が一つのフレームに対して割り当てられても良く、複数のフレームに対して割り当てられても良い。すなわち、一つのフレームが一つの状態を示しても良く、一つの状態の一部を示しても良い。キーワードモデル部120では、ビタビデコーダ121が、特徴抽出部110により抽出された特徴ベクトルを、キーワードデータベース122に登録されているキーワードの特徴ベクトルと照合する。その照合の結果、ビタビデコーダ121は、「ベクスングォン」というキーワードモデルに対し、尤度として127点を与える(図2に示されているビタビデコーダ121の出力11参照)。一方、フィラーモデル部130は、検出された音声信号「ベクスンチョン」からノイズまたは非核心語を除去するために、特徴抽出部110により抽出された特徴ベクトルを音素別に分けて処理する。図2に示されているフィラーモデル部130は特に、音声信号の全体にわたり、モノフォンフィラーネットワークの使用を繰り返す。ここで、他のフィラーネットワークが使用されても良い。その結果、フィラーモデル部130は、「ベクスンチョン」というフィラーモデルに対し、尤度として150点を与える。【0023】  フレーム別尤度比(LR:Likelihood  Ratio)計算部210は、キーワードモデル部120とフィラーモデル部130との各出力に基づき、フレーム別に尤度比を求める。例えば図3に示されているように、特徴抽出部110が、ワードwを表す一連の音声信号(n個のフレームを含む)からn個の特徴ベクトルx1〜xnのグループ310を抽出するとき、k番目のフレームから抽出された特徴ベクトルxkに対し、キーワードモデル部120が選択されたキーワードモデルH0の尤度p(xk|H0)を算出し、フィラーモデル部130が選択されたフィラーモデルH1の尤度p(xk|H1)を算出する。そのとき、フレーム別LR計算部210は尤度比LRとして、フィラーモデルH1の尤度p(xk|H1)に対するキーワードモデルH0の尤度p(xk|H0)の比を計算する:LR=p(xk|H0)/p(xk|H1)。フレーム別LR計算部210は更に、好ましくは尤度比LRの対数値を、k番目のフレーム(から抽出された特徴ベクトルxk)の信頼度点数ckとして設定する:ck=logLR=log｛p(xk|H0)/p(xk|H1)｝。こうして、特徴抽出部110により抽出された特徴ベクトルx1〜xnのグループ310に対し、n個のフレーム別の信頼度点数c1〜cnを含むグループ320が得られる。【0024】  クラスター選択部220は、フレーム別の信頼度点数ck(k=1、2、…、n)に応じて各フレームから抽出された特徴ベクトルxkに最も近いクラスターを探す。変換部230は、クラスター選択部220によりフレーム別の信頼度点数ckに対して選択されたクラスターに応じて変換関数を選択し、その変換関数を用いてフレーム別の信頼度点数ckを変換する。決定部240は、変換部230により変換されたフレーム別の信頼度点数C'kに基づき、認識されたワードがキーワードモデルに属しているか否かを決定する。以下、キーワードモデルに属するワードをIV(In  Vocabulary)ワードといい、キーワードモデルに属していないワードをOOV(Out  of  Vocabulary)という。認識されたワードがOOVワードであると判断された場合、そのワードの意味が検索され、または所定の命令のいずれにも該当しないので以後の処理を行わない。こうして、検出された音声が特に無意味な音声やノイズであった場合は、それらがOOVワードであると判別されるので、それらに対する処理が行われない。【0025】  キーワードモデル部120により算出された結果11とフィラーモデル部130により算出された結果12とはいずれも、特徴抽出部110により検出された音声信号に含まれているフレーム全体で尤度を合算した結果である。しかし、それぞれの合算過程では一般に、フレーム別に算出された各尤度に対して重みを与える方法が異なる。従って、キーワードモデル部120により算出された結果11とフィラーモデル部130により算出された結果12とのそれぞれについて、フレーム別の信頼度点数を適切に変換して合算し直す必要がある。【0026】  図3は、フレーム別LR計算部210がフレーム別に尤度比と信頼度点数とを測定する過程を例示する。検出された音声の表すワードwから、特徴抽出部110がフレーム別に特徴ベクトルのグループ310を抽出する(xkは、k番目のフレームの特徴ベクトルを表す)。各特徴ベクトルに対してキーワードモデル部120が尤度p(xk|H0)を算出し、フィラーモデル部130が尤度p(xk|H1)を算出する。ここで、符号H0は、「キーワードモデルが帰無仮説として設定されたこと」を表す。従って、キーワードモデル部120により算出された尤度p(xk|H0)は、「音声が正確に認識され、かつキーワードモデルに該当する確率」を意味する。一方、符号H1は「フィラーモデルが対立仮説として設定されたこと」を表す。従って、フィラーモデル部130により算出された尤度p(xk|H1)は、「音声が正確に認識され、キーワードモデルには該当せず、かつフィラーモデルに該当する確率」を意味する。【0027】  キーワードモデル部120により算出された尤度p(xk|H0)と、フィラーモデル部130により算出された尤度p(xk|H1)との間の尤度比LRに基づき、各フレームの特徴ベクトルがいずれのクラスターに該当するかが分かる。ここで、特徴ベクトルの分布範囲(39次元のパラメータ空間)は予め、尤度比に基づいて様々なクラスターに分類されている。更に、各クラスターに対し、フレーム別の信頼度点数を変換するための関数(変換関数)が予め設定されている。変換関数をクラスター別に設定することにより、信頼度点数の再評価で強調されるべき特徴ベクトルや、逆に弱められるべき特徴ベクトルを、フレーム別に識別できる。その結果、認識されたワードがキーワードモデルに含まれるか否かが、各フレームの影響を考慮して判断され得る。【0028】  クラスター選択部220は、特徴ベクトルの分布範囲(39次元のパラメータ空間)を、好ましくは各フレームの信頼度点数(すなわち尤度比)に基づいて複数のクラスターCluster1〜ClusterMに分類する(一般に、M>1。図4参照)。それにより、クラスター選択部220は、音声信号の各フレームから抽出された特徴ベクトルの属するクラスターを、そのフレームの信頼度点数に基づいて判別する。図4は、クラスター選択部220がフレーム別の信頼度点数に基づいて各フレームの特徴ベクトルに近いクラスターを選択する過程を例示する。図4に示されている各クラスターは、信頼度点数の範囲に応じて分類されている。従って、各フレームの信頼度点数に基づいてそのフレームの特徴ベクトルxkに最も近いクラスターが選択され得る。尚、クラスターの分類は、より一般的な、従来の分類と同様でも良い。すなわち、サブワードモデルや、サブワードモデルの状態に基づいて分類されたクラスターが利用されても良い。【0029】  k番目のフレームの尤度比率に対応する信頼度点数をckとする時、例えば図4に示されている、n個のフレーム別の信頼度点数c1〜cnを含むグループ320では、、先頭のフレームの信頼度点数c1が2番目のクラスターCluster2に対応し、2番目のフレームの信頼度点数c2と末尾のフレームの信頼度点数cnとが共に最後のクラスターClusterMに対応し、k番目のフレームの信頼度点数ckがi番目のクラスターClusteriに対応する。従って、先頭のフレームから抽出された特徴ベクトルx1が2番目のクラスターCluster2に属し、2番目と末尾とのフレームから抽出された特徴ベクトルx2、xnが共に最後のクラスターClusterMに属し、k番目のフレームから抽出された特徴ベクトルxkがi番目のクラスターClusteriに属する。各フレームの特徴ベクトルに近いクラスターが決定されれば、そのフレームの信頼度点数に対して適用されるべき変換関数が、そのクラスターに対して設定された変換関数に決まる。【0030】  変換部230はクラスターごとに異なる変換関数を用いて尤度比を変換する。クラスター選択部220が特徴ベクトルの分布範囲(39次元のパラメータ空間)をM個のクラスターCluster1〜ClusterMに分類している場合(図4参照)、変換部230は異なるM個の変換関数fcluster1〜fclusterMを各クラスターに一つずつ割り当てる(図5参照)。例えばn個のフレーム別の信頼度点数c1〜cnを含むグループ320では、先頭のフレームの信頼度点数c1が2番目のクラスターCluster2に対応するので2番目の変換関数fcluster2で変換され、2番目のフレームの信頼度点数c2と末尾のフレームの信頼度点数cnとが共に最後のクラスターClusterMに対応するので最後の変換関数fclusterMで変換され、k番目のフレームの信頼度点数ckがi番目のクラスターClusteriに対応するのでi番目の変換関数fclusteriで変換される。【0031】  変換関数は、キーワードモデル部120によりフレーム別に算出された尤度とフィラーモデル部130によりフレーム別に算出された尤度との間の比の特徴を拡張させる。従って、認識されたワードがキーワードモデルである(IV)か否(OOV)かの判別に、変換関数で変換された信頼度点数を利用することで、その判別の信頼度が高まる。好ましくは、変換関数fclusteri(i=1、2、…、M)として、次式(2)で表される線形関数が利用される。その他に、非線形関数が変換関数として利用されても良い:【0032】    fclusteri(k)(ck)=αclusteri(k)・ck+βclusteri(k)。               (2)【0033】ここで、引数i(k)は、k番目のフレームの信頼度点数ckに対応するクラスターの番号を示す引数kに依存する:i(k)=1、2、…、M、k=1、2、…、n。更に、二つのパラメータαclusteri、βclusteriは一般に、クラスターごとに異なる。変換部230により、n個のフレーム別の信頼度点数c1〜cnを含むグループ320から、n個の変換された信頼度点数C'1〜C'nを含むグループ325が得られる:C'k=fclusteri(k)(ck)(i(k)=1、2、…、M、k=1、2、…、n;図5参照)。【0034】  変換部230は更に、変換されたフレーム別の信頼度点数(又は尤度比)を一連の音声信号の全体で相加平均(又は相乗平均)し、その平均値を、キーワードモデル部120により選択されたキーワードモデルの信頼度として採用する。すなわち、キーワードwを表すキーワードモデルの信頼度CS(w)は次式(3)で与えられる:【0035】【数1】【0036】このように、キーワードモデルの信頼度CS(w)の計算では、n個のフレーム別の信頼度点数ck(すなわち尤度比;k=1、2、…、n)がM個の変換関数fclusteri(i=1、2、…、M)で重み付けされる。それにより、特にフレーム間での特徴ベクトルの分布の相違がクラスター間での変換関数fclusteriの相違(すなわち、二つのパラメータαclusteri、βclusteriの相違)として評価される。【0037】  決定部240は、変換部230から与えられるキーワードモデルの信頼度CS(w)を所定の閾値と比較する。信頼度CS(w)が閾値を超えた場合、決定部240は、特徴抽出部110により検出された一連の音声信号をIVワードとして判定し、キーワードモデル部120により選択されたキーワードモデルを一連の音声信号の表すワードとして採用する。すなわち、一連の音声信号がそのキーワードモデルの表すキーワードとして認識される。信頼度CS(w)が閾値より低い場合、決定部240は、特徴抽出部110により検出された一連の音声信号をOOVワードとして判定し、認識エラーを外部に通知する。【0038】  信頼度測定部200により実際に計算されるキーワードモデルの信頼度CS(w)にはバラツキがある。例えば図7には、IVワード(キーワード)を表す一連の音声信号(約12,500語)と、OOVワード(ノイズを含む)を表す一連の音声信号(約5,000語)と、のそれぞれに対して計算されたキーワードモデルの信頼度CS(w)がプロットされている。図7に示されている通り、信頼度CS(w)のバラツキ(特に平均値μと偏差σ)はIVワードとOOVワードとの間で大きく異なる。特に図7の縦方向では、IVワードに対する信頼度CS(w)の分布領域(In−Vocabulary)が、OOVワードに対する信頼度CS(w)の分布領域(Out−of−Vocabulary)から離れている。決定部240はその二つの分布領域間の相違(図7の縦方向のずれ)を利用し、信頼度CS(w)に基づくIVワードとOOVワードとの間の識別を実現させている。従って、二つの分布領域間の距離が大きいほど、決定部240によるIVワードとOOVワードとの識別の精度が高い。【0039】  好ましくは、各変換関数fclusteri(i=1、2、…、M)に含まれている二つのパラメータαclusteri、βclusteriが、学習により、次式(4)で表される信頼度CS(w)の平均値μと分散値σ2との関数(評価関数)Jの最小点に調節される:【0040】【数2】【0041】式(4)から明らかな通り、評価関数Jの最小点では、IVワードとOOVワードとの間での信頼度CS(w)の平均値の差μIV−μOOVが大きく、かつIVワードに対する信頼度CS(w)の偏差σIVとOOVワードに対する信頼度CS(w)の偏差σOOVとのそれぞれが小さい。従って、IVワードに対する信頼度CS(w)の分布領域が、OOVワードに対する信頼度CS(w)の分布領域から大きく離れている。それ故、決定部240によるIVワードとOOVワードとの識別が更に高い精度で実現可能である。【0042】評価関数Jの最小点は好ましくは、以下の最急降下法(steepest  descent  algorithm)による学習を用いて計算される。尚、最急降下法の他に、動的トンネリングなどの技法が使用されても良い。その学習過程では、変換関数fclusteri(i=1、2、…、M)に含まれている二つのパラメータαi=αclusteri、βi=βclusteriが使用されるごとに、次式(5)に従って更新される:【0043】【数3】【0044】ここで、m回目に使用されたパラメータαi=αclusteri、βi=βclusteriの値をαim、βimとし、更新される値をαim+1、βim+1とする。正の定数ηは学習係数であり、その調節によりパラメータαi、βiの各値が複数回の学習過程を経て評価関数Jの最小点(すなわち、パラメータαi、βiによる評価関数Jの微分係数が0である点)に効率良く収斂しうる(図6参照)。このような学習過程で得られた評価関数Jの最小点でのパラメータαi、βiの値が最適値として、変換関数fclusteriに適用される。【0045】  一つのワードwを表す一連の音声信号から得られるn個のフレーム別の(変換された)信頼度点数C'k(k=1、2、…、n)は一般にバラツキを持つ。そのバラツキは特に歪度(Skewness;平均値に対する分布の非対称性の程度、すなわち平均値の最頻値からのずれを表す)を持つ。ここで、認識された一つのワードwに対する歪度Skewness(w)が好ましくは、次式(6)で定義される:【0046】【数4】【0047】一つのワードw内で、変換された信頼度点数C'kが平均値(すなわち、キーワードモデルの信頼度)CS(w)より高いフレームが多い場合、歪度Skewness(w)が負である。一方、変換された信頼度点数C'kが平均値CS(w)より低いフレームが多い場合、歪度Skewness(w)が正である。決定部240は、このようにワードごとに異なる歪度Skewness(w)を用いてキーワードモデルの信頼度CS(w)を次式(7)で補正する(係数εは正の定数である:ε>0)。補正された信頼度CS(w)は変換された信頼度点数C'kの最頻値に接近するので、信頼度CS(w)に基づくIVワードとOOVワードとの識別が更に高い精度で実現可能である。【0048】        CS(w)=CS(w)−ε×Skewness(w)。                              (7)【0049】  図8、9は、本発明の実施形態による上記の音声認識装置と従来の装置との間で性能を比較した実験の結果を示している。その実験では、100人(男女各50人)の音声を登録することで、20,000命令語(utterance)のキーワードモデルを含むキーワードデータベースを構築した。ここで、キーワードモデルは、8個のミクスチャー(mixture)と3〜5個の状態(state)とで表現される。サブワードは304個(モノフォン(monophone)が46個であり、バイフォン(biphone)が258個)である。一方、フィラーモデルは、8個のミクスチャーと3〜5個の状態とで表現される。サブワード(モノフォン)は46個である。フィラーモデルの状態とフレームの信頼度点数とに基づいて分類されたクラスターの数は121個である。【0050】構築されたキーワードモデルとフィラーモデルとの性能をテストするために、20人(男女各10人)に、命令語、名前、地名などを言わせ、それらの音声を本発明の実施形態による音声認識装置と従来の装置とのそれぞれに認識させた。この際、パラメータ(αi、βiを含む)の学習では、IVワードを6,289語、OOVワードを2,496語使用した。音声認識のテストでは、IVワードを6,296語、OOVワードを2,497語使用した。テストの結果に基づく性能評価ではEER(Equal  Error  Rate)を使用した。ここで、EERは、OOVワードをIVワードとして誤認した比率(FAR:False  Alarm  Rate)と、IVワードをOOVワードとして誤認した比率(FRR:False  Rejection  Rate)とが一致したときの値を表す。従来の音声認識装置では、FARとFRRとの一致する点(EER)が6.08%であった(図8に示されている実線、及び図9参照)。一方、本発明の実施形態による音声認識装置では、EERが3.11%であった(図8に示されている破線、及び図9参照)。従って、本発明の実施形態による音声認識装置では従来の装置より、誤差発生率が48.8%減少した。本発明の実施形態による音声認識装置では更に、上記の歪度補正を行った場合と行わない場合との間での性能も比較した(図9参照)。歪度補正を行った場合、EERが3.02%であり、歪度補正を行っていない場合のEER(3.11%)より約0.09%減少した。すなわち、歪度補正を行った場合、従来の装置(EER6.08%)より誤差発生率が50.3%減少した。【産業上の利用可能性】【0051】  本発明は音声認識方法及びその方法を用いた装置に関し、上記の通り、キーワードモデルの信頼度をフレーム別に重み付けして評価する。このように、本発明は明らかに、産業上利用可能である。【図面の簡単な説明】【0052】【図1】本発明の実施形態による音声認識装置の構成を示すブロック図【図2】本発明の実施形態によるキーワードモデル部とフィラーモデル部とのそれぞれが一連の音声信号からモデルを選択し、そのモデルの尤度を算出する過程を示す模式図【図3】本発明の実施形態によるフレーム別LR計算部が、各フレームから抽出された特徴ベクトルの尤度に基づいてフレーム別の信頼度点数を決定する過程を示す模式図【図4】本発明の実施形態によるクラスター選択部が、フレーム別の信頼度点数に基づいてクラスターを選択する過程を示す模式図【図5】本発明の実施形態による変換部がフレーム別の信頼度点数を、対応するクラスターに応じて異なる変換関数を用いて変換する過程を示す模式図【図6】本発明の実施形態による変換関数に含まれているパラメータの学習による最適化で利用される評価関数の概形を示すグラフ【図7】本発明の実施形態による音声認識装置により評価されたIVワードとOOVワードとの各信頼度の分布を表すグラフ【図8】本発明の実施形態による音声認識装置と従来の装置とのそれぞれのFRRとFARとを示すグラフ【図9】図8に示されているグラフの原点近傍の拡大図【符号の説明】【0053】  210    フレーム別LR計算部  220    クラスター選択部  230    変換部  240    決定部
フレーム別に重み付けされたキーワードモデルの信頼度に基づく音声認識方法、及びその方法を用いた装置 - 特開2006−227628 | j-tokkyo
