一般的に人間の発する音声は、咽喉部の形状と、声帯ひだの振動によって決定されます。前者は随意筋なので意図的に動かすことができますが、後者は不随意な面を多く含んでいます。声帯は、脳の中で情動をつかさどる視床下部と迷走神経でつながっており、脳の状態に大きく支配されています。よって音声には、随意的要素と、脳の情動による不随意な要素の両方が含まれています。特に音声の基本周波数に、この要素が強く反映されるものと考えられています。
しかし従来の基本周波数の推定手法では、推定ミスが多いことが問題となっていました。当社及びAGI社では一般的な手法と異なり、ケプストラムを用いない基本周波数の推定出手法により、一般騒音下でも推定ミスのない新たな手法を開発しています。これにより、音声の周波数解析を自動で行うことが可能となりました。音声の解析を行うために、大規模な音声収録および評価の実験を行いました。総勢200名の被験者に感情を含んだ発話をしてもらい、計50000ファイルの音声ファイルを取得しました。次に、得られた音声ファイルを評価して、感情のラベル付けを行いました。1つの音声ファイルにつき10人の被験者が、発話者の感情がどれかを評価しました。
得られたデータを解析して、感情を認識するロジックルールを導きだしました。音声解析によって、データから200種類以上のパラメタを算出しました。そして得られたパラメタを解析することで、発話者間で感情ごとに共通するパラメタの傾向を明らかにしました。判明した傾向をロジックルールに置き換えることで、音声から感情を自動分析するアルゴリズムを作り出しました。これらのパラメタはあくまで基本周波数等から算出したもので、言語内容には依存しません得られたアルゴリズムの精度を検証するため、発話者本人が込めた感情と、アルゴリズムによる認識結果との一致率を算出しました。また比較のため、日本人、および日本語のわからない外国人を評価者として同様の評価実験を行い、本人の感情との一致率を算出しました。さらに、時間が経過した後に話者本人に再度評価してもらい、発話直後との一致率を算出しました。その結果、日本人と外国人のどちらよりも、アルゴリズムの一致率のほうが高く、時間経過後の本人一致率とほぼ同等であることが判明しました。このことから、当社及びAGI社のアルゴリズムの精度が第三者の主観評価よりも高く、主観評価の性能限界に迫っていることが示されました(図の結果は、ST Ver. 2.0のときのものです)。
このことから、99%や100%など、人の主観評価を大きく上回った一致率を標榜する技術については、その手法や一致率の算出方法を吟味する必要があるといえます。たとえば評価した音声データ数が極端に少ない(そのため100%の結果がでやすい)、怒りだけを検出する(「怒り」か「そうでない」かを判定するだけなら、ランダムでも50%の一致率が得られます)、といった点が重要になります。こうした基準を満たしていないものについては、科学的な検証を行ったとは認められないため、その数字を疑ってかかる必要があります。
さらに、アルゴリズムによる感情認識の結果が、本当に感情を反映しているのかを検証するために、脳活動状態との比較を行いました。被験者がfMRIのガントリー内で、防音対策を施したマスクマイクを装着した状態で、fMRI室外の実験者と会話を行いました。150秒の会話を連続で2度行い、その際の音声データとfMRIデータを取得しました。音声はST Ver. 2.0を用いて感情認識を行いました。そして脳の中でネガティブ情動活動をつかさどる右扁桃体の活動状況と、アルゴリズムの関係を分析しました。アルゴリズムの認識結果が怒りまたは悲しみのネガティブな感情のときと、それ以外のときの2条件の間で、右扁桃体の活動状況が異なっているかをt検定により分析しました。その結果、ネガティブな感情のときの右扁桃体活動状況が、そうでないときと比べて有意に高い(p<0.01)ことが示されました。
AGI: 精度検証
