本報告では実環境での音声認識に有効であると考えられる発話区間検出の手法を提案する.この手法では、音響情報と画像情報をベイジアンネットワークを用いて統合して扱うことにより,複数の音声を含む音源が存在する状況において,話者の発話区間を検出することが可能である.この発話区間情報は,音声認識で用いられるのみでなく,音声認識の前処理として用いられる適応ビームフォーマの学習に用いられる.適応ビームフォーマでは,話者位置を発話区間から,雑音の空間的特性を非発話区間から求めることにより,音源分離性能が大幅に向上する.情報統合の手段としてベイジアンネットワークを用いることにより,(1)音響座標系と画像座標系の対応を学習により求めることができる,(2)入力ノードの追加により,情報源の追加を容易に行える,(3)状況依存性を表現できる,などの利点がある.
In this paper, a method of detecting and separating speech events in a multiple-sound-source condition using audio and video information is proposed. For detecting speech events, sound localization using a microphone array and human tracking by a stereo vision is combined by a Bayesian network. From the inference results of the Bayesian network, the information on the time and location of speech events can be known in a multiple-sound-source condition. Based on the detected speech event information, a maximum likelihood adaptive beamformer is constructed and the speech signal is separated from background noises and interferences.

