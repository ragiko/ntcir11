
1.話し言葉における発声
「言葉の違う外国の人との自由なコミュニケーションを行ないたい。」この夢を実現するため、音声翻訳通信研究所では、いろいろな人が話す「自然な音声」を認識するための研究を行なっています。日常、我々が無意識に発声している「自然な音声」では、文字通りの発声になっていないことも多く、これが音声認識を難しくしています。文字通りの発声をしているかどうか。それはちょっと調べてみるとすぐわかります。
普段、友達や家族と喋っている会話を録音し、発声通りにできるだけ忠実に、子音や母音にあたるローマ字一字ごとの音をひろって書き表してみましょう。次に、喋っている言葉を漢字とひら仮名で書き表し、これを文字通りにローマ字書きをしてみます。これら2つのローマ字書きを比べてみると、一致していない、つまり文字通りに発声していない部分が多いことに気がつきます。例えば、「秋山さんの御予定は?」と言ったつもりでも、実際には「アキアマサンノゴオテイワ」となっていたり、「時間単位に」という言葉が「ジカンタインニ」と発声されていたりします。このような文字通りでない発声は、日常の会話音声では頻繁に見られますが、その言葉を聞き慣れている人間どうしの会話では問題なく理解することができています。しかし、その言葉を聞き慣れていない外国人や機械にとってこの問題は深刻です。
2.話し言葉を正しく認識させるために
前に述べたような、文字通りに発声されない音声を機械が理解することはできるのでしょうか。残念ながら、現状の音声認識システムで正しい認識結果を得ることは困難です。音声認識は、どういう単語がどういう読みをするかという情報が書かれた発音辞書を参照することによって行なっています。ここで、発音辞書の単語の読みは、文字通りの読み方で書かれています。つまり、発音辞書の単語の読みと話し言葉の喋り方が異なる場合に、音声認識が正しく行なわれなくなるのです。つまり、この発音辞書へ話し言葉の喋り方を事前に登録しておけば、「自然な音声」を正しく認識できると考えられるのです。例えば、「秋山」は「アキアマ」と発声されることがあり、「単位」は「タイン」と発声されることがあるということをシステムに事前に教えておくのです。
3.話し言葉の喋り方を一般的にとらえる
それではどうやって話し言葉の喋り方をとらえればよいでしょうか?これは、先に述べたように、録音された会話を「発音通りのローマ字表記」と、「文字通りのローマ字表記」により書き表して、両音を比較すればよいと考えられます。そこで我々は、音声認識システムを用いて自然発話音声の認識を行ない、認識結果(発音通りのローマ字表記)と人によって書き起こした文字列(文字通りのローマ字表記)を比較して話し言葉の喋り方をとらえるという方法を採用しました。つまり、「秋山(akiyama)」という単語が「アキアマ(akiama)」と認識された場合、話し言葉では「秋山」は/akiama/と発声されると考えるのです。このような対応づけを大量の音声データベースに対して行なうことにより、どういう単語がどのように喋られるかという大量のデータが作成できます。では、この大量のデータを音声認識システムで用いる発音辞書に適用するにはどうすればよいでしょうか。従来は、それぞれの単語の発声だけを用いて、その単語の喋り方を決め、この喋り方を発音辞書に登録するという方法を用いていました。しかし、この方法では、発声回数の少ない単語に対する信頼性が低くなったり、データベースに存在しない単語に対しては喋り方が分からないといった問題があります。そこで私達は、ある単語の喋り方を決める場合、データベース中のその単語の発声だけを用いるのではなく、データベース全体を使ってとらえた話し言葉の一般的な喋り方に基づいて、その単語の喋り方を決めるということを考えました。具体的には、「前後の音韻環境を考慮した音素列に対する喋り方をニューラルネットワークを用いてとらえる(図2)」というアプローチをとりました。[1] 
。
ここで、/akiyama/のように文字通りのローマ字表記を「標準発音列」、/akiama/のように発音通りのローマ字表記を「観測発音列」と呼ぶことにします。このとき、前後2音素(合計5音素)からなる標準発音列をニューラルネットワークの入力層に、標準発音列の中心音素に対する観測発音列を出力層に与えて学習を行ないます。例えば、/akiyama/(秋山)が/akiama/(アキアマ)になる場合、標準発音列「秋山」の5音素からなる部分発音列/akiya/の中心音素/i/に対応する観測発音列は/i/です。また、部分発音列/kiyam/の/y/に対する観測発音列はありませんから、これを「脱落」と考えます。このような対応を取った後に、部分発音列/akiya/に対しては、図1の入力層の/akiya/の各音素に対応するユニットに1を与え、出力層には置換用ユニットの/i/に対するユニットに1を与えます(他のユニットには全て0を与えます)。また、部分発音列/kiyam/に対しては、/kiyam/に対応するユニットに1を与え、出力層には脱落用ユニットに1を与えます。このような処理をデータベース全体に対して行なうことにより、ニューラルネットワークの学習を進めていきます。このように、1つのネットワークを用いて話し言葉の喋り方を学習することによって、信頼性の高い発音辞書ができると考えています。また、任意の標準発音列に対しても観測発音列を得ることができるため、データベースに存在しない単語に対しても話し言葉の喋り方で発音辞書に登録することが可能になります。
4.発音辞書の作成ニューラルネットワークの学習が終ると、次はこのネットワークを使って音声認識に使う発音辞書を作成します。こは、元の発音辞書の標準発音列をニューラルネットワークに入力したときの出力(観測発音列)を登録することによって実現できます。例えば、/goyoyaku/(御予約)に対しては、/goyoy/,/oyoya/,/yoyak/,/oyaku/を順にニューラルネットワークの入力層の各音素に対応するユニットに1を与えます。このとき、出力層の各ユニットの出力値の最大値がそれぞれ、脱落用ユニット//、および置換用ユニットの/o/,/y/,/a/のユニットであったとすると(図2)、「御予約」に対する発音は/gooyaku/となります。同様の処理を認識対象の単語全てに対して行なって発音辞書を作成していきます。
前の例では、ニューラルネットワークの出力値の最大値(1位の候補)を選びましたが、複数の候補を選び、一つの単語に対して複数の発音を辞書に登録することも可能です。実際に、多くの人に対して音声の認識実験を行なってみると、複数登録する方が性能が高くなるということを確かめています[1] 
。これは、ある単語を発声する場合、発声の仕方が人によって様々であることを表していると考えられます。ところで、一つの単語に対して複数の発音を辞書に登録する場合、見かけ上の認識対象の語彙数が元の辞書の語彙数より増えます(私達が行なった認識実験では、語彙数が2倍以上になりました)。一般に語彙数が増えると認識時間は遅くなりますが、実際は高速に音声認識を行なうことができることを確認しています。これは、システムに発声をした場合、話し言葉の喋り方に対する確からしさ(スコア)が文字通りの喋り方に対する確からしさよりも高くなり、システムが認識結果を出すまでに検証する候補の総数が減るためであると考えられます。このように、本方法は認識速度の改善という観点からも有効であるといえます。
5.さらに認識性能を上げるために
私達は、話し言葉の喋り方に対応した発音辞書の作成方法を開発しました。現在の方法は、各単語の先頭2音素、最終2音素に対しては文字通りの読みをそのまま用いています。これは、実際に発声されたときにどのような単語とつながるかということは、事前に確定することが困難なためです。さらに、品詞の違いなどによって、同じ音素の系列の喋り方が変わってくることも考えられますが今のところ考慮していません。現在の方法をこれらに対して発展させていくことによって、認識性能を更に向上させていくことができると考えています。 
参考文献
本文
