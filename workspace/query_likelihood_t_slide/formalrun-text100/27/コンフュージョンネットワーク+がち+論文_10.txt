 TOEICの結果が来ました
TOEIC IPテストの結果が来ていました。
Listening: 330
Reading: 380
Total: 710
実際にアメリカに行ってみて、このレベルでは全然通用しないということが分かったので、目安としては900くらい自然に取れるように(TOEICに限らず)英語の学習を進めたいと思います。
エンジニアの英語勉強法 - Yoh Okunoの日記
のエントリを書いた後は、
どんどん話すための瞬間英作文トレーニング (CD BOOK)
みるみる英語力がアップする音読パッケージトレーニング(CD BOOK)
の2冊を買ってやっています。あとは、
英語が苦手な大人のDSトレーニング もっとえいご漬け
を試してみたり、
Programming Interviews Exposed: Secrets to Landing Your Next Job (Programmer to Programmer)
で将来に備えたりしてみようかな。
あとはNHKのラジオ番組が良いという噂を聞いたのですが、PodCastやradikoはやっていないのですね…iPhoneで手軽に聞く方法はないでしょうか。
ツイートするシェア
Permalink | 07:06
Luceneの曖昧検索を100倍高速化したアルゴリズム
@nobu_k さんのつぶやきでこのエントリを知りました。
Changing Bits: Lucenes FuzzyQuery is 100 times faster in 4.0
Luceneで曖昧検索を効率化した話です。
最初の実装では、転置インデックスを全探索して編集距離がN以下の単語を拾っていた
レーベンシュタインオートマトンという、編集距離がN以下の単語のみをアクセプトするオートマトンを利用することにした
単語ごとに構築したレーベンシュタインオートマトンをマージするという操作が必要になるが、なかなかうまくいかなかった
難解な論文を見つけたが、実装は難しかった
良いライブラリを見つけたので、PythonからJavaに移植した
最後に1つだけ残ったバグは、移植の失敗ではなく元ライブラリのバグだった。報告すると1日で直ってきた。
この前のエントリでは、有限状態トランスデューサを使った辞書の圧縮についても書かれています。
Changing Bits: Using Finite State Transducers in Lucene
翻訳しようと思ったのですが息切れしたので投げっぱなし。
最近このFSTとかWFSTをよく見かけるので、ちょっと調べてみようかな、と思っています。
ツイートするシェア
Permalink | 07:03
かな漢字変換における共起情報の利用
統計的かな漢字変換において、系列ラベリングの枠組みを利用したクラスバイグラムやCRFでは、系列のマルコフ性を仮定して計算量を下げている。しかし文中の2つ以上離れた単語の間には関連性があると考えられるので、共起情報を利用して変換精度を上げるという工夫が行われている。ここでは、かな漢字変換における共起情報の利用について考える。
非局所素性を用いたかな漢字変換
NLP2011でジャストシステムの高岡さんらの発表を聞いた。
NLP2011 program
CRFで文中の共起を素性として取り込むために、ラティス中のパスを展開したりして実装するというもの。実際には展開する長さを制限しないと計算量が爆発してしまうとのことで、完全な大域的素性を取り入れるのは難しいようだ。CRFではなくStructured PerceptronやStructured SVMを使えば、ViterbiだけでForward-Backwardが必要なくなるので楽そう、などと思った。
最適パス探索後のグリーディな修正
Mozcにも共起情報を反映するためのRewriterのコードが含まれている(データは含まれていない)ので読んでみた。
collocation_rewriter.cc -  mozc -   Mozc - Japanese Input Method for Chromium OS, Android, Windows, Mac and Linux - Google Project Hosting 
Mozcの実装では、まずビタビアルゴリズムで最適パスを探索した後に、共起情報を利用してグリーディに修正している。このとき単語境界は変更せずに単語の選択のみを変更する。また、右から見た共起と左から見た共起の両方を使う。とても現実的な実装だと思う。
双対分解による構造学習
かな漢字変換ではありませんが、最近の自然言語処理の国際会議では双対分解(dual decomposition)という手法が流行っているようです。
双対分解による構造学習  |  Preferred Research 
この方法を使うと、2つの目的関数の和の最適解の勾配を各問題の最適解によって求めることができるので、バイグラムモデルの解と共起情報の利用が統合できるのではないかと思います。ただし、最適解を求めるだけで勾配法による繰り返し計算が必要となるので、繰り返しを数回で打ち切るなどの工夫でうまく動かさないとIMEでの応用は難しいと思います。
ツイートするシェア
Permalink | 07:52
Facebookの新しいリアルタイム解析システムとは?
Facebookの新しいリアルタイム解析のシステムでは、HBaseで1日200億件のイベントを処理しているそうです。以下の記事の翻訳です。
Facebooks New Realtime Analytics System: HBase to Process 20 Billion Events Per Day - High Scalability - 
Facebookがまたやってくれた。彼らは巨大なリアルタイムデータのストリームを処理するもう1つのシステムを構築したのだ。以前にもFacebookはリアルタイムなメッセージシステムをHBaseで構築している(参考)。今度のシステムは1日200億件(=1秒あたり20万件)のイベントを30秒以内に処理するリアルタイム解析システムだ。
Facebookの技術マネージャAlex Himelは構築したシステムについて説明している(ビデオもある)。それによると必要とされるスケールは以下のようなものだという。
ソーシャルプラグイン(Likeボタンなど)は過去数年間のうちにたくさんのウェブサイトにとってトラフィック源に成長した。先週、私たちはサイト管理者がよりよくアクセスを解析できるシステムをリリースした。このシステムにより、管理者は人々がいかに彼らのコンテンツに惹きつけられるかを知り、彼らのウェブサイトをリアルタイムに最適化するための助けとなることができる。これを実現するために、私たちは1日200億件(=1秒あたり20万件)のイベントを30秒以内に処理するシステムを構築する必要があった。
アレックスはそのプレゼンテーションにおいて素晴らしい仕事をしたので、一見をおすすめする。しかしここではより深く見てみることにしよう。
このような強力な解析システムの必要性は、Facebookがソーシャルプラグインを通してWeb支配を進めようとしていることに起因する。全ての外部サイトのアクセスはFacebookに取り込み、Facebookのもまた外部サイトにリンクしようとしている。基本的にできることはFacebookを通してアクセスをキャプチャおよびフィードバックすることと、Facebookで可能なことは全てあなたのウェブサイト上に表示することだ。これによって両者の関係をより近づけることができる。
あなたは外部のウェブサイトでソーシャルプラグインを見たことがあるはずだ。ソーシャルプラグインはあなたの友達がウェブ全体のどこでLikeボタンを押したり、コメントしたり、共有したりしたかを知ることができるようにする。ソーシャルプラグインを置くことで、コンテンツをより魅力的にできるということだ。あなたの友達はあなたが何を好むかを知ることができ、一方でウェブサイト側はユーザが好んでいるか知ることができる。魅力的なコンテンツはより多くのクリックと、より多くの「いいね!」とより多くのコメントを得ることができる。ビジネスやブランディング、あるいは個人にとっても、コンテンツがより人を惹きつけるほど、より多くの人々がそれを目にし、より多くフィードに現れ、より多くのトラフィックをサイトに誘導する。
例えばこのブログ(訳注:HighScalabilityのこと)のポストでは現在Likeボタンを設置している。TechCrunchはFacebookのコメントシステムを採用したことで有名だ。すぐにそのコメントシステムの質が議論になるが、問題はそれではなく、TechCrunchがFacebookの5億人以上のユーザのエコシステムに加わったということだ。Likeボタンとコメント以外には、推薦システムやフィード、ログイン、登録、Facepile、そしてライブストリームなどのプラグインがある。
すべてのデータは解析しなければ意味がないし、コンテンツ提供者にソーシャルプラグインが実際に彼らのサイトを魅力的にすることを証明しなければならない。そのためFacebookはInsightシステムをリリースしたのだ。この解析システムによって、あなたは収集された全ての有益なデータ(訳注:原文のjuicy dataという表現がツボに来ました)にアクセスできるようになる。Insightシステムは、Likeボタンの統計や、コメントボックスの解析、人気のページ、デモグラフィック(ユーザ層の分析)、組織的な共有などの機能を提供する。
数百万ものウェブサイトと何十億ものページと数百万ものユーザが、これらのソーシャルプラグインを通してデータを流し続けていると想像してみよう。どうすればこのようなデータすべてをリアルタイムに分析することができるだろうか? これは非常にチャレンジングな課題だ。
システムの目的
信頼できる方法で、たくさんの異なる基準と、データの偏りを考慮しつつ、リアルタイムなカウンタを人々に提供する。
匿名のデータを提供する。あなたは人々が誰であるかを特定することはできない。
プラグインが有用であることを証明する。あなたのビジネスは、そこからどのような価値を引き出すことができるか?
データをすぐに利用可能にする。ユーザがコンテンツをより価値あるものにするための手助けをする。
新しいUIのメタファ。漏斗のアイデアを利用している。
どれだけ多くの人がプラグインを目にして、どれだけの人がそれにアクションを起こし、どれだけの人があなたのサイトにトラフィックをもたらしたか。
データを最新のものにする。
リアルタイムにする。48時間かかっていたものを30秒でできるようにする。
この目的のためにいくつかの障害を取り除く必要がある。
なにが難しいか
イベントタイプの多さ
100以上の基準を扱う
プラグインのインプレッション(表示回数)
いいね!ボタンが押された回数
新しいフィードのインプレッション
新しいフィードのクリック回数
デモグラフィック(ユーザ属性)
データの規模
1日あたり200億件(1秒あたり20万件)のイベント
データの分布の偏り
Likeボタンはある種のべき分布に従う。大部分を占めるロングテールは少数のLikeしか受け取らないが、一部のサイトは巨大な数のLikeを受け取る。
この性質はアクセス過多の領域とキー、そしてロック競合の問題を引き起こす。
さまざまなプロトタイプ
MySQLによるDBカウンタ
キーとカウンタの行を持つテーブルを作る
データベースへの大量アクセスが起こる
書き込み頻度の高さはロック競合を引き起こし、データベースがオーバーロードしやすく、常に監視が必要で、分散の戦略を考え直す必要があった
この方法はこのような課題には向いていなかった
インメモリのカウンタ
IOのボトルネックが気になるなら、全てをメモリに突っ込むのだ
スケールに問題はない。カウンタはメモリに保存されるので書き込みは高速で、分散も容易だ。
インメモリのカウンタは、理由はわからないが他のアプローチより不正確だった。解析ではお金が動くためカウンタは非常に正確でなければならない。
彼らは実際にはこの方法を実装していない。これは思考実験であり正確性の問題から候補から外した。
MapReduce
以前のソリューションとしてはHadoopとHiveが使われていた。
柔軟で実行しやすい。大規模な読み込みと書き込みのIOに対処できる。事前にクエリの種類を知る必要がない。データはインデクシングすることなしに保存し、クエリを実行することができる。
リアルタイムではない。多くの依存関係がある。実行に失敗しやすい。複雑なシステム。リアルタイムという目的にそぐわない。
Cassandra
Hbaseのほうがより可用性と書き込み効率の点で適していると思われた。
書き込み効率が大きなボトルネックとなる点については改善された。
実際の実装:HBase + Scribe + Ptail + Puma
上位レイヤー
HBaseが分散したコンピュータにデータを保存する
追従(原文:tailing)アーキテクチャを採用し、新しいイベントはログファイルに保存され、読み出される
システムはイベントを巻きとり、ストレージに書きこむ
UIはデータを引っ張ってきてユーザに提示する
データフロー
ユーザがウェブページでLikeボタンをクリック
AJAXリクエストがFacebookに送られる
Scribeを使ってリクエストをログファイルに書き込む
Ptail
自社開発のツールで、Scribeのストアからログファイルの末尾を読み出す。
プラグインインプレッション、フィードインプレッション、アクションの3種類に分けて後続に処理に流される
Puma
バッチ処理は高負荷のキーを教えてくれる。Hbaseは1秒間に大量のデータ書き込みを処理できるが、それでもバッチ処理は必要である。人気の記事は大量のインプレッションとフィードインプレッションを生むため、巨大なデータの偏りがI/Oに問題を引き起こす。この場合、バッチ処理がより適している。
バッチといっても平均1〜5秒程度で処理される。もっと長くするとURLが多くなってハッシュテーブルの生成にメモリ不足が生じる。
ロック競合の問題を避けるために、新しいバッチが始まるときは最後のフラッシュが終わるのを待つ
UIはデータを表示する
フロントエンドは全てPHPで書かれている
バックエンドはJavaで書かれていてThriftでPHPから使えるようにしている
キャッシュを使ってより高速にウェブページを表示する
パフォーマンスは統計的に変動する。
より多く長く使われるデータほどキャッシュされやすい。
memcacheで異なるキャッシュTTLを設定している。
MapReduce
データはHiveで解析できるようにMapReduceに送られる。
これはデータをHiveから復活するためのバックアップとしても機能する。
生ログは一定期間の後に削除される。
HBaseは分散された列指向のデータストアである
Hadoopへのデータベースインターフェース。FacebookにはhBaseの内部を開発しているエンジニアがいる。
リレーショナルデータベースと違って、テーブル間のマップを作ったりはしない。
行キーからたくさんの疎なカラムを得ることができ、非常に柔軟である。スキーマを決める必要がない。カラムファミリを定義することで、キーをそこに追加できる。
スケーラビリティと信頼性はWAL(write ahead log)、つまり実行される操作のログによるものである。
キーに基づいてどの領域のサーバに分散するかを決定する。
まずWALを書きこむ。
データはまずメモリに入れられる。ある時点で、あるいは十分なデータが蓄積されたらディスクに書きだす。
もしサーバーが停止したら、WALからデータを復旧できる。そのため永久にデータを失うということがない。
高いIO信頼性のためにログとインメモリのストレージの組合せを使う。
HBaseは失敗の検出を行い、自動的に
現在はHBaseの再分散は手動で行われる。
自動的なホットスポットの検出と再分散はHBaseのロードマップにあるが、まだ実装されていない。
毎火曜日に誰かがキーを見てシャーディングのプランに変更を加えるかどうか決定する。
スキーマ
サーバ1台あたり1秒間に10,000書き込みを処理する
チェックポイントによってログファイルからデータ読み込み中にデータが失われるのを防ぐ
クリック詐欺を検出するのにも使えるが、まだビルトインでは詐欺の検出は組み込まれていない
さらなるホットトピック
今後の方向性
トップリスト
もっともLikeされたURLのリストを提示することは、例えばYouTubeのように数百万のURLが素早く共有されるような領域では非常に難しい
インメモリでソートしてデータの変更に対して最新に保つためには新しい解決策が必要だ
ユーザ数の集計
タイムウィンドウ内であるURLをLikeしたユーザの数。MapReduceでは容易に計算出来るが、単純なカウンタという方法で行なうことは困難である。
ソーシャルプラグイン以外のアプリケーションへの一般化
複数のデータセンターのサポート
現在は単一のデータセンターしかサポートしていない。しかし複数のデータセンターに拡張したい。
現在のやり直しプランはMapReduceシステムを使うこととなっている。
バックアップシステムは毎晩テストされている。Hiveとこの新しいシステムの結果の一致を見ている。
プロジェクト
約5ヶ月かかった。
最初は2人のエンジニアから始まった。次に50%のエンジニアが加わった。
2人のUIエンジニアがフロントエンドを担当している。
14人がエンジニアリング、デザイン、PM、そして運用に携わっている。
おわりに
私たちが以前のメッセージングシステムとこの解析システムを見たとき、2つのシステムには共通部分があることに気づいた:大規模、HBase、リアルタイム。信頼性と即時性を維持して巨大な書き込みを行なうというチャレンジは、これらの課題に共通の基盤となっている。FacebookはHBase, Hadoop, HDFSのエコシステムにフォーカスしており、運用上の問題が改善されることを期待している。別の部分ではCassandraが好まれていて、スケーラビリティや複数データセンター対応、運用の手軽さなどで優れているが、このような解析の課題には適していなかった。
このことはあなたにとってどのような意味を持つだろうか? もしあなたがFacebook社員ではなかったとしても、このアーキテクチャは十分にシンプルで、十分に切り離されたツールから構成されており、もっと小さなプロジェクトでもうまく動くだろう。
ツイートするシェア
Permalink | 07:16
Amazon EC2を無料で使ってみた
Amazon Web Service(AWS)のEC2は、Elastic MapReduceを通してしか使ったことがなかったのですが、停電の影響で自宅サーバーの機能を移設したいと思い、EC2を直接使ってみることにしました。
Amazon Elastic MapReduceに今さら入門してみた - Yoh Okunoの日記
なお、マイクロインスタンスの場合は1年間の無料利用枠があります。
AWS クラウド 無料利用枠 | アマゾン ウェブ サービス(AWS 日本語)
設定の方法はこの辺りを参考にコンソール上で立ち上げを行いました。
no title
立ち上げたインスタンスは、「Basic 32-bit Amazon Linux AMI 2011.02.1 Beta」というOSで、yumを使ってソフトウェアのインストールができます。立ち上げ時の注意点としては、sshの鍵(keypair)を保存しておくことと、Webサーバーを使う場合は80番ポート(HTTP)を開けておくことくらいでしょうか。sshで接続するときにコンソール上で右クリックConnectと進むとpemを使った設定方法が書かれています。ログイン名はrootではなくec2-userでログインする必要があります。
各種情報を確認してみます。OSはAmazon独自のLinuxです。
$ cat /etc/system-release
Amazon Linux AMI release 2011.02.1.1 (beta)
メモリは617MB使えます。
$ cat /proc/meminfo
MemTotal:         617016 kB
MemFree:          480136 kB
Buffers:            9296 kB
Cached:            96928 kB
SwapCached:            0 kB
Active:            58476 kB
Inactive:          62516 kB
Active(anon):      14768 kB
Inactive(anon):       40 kB
Active(file):      43708 kB
Inactive(file):    62476 kB
Unevictable:           0 kB
Mlocked:               0 kB
HighTotal:             0 kB
HighFree:              0 kB
LowTotal:         617016 kB
LowFree:          480136 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:                 0 kB
Writeback:             0 kB
AnonPages:         14764 kB
Mapped:             6508 kB
Shmem:                44 kB
Slab:              10340 kB
SReclaimable:       8196 kB
SUnreclaim:         2144 kB
KernelStack:         220 kB
PageTables:         1016 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:      308508 kB
Committed_AS:      52328 kB
VmallocTotal:     234488 kB
VmallocUsed:        1184 kB
VmallocChunk:     232636 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:      629760 kB
DirectMap2M:           0 kB
CPUは1個だけです。
$ cat /proc/cpuinfo
processor     : 0
vendor_id     : GenuineIntel
cpu family     : 6
model          : 23
model name     : Intel(R) Xeon(R) CPU           E5430  @ 2.66GHz
stepping     : 10
cpu MHz          : 2660.000
cache size     : 6144 KB
fdiv_bug     : no
hlt_bug          : no
f00f_bug     : no
coma_bug     : no
fpu          : yes
fpu_exception     : yes
cpuid level     : 13
wp          : yes
flags          : fpu tsc msr pae cx8 cmov pat pse36 clflush dts mmx fxsr sse sse2 ss ht pbe nx lm constant_tsc up arch_perfmon pebs bts aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 lahf_lm tpr_shadow vnmi flexpriority
bogomips     : 5320.00
clflush size     : 64
cache_alignment     : 64
address sizes     : 38 bits physical, 48 bits virtual
power management:
とりあえずこんな感じに設定してみました。
$ sudo yum install zsh screen git vim python php httpd
ツイートするシェア
Permalink | 09:29
Yoh Okunoの日記
