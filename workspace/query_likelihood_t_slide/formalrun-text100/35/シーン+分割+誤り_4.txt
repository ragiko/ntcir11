ネットワーク上で利用できるビデオ教材は増えてきつつあるが、まだ少ない。ビデオ教材が少ない原因の一つはビデオ編集に手間と時間を要するためだと考えられる。そこで本報告ではビデオ教材作成支援を目的とし、編集前の講義ビデオの音声情報から自動的にシーン分割位置を推定する方法について検討する。ビデオの音声情報から認識を行った結果得られたテキスト情報より独立成分分析を用いて求められた指標を動的計画法により順次比較することでシーン分割位置推定を行った。この方法で約60分の編集前の講義ビデオを用いて音声認識誤りがない条件で、シーン分割位置推定を行った結果約96%の再現率を得た。また音声認識性能がシーン分割位置推定に及ぼす影響を調査するため書き起こしテキストに単語正解率が10〜100%になるように置換誤りを挿入した結果、音声認識における単語正解率が約30%程度あれば約70%以上の再現率が得られる見通しが得られた。
This paper proposes a method of segmentation that segments lecture video material into video scenes based on speech signals for creation of educational video contents. To represent subtopics of video scenes, the text recognized by ASR from a lecture speech was converted into an index using independent component analysis (ICA) instead of conventional TF-IDF. This research attempted a method of segmentation using dynamic programming that minimizes the sum of cosine distances between adjacent indexes that represent subtopics of video scenes. The validity of the proposed method was evaluated using a sample lecture video of approximately 60 minutes. Results indicated that the proposed method using ICA obtained approximately 96% recall without ASR errors. The method of segmentation using ICA was much faster than the method using TF-IDF because the size of the index using ICA was smaller than that of TF-IDF. In addition, this research investigated how ASR errors affect scene segmentation. The recall was more than approximately 70% when the word accuracy by ASR was more than approximately 30%.

