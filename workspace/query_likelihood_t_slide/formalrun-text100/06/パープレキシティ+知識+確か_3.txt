 Kneser-Ney Smoothing を試してみた
言語モデル, 言語処理
Kneser-Ney Smoothing は高性能な言語モデルである。と、よく聞かされて知っているつもりだけど、まだ一度も試したことがなかったので、試してみた。
コードはここ。
https://github.com/shuyo/iir/blob/master/ngram/knsmooth.py
実験用にべったり書いているのでコピペは多いし、速度やメモリの効率も悪いが、まあ気にしないで。
コーパスは最初手元の適当なニュースコーパスを使っていたんだけど、それだと再現検証できないので、nltk のコーパスを使うように変更した。
nltk.corpus の下にあるコーパスモジュール名を -c オプションで与えると、そのコーパスを使って additive smoothing と Kneser-Ney smoothing の perplexity を出してくれる。
デフォルトでは Brown コーパスを使うのだが、例えばロイターコーパスを使いたければこんな感じ。
$ ./knsmooth.py -c reuters
ちなみに、与えられた名前に対して対応するコーパスモジュールを動的に読み込むところはこんなコード。
m = __import__('nltk.corpus', globals(), locals(), [opt.corpus], -1)
corpus = getattr(m, opt.corpus)
これで opt.corpus に "brown" が入っていれば corpus は nltk.corpus.brown になるし、"movie_reviews" なら nltk.corpus.movie_reviews になるって仕掛け。
出力はこんな感じ。brown コーパスの場合。
$ ./knsmooth.py -c brown --seed=0
found corpus : brown (D=500)
# of terms=1045022, vocabulary size=47349
UNIGRAM:
additive smoother: alpha1=1.0000, perplexity=1171.142
Kneser-Ney: heuristic D=0.604, perplexity=1171.068
Kneser-Ney: minimum D=1.000, perplexity=1152.640
modified Kneser-Ney: perplexity=1155.159
BIGRAM:
additive smoother: alpha2=0.0031, perplexity=1335.287
Kneser-Ney: heuristic D=0.761, perplexity=562.420
Kneser-Ney: minimum D=0.872, perplexity=556.637
modified Kneser-Ney: perplexity=543.748
TRIGRAM:
additive smoother: alpha3=0.0010, perplexity=9491.406
Kneser-Ney: heuristic D=0.881, perplexity=549.436
Kneser-Ney: minimum D=0.965, perplexity=542.305
modified Kneser-Ney: perplexity=520.433
コーパスからランダムにある割合でテストデータを選び、残りを訓練コーパスとしている。テストデータの割合は -r オプションで与えられる(デフォルト = 0.1)。テストデータを選ぶ乱数のシードは --seed で与える。
単語はコーパスモジュールの words() が返すものをすべて使っている(つまり記号類も有効)。正規化は小文字化のみしている。
結果は 1-gram から 3-gram まで、それぞれ以下の perplexity を求めている。
加算スムージング。パラメータは [0.0, 1.0] の範囲で perplexity が最小になる値を選ぶ
Kneser-Ney スムージング。パラメータはヒューリスティックな値を選ぶ
Kneser-Ney スムージング。パラメータは [0.0, 1.0] の範囲で perplexity が最小になる値を選ぶ
modified Kneser-Ney スムージング。パラメータはヒューリスティックな値を選ぶ
結果をまとめると、こんな感じか。
1-gram では大差はない( interpolate ほぼしないから当然っちゃあ当然 )
2-gram 以上では Kneser-Ney は加算スムージングよりはるかに良い性能を叩き出す
modified Kneser-Ney は オリジナルの Kneser-Ney より確かに性能が良いが、定性的に違いがわかるほどのレベルではないだろう。計算コストを考えるとオリジナルで十分に思える。
3-gram で加算スムージングが悪化しているのはコーパスが小さいせいか(それにしても悪すぎるんだが……)。Kneser-Ney はそんな状況でもちゃんと性能アップしている。
というわけで、確かに Kneser-Ney はなかなかいいね、という結果に。
KN で尤度を求めるには N1+ やらを数えて覚えておかないといけないので大変。modified KN だと数えておかないといけないものがさらに多くて、KN の2倍くらい大変。
でも尤度を求めるのではなく、「文生成などのために次の単語の分布を求める」という場合には N1 類を数えて保持しておく必要がない(語彙をぐるっと舐めている間に、どれだけ discount したかは当然わかる)ので、KN / modified KN ともに気楽に使える。
ロイターコーパスと movie_reviews コーパスの結果もおまけ。
$ ./knsmooth.py -c reuters --seed=0
found corpus : reuters (D=10788)
# of terms=1549612, vocabulary size=29774
UNIGRAM:
additive smoother: alpha1=1.0000, perplexity=854.850
Kneser-Ney: heuristic D=0.564, perplexity=855.384
Kneser-Ney: minimum D=1.000, perplexity=850.667
modified Kneser-Ney: perplexity=850.833
BIGRAM:
additive smoother: alpha2=0.0018, perplexity=207.485
Kneser-Ney: heuristic D=0.670, perplexity=143.591
Kneser-Ney: minimum D=0.704, perplexity=143.527
modified Kneser-Ney: perplexity=142.154
TRIGRAM:
additive smoother: alpha3=0.0004, perplexity=512.933
Kneser-Ney: heuristic D=0.762, perplexity=93.349
Kneser-Ney: minimum D=0.799, perplexity=93.246
modified Kneser-Ney: perplexity=89.477
$ ./knsmooth.py -c movie_reviews --seed=0
found corpus : movie_reviews (D=2000)
# of terms=1422742, vocabulary size=38181
UNIGRAM:
additive smoother: alpha1=1.0000, perplexity=908.244
Kneser-Ney: heuristic D=0.561, perplexity=908.691
Kneser-Ney: minimum D=1.000, perplexity=902.749
modified Kneser-Ney: perplexity=903.249
BIGRAM:
additive smoother: alpha2=0.0023, perplexity=515.530
Kneser-Ney: heuristic D=0.739, perplexity=283.122
Kneser-Ney: minimum D=0.806, perplexity=282.376
modified Kneser-Ney: perplexity=278.357
TRIGRAM:
additive smoother: alpha3=0.0007, perplexity=2735.080
Kneser-Ney: heuristic D=0.851, perplexity=239.729
Kneser-Ney: minimum D=0.918, perplexity=238.210
modified Kneser-Ney: perplexity=230.498
ツイートするシェア
Permalink | コメント(0) | トラックバック(1)   
機械学習×プログラミング勉強会にて「ノンパラベイズ入門の入門」を発表しました #pgml
ノンパラベイズ, 機械学習
11/9 に開催された機械学習×プログラミング勉強会 vol.2 にのこのこ参加&発表。主催の愛甲さん、参加者&発表者の皆さん、会場を提供して下さった DeNA さんありがとうございました。
機械学習×プログラミング勉強会 vol.2 : ATND
愛甲さんから発表の依頼をいただいた時、言語判定の話をすればいいか〜とか考えて気楽に引き受けちゃったのだが、あれを20分で話すと痛い目にあうと広島方面で学んだことを思い出し。
じゃあ、テキストマイニング始めました的なことでも〜と構成を考えてみたのだが、データの前処理の話だけで20分使い果たして機械学習出てこなさそう。しかも発表順で中谷の次があんちべ先生の番。後ろに専門家がいるのにテキストマイニングの真似事とかしゃべってたら、やばい。
そこで、勉強会タイトルの「〜×プログラミング」にあわないのは承知しつつ、社内勉強会でやったノンパラベイズ入門的な話をブラッシュアップして持っていくことに。
「プログラマが本当に理解するには実装しないといけない」か
PRML
ジュンク堂池袋本店にて 10/11 に行われた「パターン認識と機械学習」(PRML) 愛好家の集まり、じゃあなかった、トークセッションにのこのこ行ってきた、ばかりか前でしゃべってきた。ありがとうございました&お疲れ様でした>各位
PRML同人誌 『パターン認識と機械学習の学習』(暗黒通信団) 刊行記念トークセッション 「今度こそわかる!? PRMLの学習の学習」
http://www.junkudo.co.jp/tenpo/evtalk.html#20121011_talk
参加して下さった上に感想までブログにしたためて下さった方には感謝感謝なわけだが、そういったブログの中で、@yag_ays さんがちょうど今気にしていたことを書かれていたので、ちょこっと紹介。
「今度こそわかる!? PRMLの学習の学習」に参加しました - Wolfeyes Bioinformatics beta
余談:PRMLの学習と実装に関する疑問について.
実装の話はトークセッションではあまり出て来なかったのだが,帰り際の電車の中で実装に関する疑問をふと思いついて「さっき訊いときゃよかった」と思ったので,折角なので書いてみる.
プログラマーなどは,アルゴリズムを自分で実装しないと本当に理解したことにならない,なんてことがよく言われるけれども,そういう環境で育った技術系の人がPRMLを読み始めると「書いてあることをひと通りまんべんなく実装しないといけない」みたいな感じになって,取っ掛かりとしては非常に辛いんじゃないかなーという個人的な印象がある.
全文は yag_ays さんの記事を参照してもらうとして、まず「実装によって理解が得られる」というのは本当のことかどうか。
PRML 読んでこんだけ実装しまくっている人間なら、この点について語ってもいいだろうw
PRML 読んでやってみた(上巻編) - Mi manca qualche giovedi`?
PRML 読んでやってみた(下巻編) - Mi manca qualche giovedi`?
例えば実装してみて初めて「あれー、このパラメータの初期値どうするんだろう。PRML には書いてくれてないな……」ということがわかったりする。あるいは、「肝心な更新式はどこに書いてあるんだろう。え? この演習問題解かないといけないの!?(汗」ということも。
実装するということは自ずと全てのステップを網羅することになる。一つでも飛ばしたら実装が完成しない。そのおかげで、気づいていなかった理解の漏れをチェックしてくれる効果はある。
よって、「実装によって理解が得られる」は真、というのは早とちり。
正確には、実装は「理解の漏れをチェックしてくれる効果」があるとしか言っていない。			その漏れを埋めるためにパラメータの初期値を行間から読み取ったり推測したり、演習問題を解いたりすることで初めて理解が得られるが、それは明らかに実装とは独立した話だ。推測したパラメータでうまくいくか検証するときには実装が役に立つが、それは他の人が実装したものでも良い*1。
また、仮に実装ができて漏れているステップはなさそうな雰囲気だからといって、個々のステップの理解が完全であるかは保証してくれない。「たまたまあってる」なんてのはよくあることだし、「間違っているのに気づかない」なんてもっとよくあること(ぐさっ)。
さてそうなると、せいぜい「実装することによって理解を得るチャンスが増える」というくらいが正解なのだろう。少なくとも「自分で実装しないと本当に理解したことにならない」なんて気負う必要などどこにも無いということは断言してもいい。
それでもつい「実装しなければいけない」と思ってしまう背景には、「実装するのは時間がかかる」事情がある。価値は感じているけどやりたくないことを「〜しなければいけない」と活用変化することで、モチベーションを無理やり捏造しているわけだ。
しかし、一般に「〜しなければならない」という強迫観念は学習効果を下げるだけなことは、「宿題しなさい!」と怒られ続けた方々には今更証明の必要もないだろう。
実際、世の中的には「三度の飯よりコードを書くのが好きな連中」(※注:真実)だときっと思われているだろうサイボウズ・ラボの中の人でさえ、PRML 読んでこんだけ実装しまくっているのは約 1 名しかいないのである。
ここまで言えば「実装しなければいけない」なんて悩む必要はないことは十分納得してもらえるのではないかと思うのだがどうだろう。
そんな事情に最近ようやくちょっと気づいてきて、正直反省している。
「やっぱり実装するべき?」と聞かれた時、つい気楽に「実装してみたらいいんちゃう?」と答えてしまっていたが、そんな無責任な意見でもしかしたら何人か PRML を読むのを断念させてしまったのではないか、と。
コードを書くことに、このブログ記事を書くのと同じかそれより低いコストしか感じてない、つまり「実装するのは時間がかかる」なんて露も思ったことがなく、「ちゃっちゃと実装したほうが早い」という人なので、間違ってもこの人にそんな質問をしてはいけない。
と、だからといっていきなり「実装しなくてもいいんだ!」という方向に振り切れてしまうのも良いことではない気がするので、安定少数派な実装組の援護射撃もしておこう。
「実装」もノートの上で計算するのと同等な理解のための手段の1つである。それを使えるかどうかは人によるわけだが、いわゆるプログラマはその点で非常に有利な立場にいる。せっかく使えるのだから使わない手はない。
例えば、今ちょうど甘利さんの「情報理論」を読んでいて、今まで超苦手だった情報量やエントロピーという概念をボトムアップで定義してくれていて、なんだそういうことだったのか、もっと早く読めば良かった……! と大変感動しているわけだが、それはまた別の話。
それはともかく、「情報理論」の中に英文のエントロピーとして、1-gram のエントロピーは 4.15 ビット、2-gram のは 3.57 ビット、8-gram になると 2.35 ビット……という話が出てきて、特に 8-gram のエントロピーの計算に対しては「まことにご苦労様な話である」などと添えられている。
この本が最初に出版された 1970 年なら確かに気が遠くなるほど高価な機械とそれを扱う技術が必要だったろうが、今なら適当な PC の上で「へー本当かなあ」くらいの軽い気持ちで確認してみることができる。あなたがコードを書けたなら。
「実装しなければならない」と考えるのではなく、「理解のための手段として実装という手も使える。プログラマとして、これほど有利な立場は活かしたいところ」(非プログラマにはそもそもその選択肢がない)と考えれば、ずいぶん見え方が変わってくるんじゃあないだろうか。
ツイートするシェア
Permalink | コメント(0) | トラックバック(2)   
PRML の読む章・飛ばす章(私家版)
PRML, 読書会
機械学習の定番教科書の1つと言われ、各地で読書会が開かれる「パターン認識と機械学習」(PRML)。読み解くにはある程度の解析と線形代数の知識が必要なため、数学が苦手な学生さんや××年ぶりに数式を目にしたというエンジニアたちを次々と「式変形できない……」という奈落に叩き込んでいるという。
サイボウズ・ラボの社内 PRML 読書会でもその現象が発生。見かねた同僚の光成さんが PRML で使われている数学の解説だけではなく、PRML の中で省略されている式変形の過程も含めて書き下したメモ(社内通称:アンチョコ)が暗黒通信団から「機械学習とパターン認識の学習」という同人誌として出版され、全国のジュンク堂で購入可能となるとちょっとしたムーブメントががが。
現在はアマゾンでも購入可能となっているが、もともとのアンチョコも PDF で無料公開(CC-BY ライセンス)されているので、紙の本でないと勉強する気にならない! とかでなければこちらの PDF 版をどうぞ。
PRML アンチョコ / 「機械学習とパターン認識の学習」(PRML同人誌) の PDF 版
https://github.com/herumi/prml/raw/master/main.pdf
さてそんな PRML 旋風も冷めやらぬ 10/11 にジュンク堂池袋本店にて、「パターン認識と機械学習」(PRML) および「パターン認識と機械学習の学習」(PRML 同人誌)のトークイベントが開かれることに。
PRML同人誌 『パターン認識と機械学習の学習』(暗黒通信団) 刊行記念トークセッション 「今度こそわかる!? PRMLの学習の学習」
http://www.junkudo.co.jp/tenpo/evtalk.html#20121011_talk
【追記】
開催の時の模様:
風が吹けばジュンク堂で「機械学習の学習」 | Cybozu Inside Out | サイボウズエンジニアのブログ
http://developer.cybozu.co.jp/tech/?p=654
「今度こそわかる!? PRMLの学習の学習」に参加しました - Wolfeyes Bioinformatics beta
http://yagays.github.io/blog/2012/10/12/prml-talksession/
「今度こそわかる!? PRMLの学習の学習」ジュンク堂書店池袋本店トークセッション - Togetter
http://togetter.com/li/388657
【/追記】
PRML の邦訳の仕掛け人たる神嶌先生(@shima__shima さん)
各地で行われる PRML 読書会の火付け人と言っていい @naoya_t さん
PRML 同人誌(副読本)の著者である光成さん(@herumi さん)
PRML 同人誌及びこのトークイベントの黒幕である竹迫さん(@takesako さん)
という豪華顔ぶれでもう十分おなかいっぱい、著者でもないし全然出なくっていいですよねー、と話していたのに、中谷も引っ張り出されることになった。なぜだ。
ジュンク堂でのトークセッションと言えば、「Web を支える技術」出版記念の折りのトークセッションに参加したい、でも非コミュなので電話申し込みが億劫、ってへたれなことをつぶやいてたら、ほかならぬ著者の山本さんに自分の参加申し込みの電話をかけていただいてしまったのが思い出深い。また、その懇親会の席で gihyo.jp での機械学習連載のお話をいただいちゃったり。
今回はたぶん自分で電話かけなくていい(んだよね?)ので、まあ電話かけるより前でしゃべるほうが気が楽か、という自己暗示を今からかけておくことにする。			それに日本一の本屋でのトークイベントに名前を連ねると聞けば、R南高校文芸部だった頃の中谷はきっと喜ぶことだろう*1。まあ自分の書いた本じゃあないことはかわいそうだから黙っておこう。
イベントの進行は黒幕にすっかり任せてあるので、何を話すことになるか正直まだわかっていないが、まあ思いつくテーマ的なものを勝手無責任に書き並べてみる。
元祖 PRML & PRML 同人誌の表話と裏話
PRML 読書会の勧め方と進め方
あなたは PRML を読まなくってもいい!?
PRML を数学の本って言うなキャンペーン
機械学習の過去と現在と未来
機械学習を使うことが目的化症候群
わあいもりだくさん。でもトークイベントたぶん正味1時間半くらいしかない。てへ。			どうせここにあげたことよりもっとずっと超絶おもしろくて役に立つテーマを竹迫さんが間違いなく考えているだろうから*2、イベントの先出しみたいな感じでこの範囲から適当に見繕ってざっくり書いてみるというのはどうか。ここでネタバレしておけばその内容は振られないだろうから、自分のしゃべる時間が少なくて済むだろうと言う計算も。
そういえば2年ちょい前に PRML 復習レーンが始まるよ、だって。 - Mi manca qualche giovedi`? では「 PRML の歩き始め方」という記事を書いたりしていた。そこで、その第2弾で「PRML の読む章・飛ばす章(私家版)」を書いてみることにしよう。
といっても、目的やメンバーに応じて読むべき範囲いろいろ考えられるわけで、あくまでこれはサイボウズ・ラボにて行った PRML 読書会の経験に基づくもの。和書の公式サポートにあるPRML コース(レベル分け)のセカンドオピニオンといったあたりで、ご了解していただきたく。
PRML の読む章・飛ばす章(私家版)
PRML はなかなか厚みのある本なので、これを読み通すにはものすごく時間がかかる。
naoya_t さんの始めた初代 PRML 読書会は、当初は演習問題を全部解くんだ! と意気込んでいたようだが、初回で挫折。演習問題は必要に応じて適宜解くことにして、月1回、各回6〜9時間(!)を費やして、14ヶ月で全14章を読み通した。
一方、サイボウズ・ラボでの社内 PRML 読書会は、主宰の独断と偏見で全体の 1/3 は飛ばし、演習問題も必要最小限に絞った。週2回(!!)、各回1時間〜1時間半を費やして、それでも8ヶ月かかった。
光成さんも書いているが、PRML は「1行も飛ばさないで読むべし, という本でもない」ので、ちゃんと本質的なところに時間をかけて、枝葉末節は飛ばしていくことができれば望ましい。まあ、雑学は楽しいんだけどね。
そこで実際にサイボウズ・ラボでの社内 PRML 読書会において、どこを読んで、どこを飛ばしたか、そしてその理由をつらつら書いてみよう。
実際にはもっと細かく「この節は読まなくていい」「この数式は示すの面倒だし、あまり本質的でもないので、書いてあることを信じてもらえば十分」とかいう調子でやったのだが、そこまで事細かくするのはさすがに汎用的ではないので、ざっくりした単位で記す。
1章:いきなりだがほとんど飛ばした。PRML を始めるというタイミングで読む意味があるのは 1.2 から 1.2.3 までだけ。できれば 1.2.3 はわかっている人による補足を聞ければ嬉しい。1.4章 から 1.6章は重要ではあるが、まだ何のモデルも勉強してない状態ではちんぷんかんぷんだろう。KL ダイバージェンスもちゃんと使うのは9章から。これらは4章より後に読む方がいい。
2章:当然もちろん読んだ。特に重要なのが 2.3章ガウス分布。ここの計算は大変かもしれないが、さぼらずやり抜いておきたい。(共役)事前分布についての説明が不足しており(ベイズの本なのに!)、初読の際は正直全くわからなかった。読書会であれば、わかっている人によるフォローが是非欲しいところ。2.4 章以降は目を通すくらいで済ませたが、指数型分布族はちゃんと読む機会を設けたほうが良かったかも。
3章&4章:読んだ。機械学習では、線形回帰やロジスティック回帰のような基礎的なモデルは、要不要に関わらず一通り押さえておくべきと考えている。4.4 章からのベイズロジスティック回帰をラプラス近似で解く話は少々難易度が高いのと、実際の問題でベイズロジスティック回帰が出てくるとしたら「ロジスティック回帰をサンプラーで解きたいからベイズ化した」というパターンがきっと多そうで、ってことはラプラス近似とか使わないから、余力次第で飛ばしてもいいかも。
5章:サイボウズ・ラボの読書会では飛ばしたが、目的に適えば読むのも良いだろう。ヘッセ行列についての説明がこの5章にしかないので、本当はそこだけ読みたかったのだけど、周りと密結合していて取り出して読むのが難しく、断念。ディープラーニングとか自分で実装したかったらこの章に書いてあることくらいわからないと厳しいだろう。ただしニューラルネットワークの専門の本を探して読むほうが適している気もする。
6章&7章:飛ばした。6章7章は残念ながら必要なことが網羅されていないので、カーネル法をやりたければ 「カーネル多変量解析」または「カーネル法入門」を、SVM をやりたければ「サポートベクターマシン入門」(通称赤本)を読むべき。7.2 章の RVM は、まあ、多分いらないので気にしなくていい*3。
8章:読んだ。読むべき。PRML を読む値打ちの半分はグラフィカルモデルにあるので読まないという選択肢がない。あ、そうそう。下巻 p89 の「 head-to-head で観測時に独立性が失われる例」は間違ってはいないが、あまりに非現実的で例の意味をなしていないので、PRML 8.2章「head-to-head が観測されたら独立性が失われる」のもっとわかりやすい具体例 - Mi manca qualche giovedi`? を見てね。
9章&10章:一通り読んだ。PRML で一番計算が大変な章だが、これをこなせれば多くの機械学習論文の計算についていける。EM と VB というメジャーな推論手法を押さえておくという意味でも読む値打ちはあると思う。
11章:読んだ。サンプリング法が要らないなら読まなくてもいいかもしれない。ただ、PRML という確率でベイズな本をわざわざ読んでおきながら、高次元空間上での積分や推論に役立つサンプリング法を避ける理由がちょっと思いあたらない。11.5 のハイブリッドモンテカルロは飛ばしてもいいかもしれない。温度に関する言及が全く無かったり、全般に熱統計力学周りは怪しい(らしい)ので、本格的に必要なら他のちゃんとした本、例えば「マルコフ連鎖モンテカルロ法とその周辺」などを読んだほうがいい。
12章:飛ばしたが、目的に応じて判断。サイボウズ・ラボでは、機械学習の応用先として言語処理が第一にあったので、PCA は特には不要だろうと。(追記) SVD, LSI, pLSI は別途読んだ。
13章:読んだ。言語処理での応用目的なら隠れマルコフはやはり外せないだろう。言語処理でなくても、マルコフ性は多くのモデルで用いられる重要な仮定なので、読んでおいて損はないと思う。13.3 の線形動的システムは言語処理でほとんど用いられないのでざっくり紹介にとどめたが、分野によっては超重要だろう。が、状態空間モデルの文脈で語っているために、記法や導入がメジャーなものとは異なっているらしいので、要注意。
14章:アンサンブル学習と決定木についてのアラカルト。どれも一通り押さえておきたいトピックスではあるが、通り一遍のことしか書かれていないので、さっと目を通す程度で読んだ。つもりだったが、この前「えー読書会では14章読んでないですよ〜」と言われた。あれ〜?
繰り返すが、「独断と偏見」成分も多いので鵜呑みにしないように。
読書会ならば、PRML をすでに読んだ人に参加者とその目的にあわせて読む範囲を相談できれば一番いいだろう。
関連記事
PRML 復習レーンが始まるよ、だって。 - Mi manca qualche giovedi`?
PRML 読んでやってみた(上巻編) - Mi manca qualche giovedi`?
PRML 読んでやってみた(下巻編) - Mi manca qualche giovedi`?
ノートの作り方(私家版) - Mi manca qualche giovedi`?
ツイートするシェア
Permalink | コメント(0) | トラックバック(1)   
Mi manca qualche giovedi`?
