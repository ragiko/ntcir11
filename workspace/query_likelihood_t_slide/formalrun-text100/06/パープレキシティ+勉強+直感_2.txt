ノンパラベイズを勉強してみる (5) ベイズ階層言語モデルによる教師なし形態素解析
ノンパラベイズを勉強してみる (4) 階層Pitman-Yor過程 - Yoh Okunoの日記の続きです。NL190の論文を読んでいきたいと思います。これまでに階層Pitman-Yor過程によりNグラムのスムージングを高精度に行えることを見てきました。持橋さんはさらに階層Pitman-Yor過程を可変長n-gramに拡張した言語モデルを提案されており、これまでスムージングを用いても最大のNは固定されていたN-gramモデルに対し、無限にNを増やせるよう拡張することで、データに応じて最適なNを自動的に推定する方法を与えています。
件の論文では教師なし形態素解析(単語分割)に言語モデルを応用しています。これができるようになると、人手でテキストコーパスを分割する必要がなくなったり、半教師あり単語分割の精度が上がったりします。検索エンジンやかな漢字変換エンジンなど、実用的な成果に応用できそうな重要な分野です。
論文ではまず階層Pitman-Yor過程を文字単位のN-gramと単語単位のN-gramというメタレベルの階層構造を持つNested Pitman-Yor Language Model(NPYLM)に拡張し、これを用いて単語分割に応用しています。元となるアイデアは、HDPを用いた教師なし単語分割のGoldwaterらによる論文があります。Goldwaterらの論文では文字単位のN-gramモデルは使っておらず、HDPをそのまま言語モデルとして使って単語分割をしています。しかし、文字N-gramを単語unigramの事前分布として使うことで、さらに高精度になるということのようです。文字unigramのさらに事前分布には、文字の種類数分の一様分布が使われます。ということは、このままでは未知の文字には対応できないということですね。
確率モデルを使って単語分割するには言語モデルとしての性能を最大化(=パープレキシティを最小化)するように学習すればいいとのことで、そんな単純でいいのかなーという気がしましたが、大丈夫なようです。そもそも単語Nグラムというのは細かく切りすぎても長くとりすぎても確率(尤度)が小さくなるというところがあって、これはかな漢字変換でも観察された現象なのですが、理由はよく分かっていませんでした。
ただし、実際にやってみると少し単語長が短くなりすぎるそうで、その分だけ単語長が長くなるように、ポアソン分布で補正を入れているようです。論文には次のように書かれていて、階層ベイズ言語モデルとして自然なモデルをちょっと崩さなくてはならなかったところに、悔しさを感じているように読めるのが、失礼ながら面白かったです。ちなみに、単語長の分布がポアソン分布になっているという仮説は、かな漢字変換エンジンのAnthyでも使われているそうです。
このモデルはベイズ的な階層nグラムモデルとして自然なものであるが, 実際には式(5) だけでは, カタカナ語など, 綴りの長い単語の確率が小さくなりすぎるという問題が生じる[3]. 単語長は大まかにポアソン分布に従うから, これを補正するために, (5) 式を式(11)(12)と変形する.
単語分割にNPYLMを使う場合は、まず文字の間に{0,1}の単語境界を表す確率変数を導入して、それを変化させることで文字Nグラムと単語Nグラムを同時に最大化するという、発表でおっしゃっていた「神業のようなこと」をする必要があります。直感的には文字Nグラムのパラメータは単語境界に依存せずに事前に決まりそうな気もしますが、やはり単語unigramが決まらないとそのスムージングをどうやっていいかも決められない、ということでしょうか。ともかくその「神業」を、Gibss Samplingと動的計画法を駆使して高速に行うアルゴリズムを提案されていて、従来法が10時間かかるところを17分でやってしまうとのこと。モデルがかなり複雑になっているのにこれはすごい。
いろいろな要素を含んだ論文ですが、個人的には「言語モデルの性能最大化で、教師なし単語分割ができる」という基本の部分が目からうろこでした。なんとなく文字レベルまで分割したほうが言語モデルとしてはよさそうな気がしていたのですが、そうではないということが分かって、なんで今まで気づかなかったんだろう、というような感想を持ちました。
ただ、発表のときに質問した「コーパスを増やしたら(境界を)切りすぎるということはないのでしょうか?」という質問もあながち間違いではなかったようで、長めの単語を優先するような補正をしているということは、人間の直感に沿う単語の単位が言語モデルの最適解とは異なっていたということのようです。実験ではHDPとNPYLMを比べてF値やパープレキシティが改善したとありますが、性能向上の理由は2-gramを3-gramにしたことや文字Nグラムを組み込んだこと、PYLMをベースにしたことなどいろいろあるので、どれが一番効いているのかが気になるところです(直感的には3-gramにしたことが大きそうです)。自分の論文も論点が多すぎるところがあるので、人のことを言えないんですけどね。
ベイズ階層言語モデルによる教師なし形態素解析 | slide
Contextual Dependencies in Unsupervised Word Segmentation
ツイートするシェア
Permalink | 21:16
ノンパラベイズを勉強してみる (5) ベイズ階層言語モデルによる教師なし形態素解析 - Yoh Okunoの日記
