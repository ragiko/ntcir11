はいそれではワールドワイドウェブを利用した言語モデルの教師なし反復適応法と題しまして東北大学のが発表いたします
まず研究背景です
現在会議録の作成であったり
すね
難聴者のための字幕作成など音声の書き起こし需要てのは様々に存在しておりますが
しかし人手による音声書き起こし作業は非常に高コストである
ということが知られております
このような背景から大語彙連続音声認識の研究が盛んに行われております
この大語彙連続音声認識における有効な言語モデル適応手法として次のような
手法が考えられております
その手法とはですね
認識対象に関連したテキスト
を用いて適応言語モデルを作成して
認識に用いる
という手法です
この手法はですね非常に高い適応効果が得られることは周知の事実だと思うんですけれども
この適応テキストをどこから収集するのかというのは問題となっておりました
そこでですねワールドワイドウェブには強力な検索エンジンＹａｈｏｏやｇｏｏｇｌｅなどといったものですね
それと非常に膨大なテキスト
が存在しておりますので
このテキスト収集源としては最適である
と言えます
このような背景からですね我々はワールドワイドウェブを利用した言語モデルの適応手法の開発にあたります
それではこのワールドワイドウェブを利用した言語モデルの適応法について説明します
ウェブを利用した言語モデルの
教師なし適応法としてはですねこのような適応処理を
取るのが一般的です
その処理というのはですね一度音声を仮認識いたしまして
得られた認識結果から検索クエリを
抽出構成して
この検索クエリを
ウェブ検索エンジンに入力
しヒットしたページをダウンロード
してダウンロードしたテキストと
ベースのテキストから適応言語モデルを構成し
認識対象音声を
再認識するというものです
我々はですね
さらにこの適応を高精度化するために適応効果を最も大きく左右すると考えられるこの
ウェブページを収集する部分について深く検討してきました
それではまず従来のウェブページ収集法についてお話しいたします
従来はですね頻度やＴＦＩＤＦといった尺度が大きい
複数の名詞を検索クエリとしてＡＮＤ検索しウェブページを収集する
というような方法が取っておられました
そこで我々もこの従来法に基づいて次のような方法を実践しました
まずですね
仮の認識結果内の名詞のＴＦＩＤＦ値を計算しそれを
降順にソートして次のような単語リストを作成します
次にですね
このリストの上位Ｎ単語をグループとしてＡＮＤ検索を与える
という手法です
この従来の手法を実践した結果次のような長所と短所が見られました
まず長所はですねＴＦＩＤＦで作られたリストの上位に正解単語のみが存在する場合はですね
例えばヨーロッパとアメリカ絵画という音声ではそのような傾向だったんですけれども
そのような場合はこのように
大きな適応効果が
見られるというような結果でした
その反面ですねリストの上位に誤認識単語が混入する場合
文字の歴史という音声ではそのような場合だったんですけども二位と三位にこのように誤認識単語が混入して
これで検索クエリを構成した場合はこのように適応効果があまり見られない
というような結果でした
このような傾向からですね従来法の問題点は質の低い検索クエリ誤認識が混入する誤認識単語が混入するようなものですね
こちらでウェブページを収集する可能性がある
というところが問題だと言えます
そこで我々は次のようなウェブページ収集法を提案いたしました
その手法はですね
まず仮認識結果内の名詞のＴＦＩＤＦ値を元にリストを単語のリストを作るここまでは一緒です
次にですねこのリスト内の単語間の類似度を元に
複数の検索クエリを構成します
似ている単語をグループとして複数の検索クエリを作る
という
手法ですね
次にこの複数の検索クエリそれぞれで少量だけウェブからテキストをダウンロードしてき
そうするとですねこの集まった少量テキストから次は各検索クエリがどの程度有効だったか
というクエリの有効度というものを計算します
最後にですねこの有効度を元に各検索クエリを重みづけでテキストをダウンロードする
という手法です
こちらのステップの二番ではですね正解誤認識単語を分類して検索クエリを構成するという効果があります
こちらのステップの三では検索クエリの質を評価して効率的にページ収集を行う
という効果があります
それではですねこのステップの二
と三について詳細に説明していきたいと思います
まずステップ二ですね単語間の類似度を用いた検索クエリの構成法
その処理の手順を説明します
まず一番です
仮の認識結果におけるＴＦＩＤＦが大きい上位Ｋ個の名詞を抽出してその単語間の類似度を測定します
単語間の類似度はこの式によって与えられます
二つの単語の文書共起頻度が大きいほど丸い
でこのように例えば上位五個を抽出した五単語間の類似度をそれぞれ求めます
次にですねこの類似度を元にこの五単語をクラスタリングいたします
一つのクラスターになるまでクラスタリングいたします
最後に閾値以上のウェブ検索ヒット数
を持つクラスターを検索クエリとするという手法です
それでは次にステップ三について説明いたします
ステップ三ですね
クエリ有効度の求め方なんですが
認識対象と関連性が高いテキストほどそのテキストは有効であると言えます
このようなことからですね
各検索クエリで収集した少量のテキストこちらですね少量のテキストと
仮の認識結果のコサイン類似度の値
をクエリの有効度とする
とします
それではその詳細な計算方法について説明いたします
検索クエリのiによって集まった少量テキストＫ個のページ区分はこのようにあったとします
それとですね仮認識結果の文書ベクトル間のコサイン類似度
をそれぞれ計算します
計算式はこちらですね
名詞のＴＦＩＤＦ値を要素としてコサイン類似度を計算するというもの
そうですね
この総和の値を検索クエリの有効度とする
という手法です
その総和値を計算する際にですねある閾値以下
の文書はですね除くという作業を取っております
これはですね
閾値以下の文章の積み重ねが誤差となって悪影響を与える
と考えられるのでこのような閾値処理の作業を預かっております
それではですね提案した手法従来の手法を比較する音声認識実験を行いました
認識エンジンはＪｕｌｉｕｓです
ベースのテキストはＣＳＪより三千百二十八講演形態素は約七Mです
検索エンジンはＹａｈｏｏを用いました
で比較した実験条件は三つです
まず従来の手法
検索クエリをＴＦＩＤＦの大きい上位二単語
としてＡＮＤ検索する手法ですね
こちらの二という数字はですね一から四で実験した際の一番良かった値が二だったんで
このような条件としております
次に提案法一番ですねこちらはＴＦＩＤＦの大きい上位十単語
を単語間の類似度で検索クエリを複数構成した場合
提案法の一番では有効度による収集の効率化を行っておりません
次に最後の条件提案法の二番ですねこちらは提案した手法を全て使った場合ですね
単語間の類似によって検索クエリを構成し有効度による収集効率化はありとした場合ですね
この場合有効度は一検索クエリ辺り百ページで計算しております
ウェブ収集ページ数はですね一つの音声辺り二千ページとしております
テストセットはＣＳＪのあなたの興味関心のあることの客観的な説明
という模擬講演群から話題の異なる十講演の音声
を用いております
実験した結果はこのようになりました
このグラフは適応前従来法提案法一二の
テストセット平均の単語正解精度を示しております
結果からですね提案した手法を全て使った場合の提案法二番が
一番大きな単語正解精度で平均五十八．四八％でした
この値はですね適応前に比べて三．二ポイント
改善される
しているというもの
この従来法と比較しても〇．八ポイント向上する
というような結果が見られました
続いてですね
構成最多検索クエリと収集ウェブページ数について見てみました
こちらはですね
テストセットの認識対象をですねタイトルヨーロッパとアメリカ絵画
という音声のですね構成された検索クエリと
収集ウェブページ数を表しております
ですねこの音声は適応前のワードアキュラシーは六十五．九三％でした
従来法で適応した場合はですね検索クエリが絵画世紀
という単語となりこちらで二千ページ集めた結果
ワードアキュラシーが七十四．六六
％まで上がるというものでした
提案法ではですね次のような五つの検索クエリが構成されました
でクエリの有効度はこのような値になって収集したウェブページ数はこのように対応付けられました
でですね注目していただきたいのはこの上位の二つの検索クエリなんですけどもこのタイトルと非常に
関連性が高い検索クエリとなっております
そして収集したウェブページ数も非常に大きなものが割り当てられておりまして
効果的なウェブページ収集が行われ結果的に高い
単語正解精度の改善が見られる
というようなものでした
このような警告はですね提案法により効果的なウェブページ収集ができていると言えそうです
続いてはタイトルにもあるように反復適応についてお話しいたし
ます
今まではですね認識対象音声を一度入力して適応して結果を出す
というような処理だったんですけれども今回はですね
この得られた認識結果を再利用して適応を繰り返す
するとどうなるかということについて検討してきました
それではですね提案法した
ウェブページ収集法と従来法を比較する反復適応実験を行いました
反復適応回数はですね一回から六回まで実験しましたそれぞれ
収集したウェブページ数は
一回の適応辺り二千ページとしております
その他の条件は先ほどのものと同じです
実験結果はですねこのようになりました
このグラフは横軸が反復適応回数
縦軸がテストセット平均の単語正解精度を示しております
結果からですね
複数回適応することによりこのように
単語正解精度が徐々に上昇していくという事が見られました
またですね提案法が従来法を上回っていうことも確認できました
結論としてですね反復適応を行うことによって
ですね
提案法ではこの五回の位置が最大で適応前のこのポイントと比べて四．八ポイントの単語正解精度の改善
従来法の最大値
こちらですねこちらと比較しても〇．六ポイント向上している
というようなものでした
それではですね平均ではなくて各認識対象毎に単語正解精度の推移はどのようになっているか
そういうことについて検討してみました
それはこちらなんですけども文字の歴史という音声と僕がリストラされた理由という音声
の単語正解精度の推移
を示しております
こちらのねご覧になったら分かるようにですねそれぞれピークの位置が異なっているという事が確認できると思います
このことから認識対象毎に反復適応回数
を決定する何らかの方法が必要であると言えます
そこでこちらのグラフに注目していただきたいんですけれども
こちらはですね
横軸が反復適応回数縦軸が
認識尤度を示しております
認識尤度は
Ｊｕｌｉｕｓのデフォルトのもの
で計算されたものですね
こちらをご覧になったら分かるようにですね実際の単語正解精度の推移と
ほぼ同じものを取っていうということが確認できると思います
またピークの位置も同じですね
このような傾向を利用いたしまして
反復適応認識尤度の最初に見られるピークで覚えるというような方法を実践してみました
それでは反復適応実験2番ですね
今度は従来法提案法で比較
で反復適応回数は認識尤度の最初のピークで終える
というような実験を行いました
結果はこのようになりました
そうですまずこちら側の表から見ていただきたいんですけども
全認識対象
で反復適応回数同じとした場合は先ほどの言ったようにですね五回がピークとなりました
認識尤度で決定した場合はですね平均三.七回でした
このことからですね反復適応回数を認識尤度で決定すれば小さく
済ませることができる
という事が分かりました
またですね実際の単語正解精度もこちら提案法の二つ分比較なんですけども
認識尤度で決定した場合が一番大きくなって六十．三八％とですね
全認識対象固定で決定する場合に比べて〇．二ポイント改善する
というような結果が見られました
そうですね認識尤度で反復適応回数を決定すれば適応効果大きく
反復回数を小さく済ませることができるという事が分かりました
最終的にですね
適応前よりも五．一ポイントこちらですね適応前に比べて五．一ポイントの単語正解精度の改善こちらが
値はですね
従来の手法と比べても従来のウェブページ収集法と比較しても一．一ポイント
向上するというような結果でした
それではですね実際に最適な反復適応回数と比較した場合どのようになっているかということについて見てみました
こちらの表なんですがこちらはですね各認識対象の認識尤度で決定した場合の反復適応回数
こちらは+一となっており
おりますのですねピークを見つけるたびに+一回
行っていくというものです
こちらの右側の表はですね事後的に
結果を見て最適な反復回数を決定した場合
この表から分かるようにですねこの十個の講演中七個の講演でピークをずばり当てているという事が
分かりました
またですねテスト
平均の単語正解精度を見てもですね
事後的に最適な値を取った場合とそこまで大きな差が見られないというような結果でした
〇．二ポイント程度ですか差は
それではまとめです
ワールドワイドウェブを利用した言語モデル教師なし適応手法について検討いたしました
ウェブページの収集法においては従来と異なり単語間類似度による検索クエリの構成法を提案し
また有効クエリ有効度を用いた効率的なウェブページ収集法を提案いたしました
また反復適応においては認識尤度によって反
復適応を決定するという方法を実践しました
最終的にですね単語正解精度において通常の認識
手法と比較しても
五．一ポイント比較して単語正解精度で五．一ポイントの改善
従来の適応法と比較しても一．一ポイント向上する
というような結果が見られました
これで発表を終わります
