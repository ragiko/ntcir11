はい龍谷大学のです表記のタイトルで発表します
初めにですが研究の背景ですが皆さんここに来られてる皆さんずっと仰られてると思うんですがマルチメディアコンテンツ音声ドキュメントと呼んでもいいですが
そういうものをアーカイブして
それを検索しようと
いうような
要求が非常に高まっています
そこでですね話し言葉の
音声認識というものは
必要かつ重要な技術であるということで我々は研究をしてる訳でして
実際にはですね話し言葉
講演とか会議とかですね
そういうものの音声の認識の認識精度っていうのは大体
六割ぐらいから
八十五％ＣＳＪとかではこれぐらいになってる訳ですがこのような認識精度になっている訳です
それでですねどういう用途を使
うかということを考えてみますと
インデキシングとかですね我々もやってたんですが重要文へのインデキシングとか
情報検索こういうことをやる場合にはですね
必ずしも全てを書き起こす必要がありません
キーワード
等
とかですね重要な単語っていうのが認識できできさえすれば
現状の認識精度でもある程度こういうものは利用可能だというような状態になってると思います
一方ですね放送ニュースの字幕付与とかですね会議録の作成こういったものを考えますと
特に放送ニュースの字幕付与は
昨日の話にもありましたように日本では
かなりの精度が求められるというような話もありますし
会議録の作成
とかでもですね
いったんですね
後ほど後で人が修正する訳ですから
必ずいったん全てをテキスト化しておいて
そういう必要が要求がある訳です
で音声認識の高精度化っていうのが必要になってくると
いうような
ことが
いえ
でですね放送ニュースとかですね会議録今国際化の時代でして
でそういうところ考えますとマルチチャンネルの音声が利用可能なケースが多いと
いうことです
で具体的にいいますとニュースの場合ですと他国語で放送する
放送があったりですね
あとは国際会議とかですと同時通訳ブースなどがありましてそこで同時通訳の音声が
流れてくるというような状況が仮定できる
結構あるというような状態です
ででして
そういう場合はですね
あのチャンネルが複数ありまして
その複数のチャンネルに
異なる言語でおんなじ内容が入ってくると
そういう状況が
仮定できる訳ですね
で従来このようなマルチチャンネルの入力を仮定したような研究っていうのは今日も発表にもございましたけれども
基本的には同一の音声っていうのを仮定してまして何をやるのが目的かというと雑音除去とか
方位推定とかするのが目的
だったりはする訳なんですが本研究ではそうではなくって
全てのチャンネルに入ってくるのは複数の異なる言語
ですが同じ内容の発話
というような状況を仮定する
でこの認識をやってあげようということ
を考えてます
で各チャンネルの音声の認識を情報を補いながら同時に
実行するということをやりたい
ということを考えました
それでですね具体的には機械翻訳翻訳モデルなんですが統計的なモデルを使って音声認識をやってやろうということになります
でユーロスピーチの二千五年のところでも似たような発表があった訳なんですが
これは
両方が音声であったりする場合ではなくって片一方がテキストでというよう状態
を仮定しているようなものも
含まれてます両方が音声の場合もありますが
それは両方ヨーロッパ系の言語でして語順も比較的似てますし近い
ですが日本語英語というようなこういう機械翻訳が困難であるようなタスク
をですね対象とした研究っていうのがまだやられてないということでやってみました
という話です
でちょっと皆さん当たり前すぎて
なんですがちょっと付き合ってください
で初めに統計的
音声認識の話をしますと
音声認識っていうのは音声が与えられた時にそれを最もよく説明する
単語列を求める式で書くとこういう
プロセスになる訳ですが
展開していって結局はこのこういうような
式を最大化するような
単語列を求めるということになる訳なんですがでここで
ＰのＷというのを与えるものが言語モデルと
でＰのＸバーＷですねでこれを与えるものが
音響モデルと呼ばれます
この辺は当たり前なんで
軽く流しまして
で統計的機械翻訳の方の話なんですが
これも同じような考えでいけまして
ある言語原言語なんですがその単語列
が与えられた時に
それを最もよく説明する
別の言語の
目的言語なんですがその単語列を
あ
それを最もよく説明する別の言語の単語列を求めるプロセスと
して定式化されます
で式で書きますとこうなりましてでこのモデルを翻訳モデルと
呼んでいる
でこのモデルこの確率を与えるモデルを翻訳モデルと呼んでましてこれを翻訳スコアとか
単語の対応度ですね文の対応度とか呼びます
でこれを使った音声認識についてちょっと定式化をしてみます
でここでは複数言語といいましたが二カ国語で考えてみます
で日英の同時認識でして日本語の場合の認識
の式を書いてみます
でこの場合はですね日本語の音声Ｘと
英語の音声Ｙが与えられた時に
それを最もよく説明する日本語の文字列を求める問題として定式化できまして
で色々
変形していくんですが
この辺は
ちょっと
はしょりまして
ここで英語の可能な単語列っていうのを
ちょっと
導入しまして
さらにちょっと整理してみますと
こんな感じの式になります
これよく見ますとここが
日本語の音声認識のスコアなってまして
ここが英語の音響モデルのスコアですね
でこれが翻訳スコアなんですがちょっと見通し悪いのでもう少し
もう少し変形します
ちょっと
なんですが対数とってみまして
重み入れてみますとこんな式で
なります
でここを
ベイズの
定理で展開してみますと
こんな感じなりまして
でここにＰのＪとここにＰのＪありますので
ここの重みを
ａ−ｂをαと置いて
ｂをγと置いて整理しますと
結局こんな感じの式になる訳で
これみますと
この部分が日本語音声認識スコアなってまして
対数取ってるんですが
でこの部分が英語の音声認識のスコアになってまして
この部分が翻訳モデルのスコアになってると
そういう状態になります
従って音声認識日本語の音声認識を行う時に日本語の音声認識のスコアと
英語の
中間
構造みたいなものと
翻訳モデルのスコアがあれば
音声認識が行えると
いうことになります
でちょっと図で描く描いてみますと
日本語の音声求める場合は
日本語の音声認識の
システムと英語の音声認識のシステムを用意しまして
中間表現ですねＮベストリストだ
ったり
単語グラフなんかを出してみて
それからもうひとつ翻訳モデルっていうのを用意しまして
でここで
リスコアリングをしてやるこの式に基づいてリスコアリングしてやるというようなことに
なります
でこれが一応定式化した
ものですでこの枠組みが正しいかどうかっていうのをちょっと予備実験をして
評価をしてみました
具体的には
英語の方の音声認識は行わずにつまり英語の方はテキスト与えて
音声認識が百％出来た状態を仮定して
日本語の音声認識を行うということをしてしました
で
図で見ますと先ほどと同じ図なんですが
ここのところが
英語のテキストＥになりまして
で式はこの式になります
でこの式
もう一回見てみますと
日本語の音声
と
英語のテキストが与えられた時にそれらを最もよく説明する
日本語文字列Ｊを
求めるプロセスで
変形するとこうなります
でこれを
英語の音声を与えた場合の式と比較してみますと
ここ見ますとここで
このＥのｍっていうのが英語の
可能な文字列全てなんですが
これが一通りの場合とですね
等価になるということがわかります
という訳で枠組みとしては同一ですので
こちらを使って
評価を
この枠組みが正しいかどうかっていうのを
検証してみたと
そういうことになります
すなわち日本語の音声認識のスコアと
翻訳モデルのスコアでリスコアリングしたと
そういうことになります
で次にあのこの馴染みがあんまりないと思われるこの
翻訳モデルの方について説明します
で翻訳モデルですがこれは異なる言語の文字列
ここではＪとＥと書いてますがその対応スコア
翻訳スコアとも呼んでもいいですがそれを
与えるモデル
です
でここで問題になるのは
例がありますように
語順がそもそも異なることがあると
ほとんど異なるでしょうそれから
言語間での単語の対応がですね必ずしも一対一ではないという問題があると
いうことになる
それから
正しい対応付けこれ今人間が与えてる訳で
正しい対応付けなってる訳なんですが
それがコンピュータ的には何が
正しいか自明ではないと
いうことで
もう確率モデルとして全て
の可能性を考えてですねそれを
全部足してやるというようなことをします
でこの一つの対応関係をアライメントと呼びまして
それを Ａとして表しますとこのような
確率を全てを可能な
アライメントで
サンメンション取ってやれば
いいということになります
で具体的にはＩＢＭｍｏｄｅｌ３というものを利用しまして
採用しましてこの確率を計算します
これについて少し説明します
ＩＢＭ３ｍｏｄｅｌ３では
対応スコアを
四つのモデルでモデル化します
で
一つ目は
ここにありますここここの対応スコアを与える
ファーティリティモデルなんですが
これはある
原言語の
単語がですね目的言語の
Ｎ単語に対応する確率を
与えるモデルです
この場合はｂｏｒｒｏｗが
二単語に
繁殖してますこの
この値を
与えるような
かくり
つモデルです
それからも二つ目のモデルは
ここなんですが
ＮＵＬＬジェネレーションモデルといいまして
これはですね目的言語の
単語例えばはとかをとかに対応する英語というのが英語の単語ありませんので
その場合はＮＵＬＬというものに対応していると見なします
すなわちＮＵＬＬをジェネレーションするモデルが必要でして
その
モデルです
で三つ目は
これは直感的に分かりやすいんですがレキシコンモデルで
ある
言語の単語がも一つの言語の単語に翻訳されると
そういう確率を与えるモデルです
で最後に
語順の入れ替えを
許すようなモデルでして
これはディストーションモデルと呼ばれまして
これは元の
言語のある位置の単語が
目的言語のある位置の単語と対応するという確率を与えるモデル
でＩＢＭｍｏｄｅｌ３では
この
位置を絶対値で与えてるとそういうような状態になり
で評価実験を行いました
で先ほども同じ説明なるんですが
この式に基づいて
つまり
対訳英語テキスト与えて日本語音声認識を行いました
で音声認識でＮベストリストを生成して
翻訳モデル
のスコアを
用いてリスコアリングするということを
やりました
で評価実験のデータなんですが
評価データは
今回は日英対訳ニュース記事
を持ってきましてそれの日本語の
部分を読み上げました
日本語を読み上げたのは日本語母語話者三名
が五十文ずつ百五十文読み上げました
で基本的にニュースを読み上げてますので
読み上げ音声認識システムを使いました
これはＣＳＲＣの最終版に入っているものをそんまま
使いましてＪｕｌｉｕｓは３．４．２
で音響モデルは
ＡＴＲ多数話者モデル
ですね
それから言語モデルは新聞記事から学習した
単語トライグラム
モデルこれを使いました
で翻訳モデルの方は
先ほど説明したＩＢＭｍｏｄｅｌ３を
ＧＩＺＡ＋＋というもので学習しました
で学習データはロイター記事対訳コーパス
でしてこれは五万六千文
日本語が
百
六十七万単語で英語が百三十二万単語ぐらいです
でこれで
先ほどの
ＩＢＭｍｏｄｅｌ３の四つのモデルを
学習しました
で実験なんですが
音響モデルの方についてモノフォンとトライフォン使ってみたんですが
結果は三名ずつなんで
六個あり
で縦軸が認識率でして赤色が
普通に音声認識行った場合です
で青色が翻訳モデルを使った場合です
で見てみますと
翻訳モデルを用いた場合に
認識率が
この辺除いて向上してるということが分かります
で特にこの辺ですね元々認識率が低いようなところで
精度の向上が
見られたというようなことがわかりました
で実際の話し言葉の音声認識っていうのはもっともっともっと認識率低いところに
あるんじゃないかと思われますので
何かこういうことやってやると良くなる可能性があるんじゃないかという感触を得ています
でその次に
今はこの
ここの統合重みのγって係ってるんですがこれを
事後的に良くなるように決定してたんですがこれを振ってみたらどうなるかと
いうのを
やってみました
でこれモノフォンの結果なんですが
〇．五から〇．〇五から一まで
〇．〇五刻みでやってまして
三名ありまして
赤色がベースラインで
青色が
提案手法です
で見てみますと元々認識率が低いようなこの二つ
の結果はだいぶ何やっても良くなるんですが
元々高いようなやつはちょっと
重みが高くなっ
と翻訳モデルの重みを強くすると
逆効果になってるというような効果が結果が見られました
これで何かがゆえるという訳ではたぶんないと思うんですがもうちょっと実験が必要やと思います
それから
トライフォンの方なんですがこれはちょっともっと怪しくて
なんかあんまりどこがいいのかっていうのはよく分からない
で調査した範囲にも最適値っていうのがよく分からないと
いうような
共通するような最適値っていうのはないないなかったんで
もうちょっと頑張って調べないといけない
で
一応調べてみたんですが元々
認識率がトライフォン使った場合は高いので第一候補が
一番いんじゃないかという疑問もあったのでそれ調べてみたんですが
 Ｎベストリストで二十
ベストぐらい出してですね
その中から一番いいものを選ぶということやりますと
こっから五％ぐらい全部上がるということが
分かりましたので
なんかうまいこと選択が
できればまだまだこれは改善する余地が
あるなというところは分かってます
ただちょっと
こうなぜこうなってるのかっていうのは今検討中です
まとめますと国際会議とかニュース
とかでマルチチャンネルですね異なる言語で同じ内容の発話があるような
場合の音声認識について検討しました
で予備実験としまして
日本語音声認識時に英語のほうは認識しなくてテキストと機械翻訳を用いて
実験してみて
そういう枠組みがきちんとうまく動くことを確認しました
で今後の課題は
実際の話し言葉での実験及び評価することと
日本語と英語両方ですね
英語のほうが誤りがなかったので
良くなってる可能性も
十分ありますので
両方音声認識してみる
でそれから
今は
文の
対応っていうのがはっきり付いてるような状態ですし
同時通訳
とかとは少し性質が違いますので一回書き起こしたものですので
同時通訳とかを音声認識してみるというようなことも
必要だろうなということは
考えてます
でそれから最適な統合重みですね先ほどの
振ってみたんですが
あんまり分からないですでこれこれをちょっとどうやって決めるか
というところを
検討していきたい
思っています
発表は以上です
