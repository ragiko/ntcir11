はいそれでは龍谷大学のが発表いたします
タイトルは英日同時通訳音声の音声認識
言うことで始めます
音声ドキュメント検索ということでその中での音声認識技術の話の
発表になります
で今回対象としているのは同時通訳の音声というものの音声認識です
同時通訳音声の認識というのは
どういうことかといいますと
国際会議とかですね
テレビの放送とかですと主音声と副音声みたいな感じである日本語の発話に対して
その対訳となるような英語の発話とか他の外国語の発話というのがあるとそういう状態になると
でそれぞれ
音声認識していままで音声認識して字幕とか作ったりとか議事録作ったりとかそういうことをしていましたと
ですがここで注目したいのが基本的に同時通訳ですので同じ内容の発話がなされているいうことがあると
いうことでそれぞれ音声認識したあとにここで組み合わせてやるともっといい
結果が得られるんじゃないかと
そういう発想で研究をしています
で今回やることは英日同時通訳音声の中の英語と日本語を音声認識するんですがそれぞれ普通に音声認識したあと英語の音声認識と日本語の音声認識
と
組み合わせてよりよい結果を得るということが目的です
でまず
普通に同時通訳の音声認識をやってみるとどれくらいになるのかということをまず調査しましたと
で
使ったデータはＣＩＡＩＲの同時通訳データベース
いうことで
名古屋大学で
配ってあるやつなんですけども
その内の政治の六件と経済の四件の計十件の
音声ですで英語の音声は元があって逆もあるんですけど今回使ったのは英語の音声が元で
でそれに対して日本語の同時通訳が入っていると
いうようなものです
で英語の音声認識は普通の
システムを作りましたと具体的に言うとＩＳＰＣで配っている
音声認識の英語の音声認識キットのやつを使っているんですけども
音響モデルは話者適応を教師なしで三回
やったやつを使ってやりましたと
この辺は普通にやった結果です
横軸がＷＯＲＤＥＲＲＯＲＲＡＴＥで縦軸が実講演それぞれの結果でこっち行くほど悪いんですけどだいたい
これが非常に悪いんですけど他は
二十五％ぐらいのところにあって
英語の音声認識
は
読み上げ音声の認識システムみたいなものでも結構動くということが分かりました
で日本語のほうは
ちょっと普通の日本語とは性質が違うんですが例えばその順送りによる訳出とか短縮による訳出というのがされてましてすこし違う
特徴があるということがわかっていますで順送りによる訳出というのは
ちょっとここ間違っていると思うんですけど
私たちは作文を書きまして
悲惨な戦争の終わりについて書きましたみたいなふうに一回聞きながら訳しているので英語の語順に引っ張られて先に
動詞が出てくるとかそういうふうな訳し方がされることがあります
それから同時通訳だと
喋りに追いつかないので
いらないと思われるところですねこういう多分赤で書いてあるこれが
日本語にはまあ ａ ｌａｒｇｅ ｏｆ ｓｃｈｏｏｌとか ａｔ ｓｃｈｏｏｌとかいうのは出てこない
ということになります
でおんなじ内容の発話とか言っときながらちょっと違うということがあるそれから日本語の発話としては語順が少し
普通とは違うことがある
ということが問題
ですと
でそれを音声認識するということでちゃんと考えないといけないんですけど
同時通訳を書き起こしたやつで言語モデル作るのが多分一番なんだと思うんですけどもそんなデータたくさんないんでとりあえずＣＳＪでやってみて
どれぐらいになるかというのを調べてみましたで音響モデルのほうは教師なしの話者適応三回やって言語モデルは普通のトライグラムを作りました
でやってみると
だいたい三十五％ぐらいのＷＯＲＤＥＲＲＯＲＲＡＴＥになりました
ということです
でＣＳＪの
十講演とかの認識タスクですと話者適応とかするともうちょっと
こっちがわですね二十％とか二十五％ぐらいのところに
来ると思うんですけどもそれにくらべると少し
ＷＯＲＤＥＲＲＯＲＲＡＴＥは高いと
ただし今回の話題が政治経済だったり
すること
もありますし
ちょっと話題がＣＳＪとあってない可能性がありますので
同時通訳だから
難しいというところがほんとに
どれくらい難しいのかというのはちょっとまだきちんと整理はできていないと
とにかくやってみるとこれくらいになりましたと
いうのが
まずはそういう結果ですと
でこっからが
メインテーマでして
多言語の同時音声認識
というのが提案している手法です
でまずデモなんですが
イメージですけど
音出ないですね
英語の音声がなってから英語の音声認識始めて日本語の音声を認識し始めてでそのあと日本語の音声認識するときに英語の情報使って直しているという感じなんですけど
見ていただくとここらへんの掲載とかいうのがこのへんのエコノミーから
多分経済の方の間違いだということで直ってたりこうこうこうとかが
雇用とかに戻ってたりするんですね
雇用はここ自体には出てないんですけどまあエコノミーとかと共起確率が高くってででてくるというような感じになります
こういうイメージで
やっていますと
でその概要をもう少しちゃんと説明しますと日本語の発話と英語の発話が
おんなじ内容が
喋られていうというのがわかっているという前提なんですが
それがあ
ってで音声認識をして
で
ここでですね中間結果をもってきてでこっちの英語の音声認識の結果を持ってくると
で英語と日本語の
対応度を表すような翻訳スコアというのを与えて
でもう一回この音声認識結果を並び替えて出すというようなことが
行われます
でちょっとすいませんこんなしょうもない
あれなんですけど
音声認識をやってで友達が車で待っているという発話に対して幾つか候補を出します
で音声認識のスコアが付くと英語のほうはＭＹＦＲＩＥＮＤＩＳＷＡＩＴＩＮＧＩＮＴＨＥＣＡＲとでてきましてでこれと
これを見て
多分これになるんだろうなということを
実際に式で書くと式で書くとというか
ペアを作って
でこれとこれの対応度を測っていくわけですね
でＣＡＲと車ではあんまり対応しなくってこの英文とこの英文は対訳になっていない可能性が高くってスコアが低いとここはスコアが高くなってここは適当なスコアが付いて
で元の音声認識のスコアと加えて
一番高くなるのがこれでで結局出てくるのはこれになりますと
いうような仕組みになっています
で定式化しますと日本語の音声Ｘと英語の音声Ｙが与えられたときに日本語の最も良くなる文字列求める問題として定式化できましてでこれをいろいろ展開したりすると展開してＬＯＧとかとったりすると結局こういう形になるんですねでここはこの上の部分に書いてあるのは結局日本語の音声認識のスコアに英語と日本語の
英語の音声認識結果とその日本語の今の仮説との翻訳スコア
というものを足してだからもともとはこれだけで音声認識しているんですけどもそれに別の情報も一個たしてスコアすると
いうような形になりますと
でさっきの翻訳モデルのスコアを与えるモデルとしては統計的機械翻訳のモデルを与えればいいということになりますのでそれを学習しましたとでそれはＩＢＭＭｏｄｅｌ３というモデルを学習しましてだいたい五百万単語ぐらいの
文対としては百八十Ｋぐらいの文対から学習して翻訳モデルを学習しましたと
で評価データはさっき通りになります
でポイントはここなんですけども日本語の発話に対して英語の
同じ内容の発話がわかっていないと全然ダメになります
ここがきちっととれてないと
英語の情報を使うだけ無駄でぜんぜん違うものが出てくるということになります
のでまずここを
とりあえず
手動でまず対応付けてみて正しいものを与えるとどうなるかという実験を最初にしました
そのあとここの
対訳を自動で
推定してで使ってみるという
ことをやりましたということをやります
で手動でまず
対応付けるんですが
これ音声認識の結果これ二講演だけでやっているんですけども横軸がＷＯＲＤＥＲＲＯＲＲＡＴＥで
講演が一つ目二つ目です
で一つ目が青色が日本語だけの音声認識結果そして赤色は
それに英語の情報を加えて
認識をしたものになりますと
で見てみますとどちらも英語の情報を使ったほうがＷＯＲＤＥＲＲＯＲＲＡＴＥが下がっているという状況になりますと
で英語のほうも音声認識結果を使ってまして一つ目の講演に対して
については五十五％ぐらいのＷＯＲＤＥＲＲＯＲＲＡＴＥなんですが
それでも使ったやったのほうが
幾つかの正解の単語が入っていると思われるので
良くなると
いうことやと思います
で今度は対応を自動で推定
してやりましたと
で対応付けは
今回は時間情報だけに基づいてやりましたと
で実際には
同じ内容の発話なんですけど文単位とか
話し言葉なんで
何が文単位かわからないんでとりあえず無音区間で区切って
で日本語の発話に対して英語を幾つか
対応付けるということをやりました
結構荒っぽい処理をやりました
でこういう時間があって
で英日同時通訳ですので英語があってその後日本語が話されるというような状況ですでこれとこれと対応付けていくというような形なんですが
そのアルゴリズムとしましては
まず日本語発話がされる前に必ず英語の発話が
あるだろうということで
直前の発話に対応付けるということをしますと
それからこういう短い発話があるんですけどもその短い発話は
短い発話は
なんかフィラーだけだったり
そういうことが多いですのでこの対応付けはやめますと
で
ここが
どこに対応付くかというと
ここはこっちに対応付けとくということをします
でまだ対応付いてない文がありますので
それは
このあとの発話に対応付けとくとこういうことをしますと
でこれくらいでやって
二講演で
やってみましたとで一対Ｎの対応付けするんですが
それが完全に一致した部分が
六十％から四十％ぐらい
で正解
の対応人が付けた正解の対応と
自動で対応付けたやつの一部が一致すると
いうような状態まで考えると
全部
が一致してますと
つまりどのデータにも
一部分は
一致する内容が一致する発話が
得られてるということになりました
その自動で推定したデータを用いて
実験しましたところ
さっきの青が音声認識日本語だけで音声認識したやつで
赤が
人手で
この発話に対応する英語がこれですよというのを教えた
結果です
で緑が
自動で推定した結果でして
自動で推定して多少いろんなゴミ含んでるんですけども
しかも音声認識も
エラーも含んでるんですけども
それでも使ってやると少し認識精度の改善が得られる
ということがわかりました
でこれを十件全部で評価したんですけれども
結果から言うと十件中五件で精度が向上して一件では
変わらなくってあと四件では少し悪くなったというような状況になっています
でＮＢＥＳＴリスコアリングで
あの
最終的に翻訳モデルのスコアを使ってＮＢＥＳＴリスコアリングして並び替えてるんですけども
１０ＢＥＳＴの中に一番ええのがあってそれ選ぶと
かなり良くなると
だから赤が上限値みたいなもんなんですが
まだまだ良くなる可能性は
ありますということがわかりましたと
で反対向けにもやってみて
英語の発話を認識するときに日本語
の情報を使うということもやりましたと
でこっちも式書くと一緒になるんですが
ここが
反対向きになりますこっちは英語の音声認識のスコアで
ここが日本語と英語の発話の対応のモデルなんですけどもちょっとごちゃごちゃと式を変形していくと結局これに戻って
翻訳モデルは学習し直さなくてもいいと
英日方向の翻訳モデルに使ってもいいので
ということがわかって
今回は新たに学習
せずにさっき使った
モデルをそのまま使いますと
で発話の対応付けも
さっきとさっきの発話の対応付けを反転させて
対応付けると
いうことをやりましたと
でやりますと英語だけの音声認識した結果が青で
で緑が日本語の情報を使った
状態になります赤は上限値みたいなものです
で見ますとやはり十件中六件ぐらいで
精度向上が見られてて二件で変わらないと
でちょっと悪くなるやつがあるというような状態です
で実際の改善値は最初に見せました
やつと同じなんですけど
ですねエコノミーがあってけいさいが経済に
このが雇用に戻るとか
この文は
元々の文が
食料の不足と言っている
ものなんですが普通に音声認識すると
ここに間違って過不足となるんですけども英語のほうではＳｈｏｒｔａｇｅｓと認識されていまして
で
この過はなくなって不足
というふうに認識されると
こういう結果が得られています
まとめますと
英日同時通訳音声を
音声認識しまして
でちょっと同時通訳の音声認識のところが
ちゃんと工夫はないんですけどＣＳＪつかったりして既存のやつだけでやると
だいたいこれぐらいの精度になってますと
それに対して
自動で
同じ内容の発話がありますのでそれを対応付けて
で日本語の音声認識をするときには英語の情報を
英語の音声認識するときには日本語の情報を
というのを用いてやると
十件中五件とか六件で
精度が向上して
で変化がなかったのが一件か二件ぐらいで
で後の残りの四件とか二件とかでちょっと悪くなってしまいました
そういう結果になりました
で
１０ＢＥＳＴの中にはまだまだ登場する候補はありましたので
もう少し
色々検討する必要があると思うんですけども
先ほど対応付けの話は二件で評価してたんですけども
この二件の以外のやつも多少見てみますと
同時通訳者によっては事前に原稿大分読んでまして
英語が発話される前から日本語を喋り始める例とかも
あるんですねそういう例だとうまく対応がつかないと
いうようなことがありますと
そういうところではあんまり精度がよろしくないということもありますと
ということが分かってきましてもう少しちゃんと対応付けをすることと
やはり
音声認識誤りが
英語にも日本語にもそれぞれ含まれてますのでそういうものにもちょっと
対応していったほうがいいかなと
考えております
発表は以上です
