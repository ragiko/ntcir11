それでは表記の題目で発表させていただきます
まず本研究の
まず本研究の背景についてご説明致します
まず近年のネットワークの大容量化に伴いまして録画録画しておいた講義のビデオを
配信し学習者が
各自自宅で
自習できるようなシステムｅ−ｌｅａｒｎｉｎｇのシステムが普及し始めて来ています
ただし
普通に講義のビデオをそのまま配信するだけでは学習者にとって
使いやすい教材であるということは
言えないと思います
ではどういうものが必要かといいますと
必要かということを考えた時に
まず考えられるのは書き起こしのテキストの提供
つまりこれは話し言葉を書き言葉へ変換して
また文の区切り
句読点で区切るなどそういう処理が必要であると考えます
また講義の要約
要約したテキスト
あるいは音声
要約音声の提供
またはキーワードやトピックの選択による
講義のビデオへの
ランダムなアクセス
として利用し易いインターフェースこういうものが必要であると
いうふうに考えております
例えばこちらが
日立のイージープレゼンテイターというソフトウェアの例なん
けれども
左上の方に収録している収録した講義のビデオ
の映像が流れます
そして左下の方には
スライドのタイトルがチャプター一覧として表示
されます
そしてこちらですねこちらがその時このビデオの
時点で表示されていたスライドの画面が
表示されます
でこれを
今このシステムで使っているんですけれども
起動していれば後ほど
これを再生して学習することができると
いうシステムです
で
この先ほどのイージープレゼンテイターをちょっと改良して
我々の研究室で作成しているのが
音声認識を高度に利用した教材の例
教材ということで
こちらの画面は先ほどと変わらないんですけども
こちらに要約
後ほどお話しますけども自動要約による結果を用いて
要約率を自由に選べてあと話速
再生速度ですけどもそれをコントロールして再生できると
いう機能を設けました
またこちらにキーワードのキーワードからのインデキシング
つまりこのキーワードを一つクリックするとそのキーワードが話されている部分へビデオをジャンプさせることができる
機能も
付けました
でこちらが先ほどと一見同じスライドの画面なん
ですけれども
こちらにもキーワード
ビデオのキーワードが話されている位置へのジャンプ機能を
付けている
そしてこちらの方に要約文の一覧
が表示されています
今現在話されている
文がハイライト表示されると
いう機能を
付けました
でこういう教材を
作るという目標があるんですけれども
そこで必要となる要素技術として
まず講義音声の高精度な高精度な自動認識がまず
必要になってきます
そして
まず最初のトピックとして
高精度な自動認識
のために何が
何を調査する必要があるか
ということで
まずは収録装置デコーダ音響モデル
が違うこと違うとどのように収録
した音声の認識精度が異なってくるか
というその影響の調査を行ないました
また
収録装置いくつかマイク三種類を
変えて三種類で
収録したんですけれども
そのマイク毎に開きがありますのでそれを正規化するにはどうしたら良いかということでケプストラムの正規化法を各種
試して結果を比較しました
また
当研究室で開発しているコンテキスト依存音節
音響モデルと
いうものを使いまして
さらなる高精度化を図りましたその結果について報告します
ではまず収録装置デコーダ音響モデルの比較について
発表致します
まず先ほども小暮先生から発表が
ご紹介いただいたんですけれども
当大学では三種類の収録装置で
講義を録音しています
まずソースＡとして指向性のハンドマイクを有線でＤＡＴ入力
でソースＢとしまして
有線のピンマイク
これもＤＡＴ入力して
でソースＣとしてワイヤレスのピンマイクからの音声を
無線でパソコンに飛ばしましてそこでＷＭＡで圧縮をかけています
そのＷＭＡ圧縮というのは
イージープレゼンテイターの
ソフトウェアでＷＭＶに圧縮されるため
このようになっている
でこのソースＡＢＣについてですけれども
システム一二三
を用意しまして
デコーダ音響モデル
について比較を行ないます
まずシステム一と二を比較することで
デコーダの比較を行ないました
システム一としてはデコーダには当研究室で開発されているＳＰＯＪＵＳ
システム二にはＪｕｌｉｕｓ三．五
使用し
しています
そして音響モデルの条件としてはシステム一二とも同じ
百三十三音節モデル
言語モデルとしてはＣＳＪの二千四年度版から学習した
一万七千語彙の
言語モデルを使用している
システム三としては
またシステム二と三を比較することで
こちらの二つですけれども
音節
百三十三音節モデルとトライフォンモデル
この二つの音響モデルを比較することができます
特徴量
その他の特徴量としては
音声は十六ｋＨｚでサンプリングしておりましてそれを十二次元の
ＭＦＣＣΔＭＦＣＣΔパワーに落としていま
して音響モデルは混合数三十二
で音節
モデルでは
状態数三から五対角共分散行列
この条件はほとんど同じ
では収録者講義音声
について
ご紹介しますけれども現在までに六話者十一講義
主に情報工学分野情報工学分野の講義について収録しました
その中から今回テストセットとして認識のために利用した講義は
このように一二三四四講義となっていますでそれぞれの講義が前半と後半あります
で表記は一番目の講義の前半が一の一後半が一の二とこのように表記
しています
で話者は三人です
講義一と二
それぞれの講義の特徴について少し述べますと講義一と二では
パープレキシティがだいたいこのようになっているんですがこれは後半になると少し
増大しています
これは話者が
後半になるとちょっと疲れてくるのか
と思うんですけれども多分発音のなまけやそういうものが多くなってパープレキシティが増える
という予想が
されます
また講義四に関してはパープレキシティが
全体的に若干高めになっていますが
これはパターン認識の分野で
言語モデルにはＣＳＪを用いているんですけどもパターン認識の分野が入っているために少しそのドメインから外れて
未知語率もちょっと高めになっているためではないかと推測されます
でこのこれらの講義音声に対し認識実験を行ないましたその結果アキュラシーですけどもこちらに示し
まずデコーダの比較としてシステム一と二を比較しました
で
全体的にほとんど差が無いんですけれども
システム一が
認識率で認識精度で若干上回っている講義が
多いということで全体的にはシステム一が高い
ていうことがわかりました
また音響モデルの比較についてですけれども
これも講義によって優劣に差があるんですけれども
全体的に見るとだいたいトライフォンシステム三の方が優れているということがわかる
また収録装置の比較ですけれども
こちらに
オレンジ色のがハンドマイク
で緑色のが有線のピンマイク
紫色のが無線の
ワイヤレスワイヤレスピンマイク
ということになっています
でほぼ全ての講義において
ほぼというか全ての講義において
ハンドマイクの認識率が
他の二つよりも
上回っている
一番高いということが
わかりました
で全ての講義において一番低いのは無線
のピンマイクでＷＭＡ圧縮をした音声であると
いうことが
わかります
また話者毎に認識性能に差が出るのか
という観点から
みますと
今回
テストした
音声の中では話者ＮＫの認識率が一番全体的に
高いという結論になりました
でここで問題点ですが話者や収録装置の違いによって大分
差が出るということがわかりました
でこれは実用上不便で大変不便である
いうことです
でこれを正規化する方法として今回はケプストラムの正規化
各種正規化法
で
各種正規化法を
行い
結果を確認しました
でまず今回実験した正規化手法として平均の正規化ＣＭＮと分散の正規化
またヒストグラムの形を合わせるヒストグラム正規化この三種類を行ないました
ここで
ＣＭＮとＣＶＮについてなんですけどもこの平均と分散をどの区間で求めるかということが
問題になってきます
まず一発話ごと
講演括弧話者というのは
今回は一講演一話者
と考えていますのでこのようになっていますが
まず一発話ごとの特徴ですけれども短い発話に対して対しては推
定精度が低下することが
考えられます
ただリアルタイム性が高いので
音声を入力してすぐ認識結果が欲しいと
いう場合には非常に有効な手法です
で一方講演ごとでは区間が長いのでより正確な推定が可能ではないかと思われます
ただこれでは全体の入力が終わるまでリアルタイム処理が
処理ができないリアルタイム性が無いという
欠点があります
また話者の移動等突発的な変動には追随できない
ただし
オフラインの今回の講義音声は収録しておいて
あとでまとめて認識すると
いう
仕組みなので
それが目的ならば講演ごとにすることで性能向上があるのではないかと考えました
でこの実行単位に関する
調査にはすでに先行研究がありまして
ＮＴＴサイバースペース研究所のさんらの研究
があります
これではこの研究では四種類の実行単位
一発話内の数秒一発話一話者全話者で比較検討を行なっていました
結果としては
分散正規化
が有効で話者ごとの単位が最も効果的であるという
結果でした
今回は
このタスクでは四桁数字認識でしたが今回これを講義音声でやってみようと
思っています
で
平均と分散については以上
の観点で実験を行ないましたが最後にヒストグラム正規化について少し説明しますが
これは適用対象話者の
累積関数の逆関数を使ってヒストグラムを
合わせるという
方法です
でこれにより平均分散のみならず
確率分布の形状を一致させることが
できます
ここで基準話者をどう決めたかという問題ですけれども今回はＣＳＪのテストセット
つまり学習データに含まれない
デー
タ講演のうちそのうちの一人を選択しています
つまり音響モデルの
学習データとテストセット講義音声のデータも全てこの一人
の
ヒストグラムに合わせて
実験を行なってい
そして学習データとしてはＣＳＪ二千四年度版の
男性話者八百十四講演特徴量は先ほどと異なりまして三十八次元を用いました
で音響モデルは百十六音節モデル
デコーダは当研究室のＳＰＯＪＵＳの
バイグラムによる認識で
企画を行ないました
でこちらがその結果になりますが
一発話ごとのＣＭＮをベースラインとしますと
話者ごと
ＣＭＮＳＰＫという記号で赤い
棒で話者赤い棒が話者ごと青い棒が
一発話ごとなんですけれども
全体的に話者ごとでやった方が
認識率が若干
向上すると
最大で一．九七％向上するということが
わかりました
ただし分散ＣＭＮを用いた場合は
なぜか平均の
正規化よりも悪い結果となってしまいました
この原因はまだちょっとよくわかっていないのですが現段階ではこのようになっています
そしてヒストグラムの正規化も行なったのですが
こちらも
ベースラインである一発話ごとのＣＭＮよりも
全体的に悪い結果となっています講義によっては向上している例も
ありますけれども全体的には
悪くなっている
いう結果になってしまいました
ここでまとめますと一発話ごとのＣＭＮと話者ごとのＣＭＮでは
話者ごとのＣＭＮの方が良かった
ということがわかりました
またＣＶＮは一発話ごとのＣＭＮよりも悪くなってしまいました
またヒストグラムの正規化は講義によっては効果がありましたけれども
平均的には一発話ごとのＣＭＮよりも
悪い結果となってしまい
次にコンテキスト依存音節ＨＭＭ
によるリスコアリングというタイトルでちょっと報告します
これは
四状態四混合の音響モデル
なんですけれども
コンテキスト依存というのは直前の音節を考慮して
学習しています
発音と文頭の無音
ショートポーズそして五つの母音
×百十六音節で計九百二十八個のモデルを
作成しました
そして
これをこのモデルを発話ごと
のＣＭＮを用いた
コンテキスト非依存の二百ベスト
先ほど出した実験結果
ですけれども
それに対するリスコアとして使用しどの程度効果があるかということを調査しました
こちらがその結果ですけれども全体的に結果が良くなっていき
で最大では二．０七％くらい向上しました
のでコンテキスト依存音節音響モデルは精度向上に有効であると
いうことがわかります
で
結論ですが
話者ごとのＣＭＮはわずかながら効果がありました
ただし分散を使うと悪くなってしまいます
でヒストグラムの正規化は講義によっては効果がありました
でコンテキスト依存音節モデルは性能向上に
わずかながら有効でありましたということがわかりました
で次に要約と重要分抽出要約と
自動インデキシングトピックによる分割について
報告致します
まず要約ですけれども今回重要文抽出型で行ないました
これにより
予め音声を文で区切り
認識結果と音声データから重要箇所これがこの文が重要であるということを
判断し
その音声を提示します
そうすることによって認識誤りによる認識
利用者の誤解釈を軽減することが
できます
重要文の抽出方法としてはこのような特徴を用いました
韻律情報と表層的言語情報
がありますけれども韻律情報としては
Ｆ０の平均の高い文パワーの平均の高い文などがあります
表層的言語情報としては
一般的によく広く用いられているＴＦやまたはスライドの情報スライドの
その講義
で使用されたスライドの中の高頻出単語を含む部分
などそういった情報を用いている
また予め
これは明らかに重要でないと
判断する
特徴として
Ｆ０パワーの平均が低い文発話時間長の短い文
などを使用しています
そしてこれらの特徴を組み合わせて文の総合スコアを出し
そのスコアを高い順から
重要であるという
重要文集合に含めてい
この要約の評価方法ですけれども
被験者六名
いずれもこの講義の方面に明るい
識者の方たち六名による
重要文の集合を用意します
そしてその六人中三人以上が重要と判断した文を正解文とします
そしてこのシステムの目標値は各被験者とその被験者を除いた
つまり五人中三人が重要であると判断した文集合との一致度
を目標値としています
評価尺度としてはκ値
これは二者の判定の一致度を偶然の一致を考慮し調整した指標です
そしてＲＯＵＧＥ—Ｎ
これはリファレンスに含まれるＮグラムがシステムの文中に
どのくらい含まれるかという尺度で評価しています
こちらが結果ですけれども
要約は講義一講義三講義四について行ないました
こちらに各特徴
で括弧ＴＲＮというのは人手による書き起こしで
表層的言語情報を抽出したもの
括弧ＡＳＲというのは自動認識で表層的言語情報を
抽出したものです
ＴＲＮはＡＳＲの理想値
ということで
どのくらい差が出るかということを調査する目的でも行ないました
結果としてこちらに
それぞれの様子を組み合わせた
組み合わせ実験を行なっているのですが
この実験においては誤認識による悪影響が
ＡＳＲとＴＲＮほとんど変わらないので
悪影響が軽減されるということがわかります
また講義三四については目標値に非常に近いと
ただし講義によって性能に若干バラつきが大分バラつきがあるということがわかりました
こちらは先ほどはκ値による評価だったんですけれどもこちらはＲＯＵＧＥ—Ｎ
による評価です
これはκ値による評価と必ずしも比例しないということがわかりました
またこの組み合わせ実験では人間による要約よりシステムが高いスコアを出してしまい
若干直感的に合わない値となっている
別にキーワードのインデキシングについて説明します
今回セグメンテーションの実験も行なったんですけどもこちらは
思ったより結果が出なかったので省略致します
インデキシングについては
文章のスライドに対してスライド中のキーワードキーワードを予め
定めましてそのそれと発話文との自動対応付けをしています
対応付けはＤＰマッチングで
行なっています
またキーワードの選択の方法はＴＦＩＤＦ
を取っている
こちらは一般的なＤＰマッチングの図なんですけども講義音声で
このように単語が並んでいた場合
スライド中のキーワードとこのように関連付けていきます
で以上の実験を行いました
要約についてまとめますと
韻律表層的言語情報の複数の特徴を組み合わせることで人間による要約とほぼ同等のκ値が
得られました
また認識誤りについては頑健であるということがわかりました
またＲＯＵＧＥ—４の値はやや大きめになるため
これが評価方法として最適であるとは言い難い
いうことがわかります
今後の課題としてはこのようなものがありますが
つまりちょっとデモ
試作した教材のデモを行ないたいと思います
時間
先ほど最初に見せた
画面なんですが
説明は省きますとりあえず再生させて
みますと
このようにビデオが再生されるんですけれども
こちらの方にキーワードの一覧が
あれ
それでこちらに認識結果
の文が表示されているんです
けれども
のはずですが
おっと
なんでや
ちょっとすみませんドタバタしてまして申し訳ないですがこのような教材をとりあえず試作
してみました
今全文再生モードになっているんですけれども
要約率をこのように変えることで
重要文だけ
要約の結果に従いまして重要文だけを再生することが
できます
今ちょっと要約なのか何なのかわかりづらいところにあるんですけれども
文文番号が
十四番から十六番十八番に飛んでいるのがわかるかと思いますがその間の文は
省いても構わないとシステムが評価
したということ
でこちらが先ほどインデキシング説明だけしてしまったんですけどもキーワードのインデキシング
の機能で実現したんですが
例えばここをクリックすると
その文がその単語が話されているところへ
ジャンプすると
いう機能を
設けています
でこちらの方も同じです
で
再生速度も
変えられると
すみませんボリュームを調整しながら
で早くすると
このスクロールなんですけれどもちょっとあまりうまくいってない
こうか
ちょっとまだうまく動作できないんですがこのようなものを作成しましたということですはい
じゃちょっとお見苦しかったですが以上で発表を終わりますすいません
