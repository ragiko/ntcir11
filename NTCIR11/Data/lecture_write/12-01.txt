こんにちは
文レベル情報と複数仮説を用いた音声認識結果の自動整形の評価というタイトルで豊橋技科大のが発表します
まず研究の背景ですが
現在世界中の大学が講義コンテンツをウェブ上にアップロードするというのが盛んになっています
そのためオンライン上の講義データが増加しているという背景があります
こういったウェブ上にたくさん存在している講義音声データの利便性を高めるための諸技術が望まれています
そういった講義音声データに対して書き起こしを付与することができればユーザーに対して
ユーザーの理解を助けることができると考えられます
しかしながら講義音声というのは多くの非流暢現象や専門用語の存在またその自由な発話スタイルから認識が非常に難しいという問題があります
現在の最先端の音声認識システムを用いても講義音声の認識は七十％程度の
程度しか達成できていませんまた
講義音声は自然発話であるためたとえ認識率が百％であったとしても非常に読みづらいものとなってしまいます
そのため書き起こしをそのままユーザーに提示するのではなくユーザーに提示する前に認識結果の自動整形をする必要があります
ここでいう自動整形とは話し言葉の系列から書き言葉の系列に変換することを指しています
そのような変換には
えーとあのーといったフィラーの除去
じゅ授業を始めますのじゅを削除するような言い淀みの削除
それでは始めますの
点丸のような句読点の挿入
またちょっと食べるを少し食べるに変換するような
口語表現の変換などがあります
このような自動整形を実現するための手法としての先行研究として
話し言葉の系列から書き言葉の系列への変換を統計的機械翻訳であるというふうにみなした統計的機械翻訳による手法や
チャンキングによって言い淀み箇所を同定し削除するといった手法があります
我々の研究室では認識率が非常に悪い講義音声を対象とするために
複数仮説を用いた手法を提案してきています
我々が提案している複数仮説を考慮した自動整形手法は
音声認識結果から得られるコンフュージョンネットワークを利用することによって
認識誤りに対処可能な手法となっています
実験結果より
複数仮説を用いる場合と用いない場合を比較することによって
複数仮説を利用する効果が示してきました
しかしながら複数仮説は効果がある整形に効果があるんですけれども
複数仮説を利用することによって文レベル情報を利用しにくいという欠点があります
文レベル情報は
高精度な句点挿入精度に
句点挿入を作成するためには必要不可欠な情報ですので
整形の句点挿入精度に影響が出てしまっていました
ここで言っている文レベル情報とは
文全体を見て得られるような情報のことで
例えば前後する単語の窓を見て何かディシジョンするようなものあるいは
文全体を解析して得られる係り受け情報などが
のことです
このような文レベル情報というのは自動整形に対して非常に有効な情報ですので
複数仮説を用いる場合でも文レベル情報を用いたいということになります
しかしながら
コンフュージョンネットワークのようなラティスで複数仮説が表現されている場合に
効率的に文レベル情報を用いる方法はこれまでわかっていませんでした
しかし
近年Ｉｔｅｒａｔｉｖｅｄｅｃｏｄｉｎｇと呼ばれるコンフュージョンネットワーク上の仮説を任意の情報を用いてリスコアリングすることができる手法が提案されています
このアルゴリズムを用いることで整形に有効な文レベル情報を複数仮説と同時に使用できるというふうに考えられます
整形に有効な文レベル情報としては
文境界検出分析チャンキング係り受けなどが考えられます
このような文レベル情報を
抽出することができる手法としてらによるＩｍｐｒｏｖｅｄ−ＳＤＡがあります
これはチャンキングと係り受けの逐次適用による文境界検出アルゴリズムです
Ｉｍｐｒｏｖｅｄ−ＳＤＡは内部にＳｅｑｕｅｎｔｉａｌｄｅｐｅｎｄｅｎｃｙａｎａｌｙｓｉｓという文境界が未知の場合にも適用可能な係り受け解析手法を
使用しています
これにより
文境界検出チャンキング係り受け解析結果を利用可能となります
ということで本研究の目的といたしましてはＩｔｅｒａｔｉｖｅｄｅｃｏｄｉｎｇとＩｍｐｒｏｖｅｄ−ＳＤＡを組み合わせることによって
文レベル情報と複数仮説を考慮した制御を実現するということになります
ではまず従来法から説明していきます
従来法の定式化はこちらに示す式のようになっています
ここでＷは書き言葉の系列
Ｓは話し言葉の系列Ｏは音声特徴量の系列になっています
この定式化によって
音声特徴量の系列Ｏが与えられた時に
こちらに示す式を最大化するＷを求める問題として整形が定式化されています
このＰＳ｜Ｏはコンフュージョンネットワークからの事後確率のことで下にコンフュージョンネットワークの例が示してありますが
各コンフュージョンネットワークのアークに乗っている単語それぞれの事後確率を意味しています
次に
このデルタですがこのデルタは話し言葉から書き言葉への変換をおこなうルールのことで
本研究ではフィラーの削除及び句点挿入を考慮しました
フィラーの削除は
こちらに示すような
えーとというものを
脱落を表わす特殊単語であるｄｅｌに変換する
また
句点挿入は
句点句点というのは基本的には無音区間ポーズに対応しているというふうに考えられますので
ポーズを句点に変換する
但しポーズが無音区間が必ずしも句点に対応する訳ではないので
こちらのコンフュージョンネットワークの全てのｂｉｎに存在している脱落単語を句点に変換するというルールも用いて用いました
そしてこのＰＷですけれどもこれは
変換された
与えられた
整形文がいかに整形文らしいか表わす言語モデルになっています
これらの式を組み合わせて
この式を最大化するＷを求める問題として定式化しています
そしてこのマックス操作でいろいろな複数仮説を考慮することによって
さまざまな仮説を考慮することによって複数仮説を考慮していきます
以上が従来法の説明でした次に提案法について説明します
提案法について説明するために本研究で使用したＩｍｐｒｏｖｅｄ−ＳＤＡについて簡単に説明したいと思います
Ｉｍｐｒｏｖｅｄ−ＳＤＡは文節チャンキングとＳｅｑｕｅｎｔｉａｌｄｅｐｅｎｄｅｎｃｙａｎａｌｙｓｉｓを主要な構成要素として持っているので順に説明します
まず文節チャンキングですが文節はのちの係り受け解析の基本単位となるので係り受け解析に先立って文節チャンキングが行なわれます
文節チャンキングは文の先頭Ｂｓと文節の先頭Ｂｂを区別したＩＯＢ２チャンキングによって実現されます
またこのチャンキングは
Ｉｍｐｒｏｖｅｄ−ＳＤＡらによるＩｍｐｒｏｖｅｄ−ＳＤＡではＣＲＦによってを実現されています
この時使用されている素性は前後三単語までの表層形品詞品詞再分類活用形の全ての組み合わせとなっています
下に例が示してありますがこの場合では
えーあの今日はですねという発話単位に対して
えーあのが一つの文節今日はですねが一つの文節としてチャンキングされています
えーあのは文の先頭ですのでＢｓ今日はですねは文の先頭ではないのでＢｂという
タグがついています
ここで得られた文節結果を元に係り受け解析を行ないます
Ｓｅｑｕｅｎｔｉａｌｄｅｐｅｎｄｅｎｃｙａｎａｌｙｓｉｓは文境界が未知の場合にも適用可能な係り受け解析手法になっています
文境界が未知の場合にも適用可能とするために文境界を表わすメタシンボル及び将来的に観測されうるメタシンボルＣを導入することによって
この問題を解決しています
本研究では
係り受けが存在しない場合のためにさらにこの上記の二つに加えてεというメタシンボルを導入しています
このこれらのメタシンボルを導入することによって従来の
係り受け解析手法を適用することが可能となります
そして順に入力されてくる発話単位を順々に
係り受け解析していきます
下に例が示されています
この例では京都は日本で最も歴史のある街ですという文に対する係り受け解析の結果を示しています
ここでは京都は日本で最もというのが一つ目の発話単位で歴史のある街ですというのが二つ目の発話単位となっています
まず
ステップ一では発話単位一が入力されます
そしてそれに対する係り受け解析が行なわれます
ここで
日本でに対しては最もという係り先が存在していますが
京都はと最もに対しては係り先が存在しないので
将来的に観測されうるメタシンボルであるＣに対して係るという結果が得られます
次にステップツーでは
ツーで歴史のある街ですという発話単位が入力されます
ここで
京都は最もに対して
はＣに係っていたんですけれどもそれを取り除き
新たに京都は最も
に対する係り先も計算し直します
そして全体の係り
受け結果が得られます
これ
このＩｍｐｒｏｖｅｄ−ＳＤＡによって得られる結果を利用して
整形を定式化し直したのがこちらの式になります
まずこれは先程と同様にコンフュージョンネットワークからの情報確立
そしてこれは話し言葉表現の変換になっています但し
この話し言葉表現の変換は
Ｉｍｐｒｏｖｅｄ−ＳＤＡの結果を反映したものになっています
ＰＷは同様に整形文の言語モデルです
これらの既存のコンポーネントに加えて
チャンキングスコア及び係り受けスコア
が
入っています
これらのスコアを最大化するようなＷを見つける問題として整形が
定式化されています
ここで変換ルールに変更について説明しますＩｍｐｒｏｖｅｄ−ＳＤＡを結果を利用することで変換ルールを
変更しました
元々は
こちらに示しますように
任意の単語ＷとＳの変換を表わすルールだったんですけれどもそれを
Ｉｍｐｒｏｖｅｄ−ＳＤＡの結果を反映したルールに変更しています
これによってフィラーの削除は元々フィラーを削除するというルールだったんですけれども
フィラーの係り先がない場合にフィラーを削除するというルールに変更し
句点挿入は無音と任意の箇所に挿入するというルールだったんですけれども
Ｉｍｐｒｏｖｅｄ−ＳＤＡの結果を信用して
Ｉｍｐｒｏｖｅｄ−ＳＤＡが文
境界であると判定したこのＢシンボルの所に句点を挿入するというルールに変更しています
では次にＩｔｅｒａｔｉｖｅｄｅｃｏｄｉｎｇについて説明します
ここでは例を用いてＩｔｅｒａｔｉｖｅｄｅｃｏｄｉｎｇについて説明していきますここでは
えーあの京都は日本でという文が発話されたとしてその認識結果が
コンフュージョンネットワークによるワンベスト認識結果が
えーあなた今日は資本主義だったという例をもとに説明します
まず
すみません一番左のｂｉｎのえーえｄｅｌという三つの仮説について考え
ワンベストのえーあなた今日は資本主義に対して
えーあなた今日は資本主義えあなた今日は資本主義ｄｅｌあなた今日は資本主義という
三つの文を
考えそれぞれの文に対して文レベル情報任意の情報を用いたスコアを計算します
そしてその中で最も高いスコアを与える
仮説を残します
次に次のｂｉｎに移ります
そして同様にあなたあのｄｅｌの中で文全体を考慮した時に最もスコアが高くなる
単語を選びます
この場合は元々あなただったのがあのに変わりました
それをどんどんと繰り返していきます
最後ｂｉｎの最後まで到達すると最初に戻ります
そして同様の処理を繰り返します
そうすると先程一つ
一度目のパスでは今日は京都に変わらなかったんですけれども前後の単語が変わったことによって
今度はここが京都に変わりました
これをどんどんと繰り返していきますそして仮説が変更しなくなったところで
アルゴリズムは停止します
これによって
コンフュージョンネットワーク上の仮説を
任意の文レベル情報を用いてリスコアリングすることが可能となります
本研究で利用する情報の例を使って説明します
たとえばえーあの京都は日本でという仮説に対して
スコアを計算することを考えます
そのときまず仮説が選ばれます
そして
チャンキングを行ないます
次に
係りうけ解析を行ないます
そしてこの係り受け解析の結果に基づいて
変換ルールを適用します
そして
変換ルールが適用された列に対し
整形文の言語モデル確率を適用します
そして最終的に得られたスコアが
今のえーあの京都は日本でに対するスコアになります
この方法を用いて先程のリスコアリングを繰り返していきます
以上が提案法の説明になります
実験です
早い
使用した講義データは
日本語講義音声コーパスＣＪＬＣの四話主八講義分のデータです
一講義あたり約六十五分
単語誤り率は四十三．八パーセントのデータで非常に認識率が悪いデータとなっています
テストデータに使用したＣＪＬＣの八講義分のデータに加えて
ＣＳＪの四講義分を開発セットとして使用しました
これを使用して全てのパラメータをチューニングしました
文整形データはこれらの講義に対して人手で作成し
人手により作成しました
音声認識デコーダには本研究室で開発されたＳＰＯＪＵＳ＋＋を使用しました
このデコーダはコンフュージョンネットワークが出力が
可能なので
これを複数仮説のために使用しました
音響モデルはＣＳＪ八百十四講演男性のみから学習した
左コンテキスト依存音節モデルで
出力分布は四混合ブロック型のＧＭＭです
特徴量としては一般的に用いられているＭＦＣＣΔＭＦＣＣΔΔＭＦＣＣΔパワーΔΔパワーを使用しました
三十八次元を使用しました
言語モデルとしては
話し言葉モデルとして
認識のための言語モデルとしてＣＳＪから学習したバイグラム
整形用の書き言葉言語モデルとしては毎日新聞九年分から学習した
四グラムを利用しました
語彙は
認識用の言語モデルの学習に使ったＣＳＪ講演で頻度が上位の二万語を用いました
この時
認識用の言語モデルがバイグラムなのはデコーダの制約ではなくて
認識率がバイグラムトライグラムなどよりもバイグラムの方が良かったためです
次にベースラインと説明します
提案法をこれらのベースラインと比較しました
一つ目はフィラー除去ですこれは認識結果のワンベスト仮説からそのその品詞がフィラーまたは感動詞であるものを除去したという手法です
次のワンベスト整形は
複数仮説も文レベル情報も使用しないワンベスト仮説のみを含むコンフュージョンネットワークを用いて提案法を適用したものです
但しこれは
他の研究とは違いコンフュージョンネットワーク上の自己確立の情報は使っています
最後の複数仮説は
我々の先行研究の手法で
文レベルの情報を使用
文レベル情報を使用しない提案法になっています
実験内容としては客観評価と主観評価を行ないました
客観評価としては人手整形文に対するワードエラーレート
また人手整形文に対する句点挿入精度を計算しました
評価しました
主観評価としてはテストデータのサブセットに対する被験者実験を行ないました
被験者は十名
でした
そして読みやすさ理解しやすさの二つの尺度に対して提案法を評価しました
ここで読みやすさとはテキストの内容を知らない状態でテキストを読んだ時に
二つのテキストを読み比べた時にどちらが読みやすかったか
つまり内容を問わずにどちらが読みやすかったか
そして理解しやすさというのは正解テキストを提示し内容を理解してもらった上で二つのテキストを読み比べてもらい
どちらがテキストの内容を理解することが本来の内容を理解するのに役立ったか理解しやすかったか
ということを意味しています
こちらが実験結果です
まず人手整形文との比較結果です
こちらの結果を見ますと一番左が
認識結果そのまま
を
人手による整形文と比較した場合
これに対してフィラーを除去することで有意に改善しています
さらにこれに対してワンベスト整形を行なうことでさらに改善しています
ここに
従来の提案法である複数仮説を用いた手法を適用することで
改善がみられています
また今回の提案法である文レベル情報を使用することで
どちらの場合でも改善しています
そして複数仮説と文レベル情報両方使用した時が最も良い結果となりました
次は句点挿入の評価結果です
一番
左の発話単位というのは
発話単位ポーズの所に
句点が挿入されていると仮定して
句点挿入をおこなったもの
です
これに対して
ワンベスト整形を行なうことで
有意に改善しています
ここからさらに
複数仮説を利用すると逆に句点挿入精度に関しては下がってしまうという結果になりました
これはｎ−ｇｒａｍが前後
直前の三単語しか見ていない
という
局所
的な
情報なので
複数仮説を考慮することによって湧き出し誤りが増加してしまい
リコールは上昇するんですけれどもプレシジョンは下がって結果的には下がってしまうという結果になっていました
しかしながら文レベル情報を組み合わせることによって
両方の場合で改善し特に複数仮説の場合で大幅に改善が見られました
そして
複数仮説と文レベル情報を組み合わせた時に最も良い結果となって
複数仮説と文レベル情報を組み合わせる有効性を示しています
こちらは主観評価による
読みやすさの評価結果です
これはＡとＢのテキストを読み比べた時にどちらが読みやすかったかというのを示しています
まず認識結果とフィラー除去を比較するとこの青い
四角は
差が有意であったということを示して検定によって有意であったっていうことを示しています
でこちらを
見ますと認識結果とフィラーを比較すると
フィラー除去のほうがよいとつまり
フィラーの除去は読みやすさについて常に効果的である
さらにこのフィラー除去から
ワンベスト整形を行なうことでまた有意に改善しています
そしてこの従来法からさらに提案法である複数仮説そして文レベル情報を利用するっていうのを比較すると改善していて
主観評価によっても読みやすさを改善している提案法が改善しているっていうことを示しています
こちらは理解しやすさに関する評価結果です
理解しやすさっていうのは内容をテキストの内容を与えられた上でどちらが理解しやすかったかっていうことなんですけれども
この結果に関しては
認識結果とフィラーを比較すると
フィラーの方が良くて
フィラーの除去は理解しやすさに対しても効果的である
しかしな
で次にこのフィラー除去とワンベスト整形を比較するとフィラー除去の方が良いという結果になっています
読みやすさではこちらが良かったんですけれども理解しやすさではこちらが良いと
これはワンベスト整形によって
ん
内容の理解に必要な単語の脱落が生じてしまい
理解のしやすさを低下させてしまったものというふうに考えられます
このワンベスト整形と
提案法の複数仮説文情報ありを比較すると
提案法のほうが良く
従来法に対しては提案法が読みやすさに理解しやすさに関しても改善しているっていう結果が得られました
フィラー除去がワンベスト整形よりもよくて
ワンベスト整形よりも複数仮説がいいのでフィラー除去と複数仮説の間にはあ関係を定義言えないんですけれども
このフィラー除去と
文レベル情報を使用してない複数仮説を比較した結果
フィラー除去と複数仮説の間には優位差がないという結果になりました
これより認識率が低い条件下でも提案法により理解しやすさを損なうことなく
読みやすさを向上するという整形ができたということがいえます
最後にまとめます
今回複数仮説を考慮しながら文レベル情報を使用した整形を行なう手法を提案し文レベル情報を複数仮説と同時に使用する効果を示しました
被験者実験により提案法は理解しやすさを損なうことなく読みやすさを向上するということが確認できました
今後の課題としては今回は人手で作成した簡単な変換ルールを使用しましたが
変換ルールを書き起こしありで変換ルールを自動獲得する手法あるいはリアルタイム性を向上するために今回使用したコンフュージョンネットワークでなく代わりにラティスを用いるというようなことを検討していきたいと考えています
発表は以上です
