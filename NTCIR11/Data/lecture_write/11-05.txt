はい自然会話の言語情報による発話権譲渡推定手法の比較と題しましてのが発表させていただきます
研究背景と目的なんですけど一言で言うと人間とロボット
に自然な会話をさせたいということで
音声を介したコミュニケーションが必要で
ユーザーの
が喋っている途中にいきなり割り込みされても困るのでユーザーの発話の終了を判断した上で
させるために
させたいということで
このユーザーの発話の終了を発話権の譲渡と
定義しまして
発話権の譲渡を意図した発話を推定するというのが本研究の目的です
で関連研究として
いくつかあるんですけど発話末の品詞や韻律情報による
発話終了予測などがあって
これは発話の最後の品詞の情報や音声のピッチパターンパワーパターン
から予測したもので
韻律が重要だよと言っているような研究なのですけど
その一方で
次
品詞や発話末の品詞や韻律情報と相槌との相関を調査した結果
発話末の韻律情報と相槌の
正規頻度との相関は低くて
発話末の品詞情報と相槌の
正規頻度との相関
は高いというような
報告が
ありまし
た
はい
でさらには
そうですね韻律情報や
品詞情報などを
決定木によって
を使って決定木によって発話者の交代を推定したものがあるんですけど
この構築された決定木では韻律情報の自由度はあんまりなくて言語情報が
よく使われてたというような報告があります
このように言語情報が重要だよというような報告があるのですけど
その発話末の品詞情報のみ
などその断片的な情報しか使われて
なかったりあと会話の流れっていうものを考慮されていないものが多いかなと感じたので
今回は
本稿では言語情報に注目して様々なモデルや異なる素性を用いて
比較を行いました
はいそれでその発話権の譲渡を推定しようということなのですけど
この
これらのモデルを試しました
でこれらについて簡単に説明します
で決定木
色々種類あるのですけどＣＡＲＴを使いました
決定木
ご存知の方多いと思うんですけどルートノードから
質問に応えて最終的に行き着いたノードのクラスで分類するようなもので特徴としては
量的変数や質的変数が混在するような素性も簡単に扱えたり
あとは構築したモデルの解釈がこういうふうに
綺麗に出てくるのでどういった要因があるのかとか
ていう解釈がすごくしやすいという
メリットがあります
でその一方であんまり多くの素性大規模な素性を扱うには不向きといったデメリットもあります
で学習は
不純度の減少を最大化させるようなノードを生成するんですけど
要は
綺麗にクラスが別れるような
より偏りクラスの
割合に偏りが生まれるように分類ノードを生成していきます
でぼんぼん木を成長させていくと過学習を起こすのでＣＡＲＴでは
１ＳＥルール
で
クロスバリデーションエラーの最小値から標準偏差を足した値
の所
を
下回るノード数の所で枝刈りをするというような
やり方で過学習を防ぎます
で次最大エントロピーモデルなんですけど
これは経験的確率分布による素性の
期待値とモデルによる素性の期待値を一致させるっていう制約のもとでモデルのエントロピーを最大にするモデルです
こういう形で対数線形モデルで書けるんですけど
書けます
で素性
大体分かるかなと思うんですけど素性
として
二次関数が使われます一般的には
でこの
Ｘが入力でＹが出力に当たるんですけど
その
Ｘの条件がＴＲＵＥで
なおかつＹがＳであれば一でそれ以外だったら
〇
というような形で例えば発話Ｘに単語
がＷ
が含まれているかどうかがこのＢＸＥ
ＩＳＴＲＵＥ
とかで
かつ発話が発話譲渡権を意図しているかどうかっていうのがこのＹ＝Ｓみたいな
形になるんですけどこういうような素性
になります
で特徴としては
こういうように柔軟な
素性を制約条件に取り込むことが可能っていうのと後は外れ値に
の影響を受けにくいというような
特徴があるんですけど
その有効な素性の
集合の選択が必要という所で少し手間だったりします
で学習は
こちらの対数誘導関数を最大化するんですけどＧＩＳとか
順法などを使って
やります
これだと過学習を起こしちゃうんで
パラメータの事前分布としてガウス分布やラプラス分布を
仮定して
正則
正則化項を足すのは
一般的です
で次にサポートベクターマシンです
これは二つ
これだったら丸と三角の二つのクラスがあるんですけどこの
境界
この二つのクラスの間を最大化させるマージン最大化に基づいたモデルです
でこうゆうふうに
その
ＳＶＭスコアが〇より超えてたら一で
〇より下回ってたら—一というような
形でクラスを分類します
特徴としては
パラメータ数の割に
過学習に陥りにくいという特徴や後は
今回は複雑な複雑なというか線形カーネルで使っているのであまり関係ないんですけど
多項式カーネルとか使う
ガウスカーネルとか使うと複雑な素性も
容易に扱うことができるという特徴があります
どのカーネル関数を使おうという問題が
出てきたりもします
で学習はそのマージン最大化ここを最大化するようにパラメータ学習するんですけどこういう形で表せます
でこれはハードマージン最大化といって
完全に分類できる場合なんですけど
完全に分類できない場合や汎化性を高めたい場合はソフトマージンの最大化で
内側に入り込んだ分の罰則
項を加えてやって
この値を
最小化するというような
形になります
で
さっき
会話の流れをあまり考慮されていないって
言ってたんですけど
系列ラベリングモデルも使って推定しました
でこっから系列ラベリングモデルになるんですけど
代表的な隠れマルコフモデルです
これは状態遷移に単純マルコフ仮定を仮定したマルコフモデルなんですけど
観測系列Ｘこれが発話の系列に当たるんですけど会話に当たるんですけど
で状態系列Ｙ
発話発話権譲渡を意図しているかどうかを求める問題になります
で
本来なら系列全部を見て
最もらしい系列を選んでくるんですけど
逐次推定になるので先の会話っていうのは予測できないので逐次推定することになるので
こういう形で
推定することになります前の推定した結果を利用して逐次推定することになります
でこれだと決定的に
推定していかなきゃいけな
いんですけど
前向き確率を利用したら
直前の状態の不確実性を扱えて
ある程度安定になるのかなということで
この直前両方の推定を
比較してみました
でＨＭＭの特徴としては
系列ラベリングモデルなので前の状態を考慮可能っていうのと
あとは生成モデル
というこれデメリッ
トかなと思うんですけどＰ
Ｐ（Ｘ｜Ｙ）が定義できてないと
できないといけないという生成モデル
の特有の
欠点があります
あとは非独立な素性を
そうですね非独立な素性を扱うことができないというような
デメリットもあります
で学習は単純に
最尤推定で
解けるんですけど
ゼロ頻度問題
学習データに現れない単語が
テストデータに現れると確率ゼロになっちゃうので今回はラプラススムージングをしました
で次最大エントロピーマルコフモデルです
こちらはＭＥ先程説明したＭＥモデルに単純マルコフ仮定を
仮定したモデルです
で
同じように
形式化できるんですけど
特徴としては
識別モデルというのが大きな違いで
こっちがＨＭＭのグラフィカルモデルでこっちがＭＥＭＭの
グラフィカルモデルなんですけど
ＨＭＭは状態から
観測値を出力するのに対して
ＭＥＭＭは
前の状態と観測値を基に
今の状態を
生成するような形になってます
はいで学習は直前の状態が
同じもの
ごとに
ＭＥモデルを学習して
それを使って
推定するという形になります
であとこれは別に一般的に使われ
てないんじゃないかなと見たことはないんですけどＳＶＭ
を
マルコフモデル
ＳＶＭとマルコフモデルを組み合わせたモデルを提案というか
試してみました
さっきＭＥＭＭは
なんですかね
識別関数に
ＭＥモデルを使ってたんですけどこれはそのＭＥモデルをＳＶＭに切り替えたバージョンなので全く同じような
形なんですけど
そのＭＥ
全く同じような形です
でただＳＶＭだと
確率を出力するわけじゃないんで標準シングルモデル関数でＳＶＭスコアを
確率に変換してます
はい
以上が
用いた
モデルの説明簡単な説明です
で評価実験なんですけど
自然会話のデータ
セットに対するモデルの比較を行いました
でどういったデータかというと二者の会話
二者が
自然な
テーマを
フリートークをしてもらって
それを書き起こしたデータセットです
で漢字かな混じり
になってます
なってるんです
けどただ
！マークとか？は入ってないです
で会話のペア数が三十ペア三十組のペアで
その発話権譲渡を意図した発話
っていうのを
がその書き起こし者によっ
てラベル付けされているんですけど
それが
六千五百三十でその他の発話が
一万四千三十二なので大体一
対二ぐらいですかね
で語彙数が三千八百三十八で発話毎の平均語彙数が三±二くらいなのですごいスッカスカの
データです
で
で素性として二種類
別の取り方をしたんですけど
一つは単語素性
と呼ぶことにするんですけど
発話の中の単語の有無を〇一で
表現しました
でもう一つを品詞素性とするんですけど
これは
発話の中の品詞の有無を〇一で表してさらにそれ＋単語数と
読みの文字数を素性としたものです
こちら先行研究で使われてた
単語数とか
その局所的な
特徴量を使ってたっていうやつに
似た
取り方かと思います
で各モデル各素性につきテンフォールドクロスバリデーション
で評価したものとクローズドデータで評価したもので
そのクロスバリデーションの方をオープンデータと
しました
でこちらが実験結果ですオープンデータに対する各モデルのＦ値を表してます
で青が単語素性で
ピンクが
品詞素性で
全体的に
オープンデータでは品詞素性の方が良い結果となってます
で
そうですねで一番良かったのが
とりあえず単語素性では
ＳＶＭとマルコフモデルを
組み合わせた
もので
で
品詞素性ではＳＶＭが
最良の結果を示しました
でこちらがクローズドデータに対する各モデルのＦ値なんですけど
当然ながら縦軸の値が大分違うんですけど
クローズドデータなんで大分良くなってます
で
そんなに素性
品詞素性だとそれほど上がってないんですけど
単語素性だと大幅に
上がってる
かなと思います
でこちらも先程と同様にそうですね単語素性だと
そのＳＶＭとマルコフモデルを組み合わせモデルが最良の結果を示していて
品詞の素性では
ＳＶＭが最良の結果を示しました
で素性による結果の違い
についてなんですけど
オープンデータで
言うと全体的に単語素性よりも品詞素性の方が良い結果を示しました
で
これなんでかっていうと次元数が多くなると学習データとテストデータの間に類似したデータっていうのが
現れにくい
すごい表現力が豊かになる分そういう類似したデータが現れにくくなるので
その分品詞素性のように少ない次元数で表現しているものは
は
学習データとテストデータの類似性が高いので
結果も良くなるのかなと
思いましたはい
で
で次クローズドデータなんですけど
単語素性
単語を素性にした方が品詞品詞を素性した
のに比べて
大分
オープンデータの時よりも結果が大きく向上してますと
でこれはなぜかっていうと
そのクローズドであれば学習データと
テストデータが全く一緒なので類似していると
なので
さっきと
逆になるんですけど
クローズドで良い
かなり良い結果が出たということは学習データが大量にあってその
学習データとテストデータの差を埋めることができれば結果が大きく向上できるんじゃないかなと
考えられます
よって少量のデータしか学習データがない場合は識別に有効で小規模な
素性の選択が重要になってくる
ですけど
逆に大量のデータの場合は単語のように
大規模な素性を使った方が有効なのかなと
思います
であと
系列ラベリングで前向き確率を用いる
っていう話をしたんですけど
そうですねオープンデータだと前向き確率を用いた方が全体的に良い結果が
若干良い結果になってました
でこれがそのＳＶＭと
ＳＶＭのマルコフモデル
を比較し
の結果の例なんですけど
これが正解ラベルをＧっていうのが発話
権の譲渡を意図した
発話
としてラベル付けされているもので
これが各モデルの推定結果と発話権譲渡の
確率を表しているもので〇．五を超えたら
Ｇってラベルが振られるんですけど
注目して欲しいのは三番目なんですけど
ＳＶＭでは〇．八三四
ていう結構
高い確率で発話権の譲渡
を意図してますよって推定しているんですけど
前向き確率用いない場合は
〇．四ぐらいで
誤識別誤判別しちゃってると
あるんですけど
前向き確率を用いると
そうですねもう片方の
状態からの
影響があってなんとか
正しい正しく推定できてますというような
結果です
でクローズドデータだと
前向き確率が
を用いた方が
全体的に
悪
くなる傾向があったんで
本来
モデルの信頼性が低い場合に
有効なのかなと
考えられます
でまとめです
で本稿では
発話権譲渡を意図した発話の推定についてモデルや素性を比較しました
でモデルとしてはオープンデータに関してはＳＶＭとマルコフモデルを組み合わせたモデルが最良の結果を得られて
前向き確率を用いることで不安定さが解消されたと言えます
でクローズドデータは
ＳＶＭが最良の結果となりました
で素性に関しては
少量の学習データの場合は識別に有効で小規模な素性の選択が重要で大量の学習データの場合は
単語のような
大規模な素性が有効なのかなと
考えられました
以上です
ご静聴ありがとうございました
