当然の結果になるということ
で
えーと問題点としてですね今お話ししましたけれども
そもそもウェブ文書が適切なものがちゃんと集められているのかと
いうことそれから
えーま
ま適切でかつ十分な量の文書収集できているかと
いう二つの問題が
えー問題とし
点として考えられます
で
え
ま先程もお話ししましたけれども
えー誤認識があると違うトピック拾ってしまう可能性がありますし
まそもそも
えそのニューストピックそのニュースの内容ですねえー
例えばあの長期間に渡ってよく見えニュースで報道されてるなもんだと非常にたくさんの生地が存在する訳ですし
えー
ま自分なりを集めてくることができるんですが
え一時的なニュースであったりもしくはまローカルな記事だったりするニュースだったりすると
えーま
そーその類似した記事自体が見つからないという問題が
あります
でえー本研究では
えまこの問題点を解決する為に
えー
ウェブ上から集めてきた
えー
文章の中から
適切な文章だけを選択してそれを用いて
えー言語モデルを作る
いうことで
でそれからえー三点目としては
えー昨年度のえー発表では
えニューストピックを予め切り出しておいて
えーま全体でえ検索全体の認識結果から検索をするってことを行なったんですが実際にはそのニュースの中では
えー例えば該当インタビューの途中にす挟まれていたりそれから
社会県であったりとか
えーもしくは背景に雑音があったりとかで
発話ごとにえ
かなり認識精度が異なるということが
えーありますので
え発話単位で処理をすると
いうことを考えました
で実際にそのアナウンサーの音声は非常に明瞭な
発音なりますので
えー
奇麗に喋っていただき
非常に認識精度になるんですけれども
ま途中で挟まれたそのインタビューであるとか
えー社会系のようなものと全く認識誤認識が多くなってしまうという
そういうものがあります
で
えそれからえーとですねえー
これまであの私達の提案した手法ではえーニュース記事であるということから
ニュースサイト限定してですねニュースサイトから記事を集めてくるってことを行なっていたんですけれども
えー
まー
その場合
まローカルな記事であったりえー一時的な記事であったりすると十分な量を集めることができないということでま今回はニューされたニュースサイト以外の対象にして検索を行なっています
