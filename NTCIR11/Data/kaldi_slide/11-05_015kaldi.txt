でえっとー
また
その
会話の流れをむすえーっとあんまり考慮されてないっていう
言ってたんですけど
まあのー
系列ラベリングモデルもえー使って推定しました
でこっから系列ラベリングモデルになるんですけど
あのー代表的な隠れマルコフモデルです
まこれはま状態線に単純マルコフ仮定と仮定したマルコフモデルなんですけど
えー
ま観測系列Ｘまこれがま発話の系列に当たるんですけど会話に当たるんですけど
で状態系列はい
ま発話じょえー発話現場と意図してるかどうかの系列を求める問題になります
でえっとー
ま本来ならえっと系列全部を見てえー歳
えピッチ尤もらしい系列を選んでるんですけどそのー
逐次推定になるのでその先の会話っていうのは予測できないので父に推定することになるので
まこういう形で
推定することになりますその前の推定した結果を利用して逐次推定することになります
でこれだとあの決定的に
えーで推定していかなきゃいけない
んですけど
まその前向き確率を利用したら
そのー
直前の状態の二から九時性をえー扱えて
えーある程度安定にになるのかなということで
まこのちょっとでえーとー両方の推定を
使い
えー比較してみました
でえーっとＨＭＭの特徴としてはそのまー
系列ラベリングモデルなどでん前の状態を考慮可能というのと
後はま生成モデル
というまーこれデメリット
とーかなと思うんですけどまその日
ＰＸ自分はいが定義しできてないと
えーできないといけないという生成モデル
あの特有のまー
欠点があります
後はまー一独立な素性を
そうですね一独立な素性を扱うことができないというような
まデメリットもあります
でま私はえーま単純に
最尤推定で
解けるんですけど
えー
ま
商品の問題
えっと隠し
えー学習データーに現われていないＲ談話れない単語が
えー
テストデーターに現われるとえ確率〇になっちゃうのであの今回はだからスムージングをしました
