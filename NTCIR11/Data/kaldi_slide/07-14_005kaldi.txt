で簡単に処理の流れを説明しますけれども
え最初にニュース音声をえー汎用言語モデルで認識します
で
えー認識結果から索引語候補を抽出してえ索引語を選び出し
えーウェブ上から記事を
え関連した記事類似記事をえ検索し収集してきてえー
それを用いて言語モデルを更新する
で
この処理がえーとま音声認識
最初の音声認識の時点では
えーま誤認識と含まれているんですけどもうまく類似記事を集めてくることができれば
えーま
高精度の認識ができるとえー誤認識が少なくなるってことで
えそれをまた繰り返し何回もすることでえー
精度が上がるんじゃないかということでえーまこの
えこれについては昨年のですねえー情報が
フォーラムで
発表したんですけれども
えこういうやり方を
取っています
で
ま結論としましてはま
正直に言うとえー
うまく行く時はもう
でまー当たり前の結果ですねでうまく行くってなつまり
えーニュース記事がうまく集められて言語モデルがうまく
えー作れれば
えーどんどん更新
の度にま精度があるんだけれども
もしここでウェブじえーウェブし
えウェブ上から収集してきた記事がまノイズがあったり違うトピックの記事だったりすると悪くなるというまー
