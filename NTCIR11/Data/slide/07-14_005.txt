で簡単に処理の流れを説明しますけれども
最初にニュース音声を汎用言語モデルで認識します
で
認識結果から索引語候補を抽出して索引語を選び出し
ウェブ上から記事を
関連した記事類似記事を検索収集してきて
それを用いて言語モデルを更新するというもの
で
この処理が音声認識
最初の音声認識の時点では
誤認識等含まれているんですけどもうまく類似記事を集めてくることができれば
高精度の認識ができると誤認識が少なくなるということで
それをまた繰り返し何回もすることで
精度が上がるんではないかということでこの
これについては昨年のですね情報化
フォーラムで
発表したんですけれども
こういうやり方を
取っています
で
結論としましては
正直に言うと
うまく行く時はうまく行く
当たり前の結果ですねでうまく行くというのはつまり
ニュース記事がうまく集められて言語モデルでうまく
作れれば
どんどん更新
の度に精度が上がるんだけれども
もしここでウェブ記事ウェブ
ウェブ上から収集してきた記事がノイズがあったり違うトピックの記事だったりすると悪くなるという
