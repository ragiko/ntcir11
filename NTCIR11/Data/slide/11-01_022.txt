でそれでやるとこうなりまして
この
ベースラインというのは一クラスの言語モデル
けども未知語に対してそれがこういうような
誤り率に対して
百クラスか二百クラスにやると誤りが減ると
いうことですね
でもう一つちょっと書かなかったんですが
インボキャブラリのサイズをこれが一ｋ増やすいうことは二十一ｋ二十五ｋ
三十ｋにするということですねインボキャブラリ
その通常の方法だともうちょっとよくなります先程の結果より
これが少し一％
○．四％ぐらいよくなるだけですね
さっき言った
三十ｋを戻ると
三十ｋこれ二十ｋプラス十ｋのインボキャブラリとしてやったのが誤り率が
二十三．四ですね
二十三．四に対してクラス言語モデル使っても二十三．八％ですから
単語をどんどんどんどん登録して
Ｎグラムの
単語
モデルを作るのと
を単語登録するだけでクラス言語モデルを自動的に作る
やっても認識率は
それほど変わらない程度に得られた
いうのが
