当然の結果になるということ
で
問題点としてですね今お話しましたけれども
そもそもウェブ文書が適切なものがちゃんと集められているのかと
いうことそれから
適切でかつ充分な量の文章を収集できているかと
いう二つの問題が
問題と
点として考えられます
で
先ほどのお話しましたけれども
誤認識があると違うトピックを拾ってしまう可能性がありますし
そもそも
そのニューストピックそのニュースの内容ですね
例えばあの長期間にわたってよくニュースで報道されているようなものだと非常にたくさんの記事が存在する訳ですし
充分な量集めてくることができるんですが
一時的なニュースであったりもしくはローカルな記事だったりするニュースだったりすると
そもそも類似した記事自体が見つからないという問題が
ありました
で本研究では
この問題点を解決するために
ウェブ上から集めてきた
文書の中から
適切な文書だけを選択してそれを用いて
言語モデルを作ろうと
いうことです
でそれから二点目としては
昨年度の発表では
ニューストピックを予め切り出しておいて
全体で検索全体の認識結果から検索をするということを行ったんですが実際にはそのニュースの中では
例えば街頭インタビューが途中に挟まれていたりそれから
記者会見であったりとか
もしくは背景の雑音があったりとかで
発話毎に
かなり認識精度が異なるということが
ありますので
発話単位で処理をすると
いうことを考えました
で実際にはそのアナウンサーの音声は非常に明瞭な
発音になりますので
きれいに喋ってるとこは
非常に良い
途中で挟まれたそのインタビューであるとか
記者会見のようなものだと全く認識誤認識が多くなってしまうという
そういう問題があります
で
それからですね
これまで私達の提案した手法ではニュース記事であるということから
ニュースサイトを限定してですねニュースサイトから記事を集めてくるということを行っていたんですけれども
その場合
ローカルな記事であったり一時的な記事であったりすると充分な量集めることができないということで今回はニュースサイト以外を対象にして検索を行っています
