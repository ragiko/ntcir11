でそれではえー表記のタイトルで発表さしていただきます発表者はえーとーぐらい学校学研究科の山な音ですよろしくお願いし
まず初めに研究背景ですが
えー
話し言葉音声のえー自動書き起こしを大語彙連続音声認識もしまして
で海に六八
二六などを自動的に作成するという研究が
え広く行なわれています
そしてえー話し言葉の音声認識が難しい理由として
えー内容その場で考えながら話してるという特徴がある為に
成分なまとまりのないものになり易い嫌いな理由三四の女とか現われる
といったことがえー
通常のえーこれまで用いられていた方では認識が難しくなると
はい難しくなるというえー理由になっ
でこっちへ制限モデルについてえーまずえー説明いたします系列え言語モデルというのは
えー話題四モデル化するという目的で
で壊されたものでして
えー
特定の話題他した単語出現確率を複数混合モデルという形態を取っています
えー
言語モデルの内部にえー
潜在面と呼ばれる
特定の話題や話し方の特徴にＮグラムのモデル
例えばえー政治スポーツ経済の部分を
えー話題であるとか
あるいは方言の話し方で話している場合英語で話している場合といった
話し方の特徴のＮグラムのモデルの
二．五分持っていますそして
表わしたい話題やえーハムスターの特徴に対して
それらの複数のモデルの最適な混合比を推定して混合することで
で目的の
若いではえー話し方の特徴を持った言語モデルを得るというので
でこの系列で言語モデルを話し言葉
のえー式に適応した場合の問題について説明いたします
まず話し言葉のコーパスというのはえー
ＣＳＪなどが代表的ですがえー広い話題性を持ってるものではありませんので
でそれだけを使ってもえー
話題性を十分明日系列を作ることはえー難しいと考えられます
逆にえー広い話題性を持ってるコーパスとして新聞記事などがあるのですが
こちら書き言葉で書かれてますのでその町をすることはできません
えそしてえーこれを解決する為の先行研究として二千三年八月日に行なわれた方法に
音系列Ａを二つ
用いまして話題の家をですえーモデルと
えー話者性のＰＬ
一えー千モデルという二つのモデルを使う方法が提案されています
えその研究ではえー
認識対象成人関するところ番組の
音声書き起こしという風に設定しまして
話題の言える性を他の改良から
えーそしてまた精度言える性を
ＣＳＪ過程中だったコーパスから学習する方法が取られている
でこの方法でえー問題はある程度入ってたのですがまーあのこの問題点として
えーこの話題の日予選
でもこの学習データーの
文て話が可能性だとは残ってしまうまた
えー
話者性が言える性の方にはえーその適用対象とまー講演の話題性が残ってしまうであると考え
また話題性と話者性のえーっと発話有声音モデル重み付け混合しますのでそちらにどれだけ強く適応するかというのが
これと関係になってしまうという問題
でそこで本研究ではえー提案手法としてえーこういう分割ＰＬ制という違った方法で普通の許すモデルを使う方法
提案しています
でこれは
でモデルの行為四分という考え方をしてまして
えー話題によって出現頻度が変化する声
プロは対応と呼んでいます
でそれから文献によって出現頻度が変化する語
ま話し方や分解能ですねそれを文傾向そして
この二つの影響どちらも県内紅葉汎用思っという風に呼んでこの三つのクラスにえーこういう分割するという
方法を取っています
えー
そしてえーそれぞれ別々にＰＬ性の学習取ってき行なって今
でそうすることで
でまずえー先程の先行研究と同じように同じ学習データーやっコーパスの中にはない
話題え文献他者を適応が容易である
またえー
えー
言語モデルの合意を三つに分割であこれ一つにまとめるという処理をしますので
えー話
最もえー話し方の特徴と話題の特徴
こちらにどれだけ適用するとこれどう関係にならないです六という特徴がある
でじゃ具体的なうえー言語モデルを生成方法を説明します
音系列え言語モデルは学習データーとして空間とは第八文献を接続し地元の
単語出現頻度のデーターから最尤推定を行ない
でそしてえー
本研究で用いてる方法ではまずこの学習データーを
えー話題の
えー話題に関連する母音のみの出現頻度
え文献を
後でえと大量の推定によってそれぞれの行為
クラスだけません本当に
学習データーを分類しますそして
えー若い時えーすえモデルを話者性の家ですので
でそして三四五のモデルこちらはえー
特徴いけないという前提に考えてますので適応を行なわない為にえ二グラムのモデルを生成して
都内に別々にえーモデルの学習を行ない
でそしてえー生成したあ三つのモデルをえーまず話題の並列モデルと
文献二重性モデルをそれぞれ
え目的の
表わしたい特徴が単語対五人とでえー適応を行ないます
そしてえー三つのモデルをそれぞれの行為クラスの
制限確率それぞれの語彙クラスの二分の確率で現われるかという
そこでえー重み付けをして普通の言語モデルに基づくえー取って
ちなみにここにクラスの出現確率というのはえー適応に用いた単語数で変化のデータートライグラム言語モデルを元にして
えーとある文脈の下でえー次に
この三つの声クラスターそれぞれどれだけの確率で現われるかというのを
え設計するクラス出現確率を推定モデルというもので
求めて
て式の上であこのような形になっていまして
で先程説明しました話題の一え専門データーはえーこの
えーう
え文系ＰＬってモデルのこの部分そしてえー範囲をこの
モデルがえー
この記事の方となっていましてそれぞれの
言語モデルの確率を
えー
あー文脈列とえー直前にいたもの後ろに
それぞれのクラスター現われる確率で重み付けしてえー混合数で計算をして
でですが実際にはこの三つのモデルの
えー語彙が完全に分かれてますので
えーこの
例えばＷＹＩ目的田んぼが
えーと話題語であった場合はこの話題語の並列モデルだけ確率を持っていって他の二つの確率が〇には
という状態で
三つのモデルと劣化を選択的に使っている状態
で次にえーその前の文型を履い用語の分割基準について説明をいたします
で基本的には高いものを持っている品詞分類九十二種類を
この話題文献範囲の方もえーそれがに分類するという方法で作っています
えまず
最初にえー
このえー九十二種類の分類を一の判断で
それがに分類するという方法で
二ページにこのような分割基準を考えることができます
話題語はえー一般名詞は固有名詞など
文献はえー情報を指示代名詞フィラー間投詞など
でソースｉに主に私はえー
文頭文末記号学という
ような考え方でえー基準を作ってことができます
別の人の判断で作った記述文がやはり間違いを含んでいる可能性もありますし
えー山も掛かるということでこれを
えー統計的な方法で生成できないかということを
考えました
えー品詞分類ごとにえーその品詞分類側帯に対してどれだけ関係性があるか
文献に対してどれだけ関係と思っているかということを統計的に測って
でそれに基づいて
えー分割の基準を決めるということを検討して今
でその為の
方法として整理情報量に基づいたあー語彙分割基準の生成ということ
で本研究で行なっています
で頭情報量というのはえー二つの確率分布の間で
えー科学大和説を二つありましてその
えー
月の値を足し合わせここで二つの確率分布の間をこちらから見ても
取り出すとし距離尺度行なうようにしたものです
四百五名の形になって
でこのテレビ情報量を
もうえーっとこのこことここで求めたと言いますと
ある決まった品詞あ一つは品詞分類の上での
えーっとその記事
ある決まっ多分今日もえー生地の上でのそのクラスの
単語出現確率の分布
一方もう一つにえー学習データー全体で見た場合のその
別クラスのおー
えー確率の分布というこの二つの分布の間がどれだけ
距離が一八分の一で情報量で計算
つまりえー
晴れて四分類の上で
でこのように全体の平均の分布となっているであるえ二グラムの確率分布と
しまった話題や文献の特徴を受けている
五つの記事の上での確率分布これがどれだけ違うというのを
計算していること
でこれは基本的な特徴量としましてえーまず
えー
若い構文敬語と
三四五
でこの二つ
三種類にえー分離する方法を考えます
で先程の距離をえー一グラムを構成して全ての記事について
えーします申しますと
えー
後
その品詞分類の範囲を後であった場合というのは
えー話題の特徴文献の特徴を持ってないつまりこのような生地の上でも
えー出現パターンは変化しないまずそういうことで
このように
えーこの図で言うとこのばつ印とがえーやめて
まず一つの記事がえー同じような位置にある距離が近い分布をしているということが考えられます
逆にえー話題にこうやっ分布英語であった場合はそれぞれの記事のモデル固有の特徴で出現パターンは変化しますので
距離が遠くなっている
ということが考えられますつまり
えその平均値が
高かったものは若い子が不一語である
提示耳が低いものがハイ四五になってるだろうと考えることができます
えそしてえー同じような方法でえーと話題語と文献を分離することを行ないことができます
で今度は事前に文献のこともあると分かっている二種類のコーパスを考えます例えば
えー書き言葉の新聞記事と
話し言葉のＣＳＪでえーあります
えーそしてえー例として
話し言葉のえ二グラムでの確率分布を
書き言葉の上での一つの記事の確率分布話し言葉の確率という風に
えー
この二つの距離を測ること
考えますと
えー同じ
文献を持っているものはえー文献のクラスであるとですが
多分Ｋ語では
えー
同じような
文献の特徴を持ってますので出現パターンも似ているつまり距離が近くなるだろうと
考えられます
一方
書き言葉と話し言葉というように違った文型を持っていた場合は
でこの距離は遠くなる
と考えられます
でそしてまた英語であった場合はこの同じ二つの距離は掛かったらどうなるかとか
言いますと
こちらは体制が一しない限り単語の出現パターンを変化するだろうと考えられますので
えー
この二つの距離がどちらも多くなると考えられますつまり
同じ種類のコーパスに対する距離と違う種類のコーパスに対する距離で
距離の傾向が異なってくるものが文献をじゃないかと考え
全体まとめますとえーこのような
ことが考えられまして
えー話し言葉の
えっと
書き言葉の生地
書き言葉のＮグラムと話し言葉のＮグラムで
えー四つの確率分布を考えることができますが
でその間である四つの
距離が
これも大きくなるものが話題こう
それも小さくなるものは汎用そして
えー話し言葉の記事をえー話すことも年話し言葉工学部というに違ったもので掛かった場合で傾向が異なるものが分敬語であると
ということでえー
えー
計算のし方としましてはえー
この四つの距離尺度のえーその逆数を取りますつまりえー距離がどれだけ小さい方範囲を御覧さを表わしていると
でそしてえー違う記事で測った
夏の距離の差分の事情は
えーつまりこの図で言うとこの話し言葉の指導話し言葉と書き言葉の処理の差分を計算しまして
でそれが大きければ大きい程文献クラスターが高い
のようにして何を貰った文献コーラス部尺度を定義して
えー実際にえー
各品詞分類について
え実験を行なっていました
でその結果が
こちらになっていましてえーグラフの横軸が三四五月二十二語が文系コーラスたいなって
そしてえー
この赤いで一つ一つがあさせるので自分で九十種類の
えーと二一つに対応してます
具体的にどのような
分類であっがえー現われていたと言います泊まったような差が非常に高かったものが
女性の連帯が
何々のももですね
それから六十四などが非常に高い値を示しました
えーそして文献は差が高かったものが
えー二の一二五乗法性とか後何々の四五にえー
でそれからえーピアノとも高い値を示しました
そして二つ共低い
値となる傾向だったのが名詞のさえー接続や
固有名詞などえー話題性を強く
反映してると思われるものは
でこちらの体も小さくなるという傾向でした
でそこで
えーこの配慮を網羅さ文系コーラスの閾値を
このグラフの
の音を聞いておりませんですま閾値を設定しまして
えーグラフの上で
二十されているもの
はい用語のクラス
左上に出るもの文敬語のクラス左下を若い方のクラスター
このように考えてこい分割の基準を生成いたしました
その結果得られるのがえーこちらの表です
えー最初に示しましたその判断で暫定的に決めたものに非常に近い
えーものが得られています
えー多少変化したものもありますでこのことが斜めになっているものが
その判断の基準からえー分割される
クラス係ったものですが
えー若い方に接頭辞や接尾語が
分類されたこと
でそれから文献をにえーっと音声は全て文系行ったことなどで若干の違いは
えー生まれて
家では以上説明しましたこういう分割形成言語モデルの評価実験の説明付けます
えー
言語モデルの語彙サイズはえー三万語彙のもの生成しましたデーター中データーに用いたのは
で話題性をカバーするコーパスをして毎日新聞の二千えーっと話者性をカバーするものとしてＣＳＪの学術声も聞こえ
会話を使って
でそしてえー
ほい分割系列Ａのえー三つのモデルそれぞれの
ボイス形態素数は御覧のようになってますが三これは
えー情報量基準で
えー
これ羊を決定した場合の
えー
条件となって
そして評価尺度にはテストセットパープレキシティーを用いまして
えーテストセットには
ＣＳＪの模擬講演の中にある現在から過去数年前です雑誌などで扱われた人
というものをえー百五十二個えー用いて
でそしてえートライグラム従来の日えー千モデル
そしてえーこの分割生成の種類えこれは
最初に示した一の判断で基準を生成した場合と
情報量基準で分割基準を決めた場合二種類でえーこの八つのモデルについて
学校行ないました
実験結果です
で左から順にトライグラム従来のＰえーせ
ほい分割平成でえー基準は人の判断で決めたそして情報量基準で決めたもの
このような結果になりまして
えー情報量基準でこういう分割を決めた者が最も
良い結果を示しました
従来のＰＬ性と比べますと四．五六パーセント
パープレキシティーの改善が得られて
で次にえーこの言表を認識実験に適用した場合のえー実験について説明いたします
えー
を言語モデルをＪｕｌｉｕｓに直接使用するといます
えー難しかったのでえーまずは
でベストリスコアリングによる認識実験を行ないました
えー認識エンジンとしてはＪｕｌｉｕｓを用いまして音響モデルにはＣＳＪの底の
確実も結構いてありモデルを使っています
そしてえートライグラム言語モデルを使ってま高い認識を行ないまして
えー五百別そのー
認識結果高校生六させますそれを
でこういう分割基準線の
言語スコアでえー
一五人をするという方法を取っています
体のリスコアリングの際にはえーこの
えー言語モデルの効果が強く現われるように
言語重みと挿入えー内容通常のＪｕｌｉｕｓのデフォルトの間に場合に大きめに設定してあります
認識対象としたのはえー
先程の
パープレキシティーの評価に使ったテストセットから一個の講演を選択して使って
実験結果ですがえーこのようになりました
えーそれぞれ一本もテストセットの
えー物理量がトライグラム
のかいえー認識結果の時点でのワンでその
認識精度そして青がリスコアリングを行なった後の
えー認識精度行って
そして順番にそれぞれのテストセットのてこのテストセットそれぞれの結果となってまして一番いいか平均値です
ま結果としてはえー
六十四．四五パーセントがリスコアリングを行なって六十四．八五パーセント
えー改善したのが〇えー三三パーセントということで
ま殆どえー結果としては変化が得られないというものになってしまいました
えーテストセットごとに見ていきますと
えー要因もので〇．七パーセント程度
改善
せまして逆に悪化してしまうものもの
ものもありましてそちらではえー
で七パーセント
何かすると
まそのような結果になってまして
全体を平均するとあまり変化がなかったという結果です
でなぜこのようなえーあまりであった結果が得られなかったというところなんですが
えーまず
この
五百ですとこのリスコアリングというこうもう頭があまり適切ではなかったのではないかと考えています
財はこの五百別分布の子が五グラム
政府で選ばれてますので
えーその
えー一文よりも
より分ぐらいで見て認識精度の高いものが
二日五百以内なければならない訳ですが
えー実際に
えー
認識精度が仮説はあテストセットを見ますと
この一文より良い文というのが二回ない
という
傾向が見られました
でまたそれ以外にも
この右側のグラフというのは
えー横軸に
えー
テストセットの正解文
を使ってえー海認識に用いたトライグラム言語モデルのパープレキシティーを計算下を通っておりまして
縦軸には各テストセットのリスコアリング前後での認識精度の変化を取ったのですがこのように
元々えーパープレキシティーが高かったものが書かせしまっていて
良かったものについては改善が得られてるという傾向が見られました
でこのことからもえーこの仮に二つの
トライグラムにえーリスコアリングの結果まで
国際されてしまっていたのではないかという
えーことを考えていますその為えー認識の適用方法に直す必要があるのではないかと
現在考えていましてえー
文内のリスコアリングではなくてえー単語グラフの上でリスコアリングを行なうという本やあるいは
気凄いのではなくてえー文脈適応したえ一重性をバイグラムやトライグラムに変換して
知らずに直接与えて
小するといった方法が必要なのではないかと
いう風に考えて
でまとめます
で本研究では話し言葉認識の為にえー話題と文献の特徴それぞれ
えー独立して適用できる言語モデルってものをえー研究しております
方法としてもい分割と言える性言語モデルというものを提案してして
紅葉は大部分て後半用語のす二月に
分
でそうすることでそれぞれ独立な適用が可能になるというものをえー提案しています
またその語彙の分割基準の生成方法に
えー統計的な特徴に基づいてえー文献コーラス範囲を網羅さというものを
数値化して求めましてそれに基づいた基準を作ること行って
必死でその結果えーパープレキシティーの評価では従来のものと比べ四．五六パーセント
パープレキシティーの削減効果がありました
またえー五百ベストリスコアリングで認識に適応した場合
えー〇．〇三三パーセント認識精度を書いておりました
で今後の課題としてはえこの認識への適用方法があって適切であったと考えるのでそれについて検討したいと考えて
以上です
