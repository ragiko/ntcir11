で
さっき
会話の流れをあまり考慮されていないって
言ってたんですけど
系列ラベリングモデルも使って推定しました
でこっから系列ラベリングモデルになるんですけど
代表的な隠れマルコフモデルです
これは状態遷移に単純マルコフ仮定を仮定したマルコフモデルなんですけど
観測系列Ｘこれが発話の系列に当たるんですけど会話に当たるんですけど
で状態系列Ｙ
発話発話権譲渡を意図しているかどうかを求める問題になります
で
本来なら系列全部を見て
最もらしい系列を選んでくるんですけど
逐次推定になるので先の会話っていうのは予測できないので逐次推定することになるので
こういう形で
推定することになります前の推定した結果を利用して逐次推定することになります
でこれだと決定的に
推定していかなきゃいけな
いんですけど
前向き確率を利用したら
直前の状態の不確実性を扱えて
ある程度安定になるのかなということで
この直前両方の推定を
比較してみました
でＨＭＭの特徴としては
系列ラベリングモデルなので前の状態を考慮可能っていうのと
あとは生成モデル
というこれデメリッ
トかなと思うんですけどＰ
Ｐ（Ｘ｜Ｙ）が定義できてないと
できないといけないという生成モデル
の特有の
欠点があります
あとは非独立な素性を
そうですね非独立な素性を扱うことができないというような
デメリットもあります
で学習は単純に
最尤推定で
解けるんですけど
ゼロ頻度問題
学習データに現れない単語が
テストデータに現れると確率ゼロになっちゃうので今回はラプラススムージングをしました
