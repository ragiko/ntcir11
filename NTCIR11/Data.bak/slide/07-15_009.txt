そのための
方法としてＪｅｆｆｅｒｙ情報量に基づいたこういう分割基準の生成ということを
本研究では行っています
ジェフェリー情報量というのは二つの確率分布の間で
Ｋｕｌｂｕｃｋ−ｄｉｖｅｒｇｅｎｃｅを二つ測りましてその
二つの値を足し合わせることで二つの確率分布の間をどちらから見ても
距離が等しい距離尺度となるようにしたものです
式の上ではこのような形になっています
このＪｅｆｆｅｒｙ情報量
をどこにどことどこで求めるかと言いますと
ある決まった品詞ある一つの品詞分類の上での
一つの記事
ある決まった文型を持っている記事の上でのそのクラスの
単語出現確率の分布
一方もう一つに学習データ全体で見た場合のその
クラスの
確率の分布というこの二つの分布の間がどれだけ
距離があるかというのをＪｅｆｆｅｒｙ情報量で計算します
つまり
ある品詞分類の上で
このように全体の平均の分布となっているであろうｕｎｉｇｒａｍの確率分布と
決まった話題や文型の特徴を受けている
一つの記事の上での確率分布これがどれだけ違うかというのを
計算していることになります
