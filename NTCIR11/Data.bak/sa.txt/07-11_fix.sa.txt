豊橋技術科学大学のです


検索のセッションがここまで続いてきましてご発表がいろいろあったんですけれども
この発表はその検索
の研究をサポートするためにですね

音声ドキュメントの
テストコレクションを作成しましたと
いう話です
でこの発表はご覧の先生方の共著での発表と
いうことになります
でまず背景ですけれども今まで話がありましたように
音声認識が非常に高精度化されて実用化になってると
でそれに伴いまして音声データを文書のように扱うことが可能になってきてると
いう状況にあります
でそこで
その時に鍵となる技術として
検索とか要約とかマイニングとかコンテンツ生成とか
質問応答とかいろいろあるわけですけれども
この発表はそのうちの検索に
焦点を当てることにします
で検索の研究を進めるためにはそれを評価するための

なにか

テストコレクションが必要でしょうということです
で
今までにも音声ドキュメントを対象にしたテストコレクションていうのは
ありまして有名なところでは
ＴＲＥＣのＳＤＲですね
これはニュース音声を対象にした
テストコレクションです
でそこから
クロスリンガルこれまったく同じデータを使ってるんですけれどもヨーロッパのＣＲＥＦという評価
団体で同じようなタスクを設定してると
あるいは
ＴＲＥＣのＳＤＲの後に続くものとして
ＴＲＥＣＶＩＤ
というこれはより
マルチメディアデータの方に対象を焦点を
移しているというような
テストコレクションがあります
ただしこれらのテストコレクションは
すべて
英語とか欧米語なわけですね
で日本語を対象としたテストコレクションは今のところ存在しないと
いう状況になっています
でそこで

情報処理学会の
音声言語情報処理研究会で
昨年の四月にワーキンググループを
立ち上げました
音声ドキュメント処理全般に関するワーキンググループです
でその活動の一環として

音声ドキュメント検索
の研究のための基盤整理というものに取り組んでいます
そして
そこの
活動の第一歩としまして今回
音声ドキュメント検索評価のためのテストコレクションを構築しました
いう
その発表をするということです
で今日の発表ですけれども
まずそのテストコレクションを試作しましたという話をしまして
でその作成した
テストコレクションがどのぐらいの
性能を持っているのかと
ベースラインの性能を計りましたという
二つの発表をします
でこのうちですね前半の部分ここについては
去年の
十二月の
音声言語シンポジウムで既に発表した内容と被るところです

簡単にご説明します
でここの後半の部分が今回新しいところで

ここで作ったけれどもその
テストコレクションが一体どのぐらいの性能なのかと
いうところを計りまし
いうところ
の発表を新しくやろうと思います
でまず
前半の部分についてご説明簡単にご説明します
で

テストコレクションを作りたいということですけれども
まず一般のテキストの場合を考えましてテストコレクションとして何が必要かということを考えてみる
そうすると普通のテストコレクション検索用のテストコレクションは
この三つの要素から
構成されてるということが言えます
で一つは
検索対象ですね
何を検索するかと
その検索対象は何か決めると
で二つ目はそれに対する検索
検索クエリです
質問ですね
その質問を決めてやると
そしてで各質問に対して
これを
検索対象検索した時の
正解文書
の
集合というものが必要である
でこの三つがあればその検索の性能が

評価できるということになります
で
テキストの場合にはこういった規模の
テストコレクションが作られているという状況にあります
で今回作ろうとしているのはこれが
音声ドキュメントになるわけですね
で音声ドキュメントになる
時にどこが変わるかと
いうことを考えると
要は
一番ですねここが変わると検索対象が
音声ドキュメントになります
でそのために
必要なものは

まず音声データが必要です
そして

いろいろな評価のためにこれ音声データに対する人手の書き起こしテストというのが
あった方が良いと
いうことが言えます
でさらに
標準的な
音声認識による書き起こしのテキストも必要でしょうと
いうことですねこの三つを
用意しなければならないと
いうことになります
で今までの
テストコレクション
ＴＲＥＣのＳＤＲの
場合にですねこれを見てみますと
この今の一二三
三つについてどういったものが作られてるかというと

ＴＲＥＣは
四回ほどやられてますけれど
四年間やられてますが
規模としては
だんだん大きくなってきてるわけですねで一貫してニュース音声を使っていると
で
最終的には
この程度の規模のものが対象になっていると
いうことですね
で
目指すところは大体これが
目標になるだろうということですね五百五十七時間で
検索対象としてはニュースニュースの
ストーリーっていう単位で呼んでますけど
文章ですね検索の対象の文章数が二万一千ぐらい
そのぐらいの規模のものを
テストコレクションにしています
で検索質問としてはだいたい五十問ぐらい
が用意されていると
でこういうのをここの辺あたりを目標にすれば良いということになります
でただもう一つこういう
規模になりますと書き起こし自体を作ることも結構大変で
書き起こしがあるような対象にしないとなかなか難しい
でＴＲＥＣの場合には
ニュース音声でクローズキャプションが使えますので
これが書き起こしとして使っていると
ただこの品質はあんまり良くないと
いうことで
でそこでこういったものを
目指したいと
いうことで


我々の方でも
日本語に対してそういうものを作っていこうと
でそれそのテストコレクション設計の話に移りますけども
まず

今ぐらいの検索対象の規模として
一番の検索対象を決めなきゃいけない
で日本語でそのぐらいの規模で

書き起こしも
あるものということを考えますと
あんまり自由に使えるものありませんで

と


選択肢が無い状態で

ＣＳＪがこれこれぐらいの規模になると
いうことで今回はＣＳＪを使うと
いうことにしました
でＣＳＪのうちの
学会講演と模擬講演を対象にしました
でこれらは独話かつ自由発話の
音声にある
いうことですねでその規模を見てみますと
学会講演だと
だいたい

このぐらいの時間で模擬講演
だと
このぐらいの時間ですねこれぐらいの規模になる
で両方合わせてみますと
だいたいさっきのＴＲＥＣのＳＤＲの
時間差でいうと
これに匹敵するぐらいの時間は整ってる
いう状況になっています
ただですねえ

ＴＲＥＣの場合と違うのは
検索対象となる
文書ですね

ＴＲＥＣの場合ニュース音声のニュース一つ一つのニュースを文書にしてますけども
学会講演で講演という単位でみると
ここここに大分差があるということですね
二万に対して
ＣＳＪの場合は二千
七百ぐらい
ですね
それそのぐらいの講演しかないということで講演を単位に検索するというタスクは非常に

ちょっと簡単過ぎるということが言えます
でここの部分をどうするかっていうのが
鍵になり
でそこでそれに対しては
二番の検索クエリのところの設計で
対処しましょうということにしました
具体的には
検索クエリを作る時に
講演全体を
講演全体が答えになるような質問を作るのではなくて
講演の一部が答えになるような検索クエリを作成すると
いうことにしました
これによって
検索対象の粒度を
細かくすることで
その検索対象文書数をある程度確保しましょうと
増やしましょうというようなアプローチをとることにしました
で具体的には
質問応答のような答えの特定性の高いような質問を想定しています
でいくつか例を上げましたけども
例えば講演音声の特徴について知りたいとか

個人評価の高い温泉地について知りたい
あるいは
検索性能を評価するにはどのような方法があるか
というような感じですねでこういう質問で
講演全体に対して
答えを求めるんじゃなく
て
講演の一部を探したいとこういう
タスクですねこういうクエリを作って


検索クエリにしましょうと
いうアプローチをとることにしました
で三番目に関して正解文書ですけれども

大規模なテストコレクションの場合普通プーリング
といいまして
いろいろな検索システムが出した答えに対して適合性判定をすると
いうことをやるんですけども今回は
そういうベースラインいろんなシステムが用意できなかったので
人手で頑張って
答えを
探そうということにしました
でその時に
今言いましたように質問は
講演の一部が答えになるようなものを作りましたので
答えとなる部分
は

講演の一部になります
連続した発話区間を対象に適合性を判定するということにしました
でただその時に最小単位は
ＣＳＪで定義される発話をそのまま使いましょう
いうことですねそこが最小単位としてただ長さに関しては今回

不変不定であると
いろんな
いろいろな質問に対して
いろんな長さの答えが考えられる
いうことですね
そういう可変長の答えを許すと
といういうことですね
可変長の長さに対して
適合性判定をしようと
いうことにしました
これはその後の利便性で最初から
区切りを決めてしまうとその後使いにくい
ということが
考えたので適合性判定に関しては可変長にするということにしました
そして
判定に関しては
三段階で判定をしてやる
いうことですね
でまた
その判定をする時に一般の文章と違うのが

講演の一部が答えになりますので
その答えの判定する時にその講演の他の部分に依存するという
問題が
生じるというのがわかった
でその時に
その
検索された部分だけを見て
答えだとわかるということを
正解の条件としました
これについてはちょっと後で例を使って説明します
でまず最初の
区間
可変長のに区間になるという話ですけども例えばこの
検索質問に対して適合性判定をどうするかというと
あるものに対しては
ここの二つ
二つの発話が
適合であるという風に判定をする
である場合には
こっからここまでというふうに
発話の長い区間ですね
さっきと比べて長いところを正解です
いうように判定するというアプローチをとると
でさっき言いました別の部分に依存するというとこに関しては
その根拠を明示しましょうという
ことにしました
だからあるある一部だけを持ってきて
正解だと判定するんですけれども
ここだけを見て
その答えかどうかっていうのを判定できないと駄目である
でそれが他の文章の他の部分に依存する可能性があるわけですね
で例えばこれ情報検索を探し
検索の評価する手法ですけどもここで
適合率とか再現率とか
いわれただけだと
それが本当に検索の評価指標かわからないと
でそれをサポートするような
部分というところも
同時に指定して
適合であると
いうふうに判断し
判定するということにし
で一方
例えば十点平均適合率とかいう言葉だけが出てきたところはこれ見ただけでは

ほんと正解かどうかわからないのでこれは
不適合であるというふうに判定をする
というような

適合性判定をするということにしました
で

そういう指標でテストコレクションを作成しましたと
いうことです
で必要なものは
さっきの一二三のうちの
ＣＳＪを対象にするとここはもうすでにできていますので
音声認識のテキストと検索クエリと
正解文書集合を作れば良いということになる
で今日はちょっと時間の関係で音声認識をどうしたかという話は省略させていただきたいと思います
で検索クエリは
さっき言ったような基準で
十人ですね十人が
だいたい一人当たり十件を作りまして頑張って作りまして百件ぐらいの

質問を作る
という方針で
作りました
そして
作ったクエリーの作者が自ら作成したクエリーに対して

ＣＳＪを検索すると
検索エンジンを用意して

検索してもらいましたそして
適合性を判定して

検索クエリーと
正解文書を作るというような作業を行いました
で実際の判定用のフォーマットですけれどもこういった報告用のフォーマットを用意しまして


適合性正解をみんな作ってもらうということをし
でこの質問に対して
抽出した講演ＩＤと
発話区間
で
判定結果ですね適合か部分適合か不適合かというラベルと
このこれこの発話区間に対して根拠がどこにあるものかという情報ですねそれを指定する
であとコメントを付けるとこういうフォーマットでみなさんに作成してもらう
いうことです
でその結果ですけれども
だいたい

五区間以上


作ってみてほとんど検索結果が出ないという場合もありますので
ある程度の答えが出るもの
を抽出するとあとから抜き出してみると
全体として三十九クエリが集まったということになります
でその統計量は
ご覧の通りですねだいたい

質問あたり十区間見つかるということになります
でちょっと時間も
押してきましたけどこの辺は
いいかな
今回作ってみてわかったのは
学会講演を検索するっていうのは非常に難易度が高いというのがわかりました

学会講演の場合には非常に省略が多いんですね
重要なことが言わない特に音響学会だと十五分で
内容を説明しなきゃいけないので
あんまり重要なことは言わないということがある
でスライドも使うのでテキストだけ見る
見るとよくわからないということがありましたということで検索タスクとしては非常に難しいというのが

作成者の実感として
上げられている
で
あるいは
作成者がちょっと音声の専門家に偏っていましたので
質問が音声に偏りがちであったと
いう特徴がある
で
後半ですけれどもこれに対して今度は検索
性能を評価してしましたということをやってみました
で
評価の方では


非常にベーシックな
検索をやってみてどのぐらいの性能が得られるテストコレクションが得られたのかと
いうことを
調べました
でまたテキストと書き起こしを比べて
どのぐらいの性能差があるのかというようなことも調べてみる
いうことをやってみます
でまずここで調べた検索のタスク設定なんですけども

先ほど言いましたように正解判定は
可変長の区間でやってたんですけれども


それその可変長
というものを単位にするとちょっと難易度が高すぎるので
ここで調べたタスクは
予めですね十五発話とか三十発話で
講演を
自動的に区切っておいてそれを仮想的な文書とみなそうと
でその文書に対する
検索タスクというのを設定してその性能を調べると
いうことをやりました
で具体例としてはこういう
講演に対して
十五発話で自動的に区切るわけですね
でただ
正解判定はこういう
部分的な可変長の区間でなされてますので
これに対して
この区間
が少しでも被ってるところ
それを
今回の正解だとみなす
でここをみつけましょうと
いうタスクを設定して
どのぐらいの性能が得られるか
いうのを調べました
でこのようにタスクを設定すると

その統計量
としては
擬似的な文書が十五発話にした場合は六万文書ぐらい
三十発話だと三十
三万文書ぐらい
得られる
でこれを
さっきのＴＲＥＣＳＤＲと比べてみますと



Ｓ
ＴＲＥＣよりは
多い文書数が得られるとタスクとして適当な
単位になっているというようなことが言えます
でクエリあたりの正解文書数の分布がこのようになっていると
で今回調べた検索手法としては
書き起こし
のテキストだけを用いたテキストベースの一般的な文書検索手法を
実装して使ってみます
で索引としては
形態素を使う場合と文字バイグラムを使う場合
でそれらの組み合わせを試しました
で検索モデルとしてはＴＦＩベースですねそれを長さ文書長で
正規化するという手法を使います
で検索対象テキストとしては
音声
認識した書き起こしのワンベストを使う場合と
テンベストを使う場合と
人手書き起こしをした場合その三つを比較しました
で実際に検索をしてみて千件引いて
これに
この千件についての性能を見たと
いうことをやる
検索尺度としては
この手の
タスクでよく使われている
ミーンアベレージプレシジョンていうやつですね十一点
再現率を十一点レベル用意しておいてそれの
平均をとると
でそれを全
質問で平均化すると
いうような尺度を使います
でその結果ですけどもこれが全質問に対する
検索性能の図なんですけども

このようにデコボコしていまして質問ごとに
かなり性能が違うと
作成者にとっては
だいたい同じぐらいの
答えがみつかるようなもの
設定したんですけども実際に
検索してみるとずいぶん違うと
で一番悪いのと
一番大きいので
ずいぶん違うわけですね
一番悪いのだと
十件中
上位十件中八件も
正解なんですけども
一番一番
これ逆ですね
一番良いのだと十件中
八件
ですねで一番悪いのだと百件中百件ひいても一件
しか
ひけない
でこれたまたまなんですけどもこれ
この二問は同じ
質問作成者が作ったような質問になっている
たまたま

一番
非常に
差が大きいものを作っている
いうことになり
で性能ですけれども

だいたい
十五発話と三十発話で
やっぱり
三十発話のが良いと
で十五発話の場合にはちょっと難し過ぎるかなあという結果になる
で
これ
青と赤と黄色は青が書き起こしの場合で
赤がテンベスト
黄色がワンベストの場合ですね
でこれを見ると


書き起こしが一番良いんですけども
テンベストを使うことでだいたい
半分ぐらい改善ができてると
いうことですねそのような効果が得られた
いうのがわかる
であと
正解判定二通りやったんですけどもこれについての差はほとんど無いというようなことが
わかりました
であと
いろいろ索引の種類も変えてみたんですけども形態素が一番良いと
で文字を混ぜてもあんまり変わらないという結果になりました
でこれできたものを最後
ＴＲＥＣ内のＳＤＲと比べてみましたけども

規模に関してはだいたい同じようなものができていると
いうことが言えます
赤の赤で書いたとこが大きく
違うところで
まず対象文書が
講演とニュースで大分違いますね
であと
今回の結果ですけれども

ＴＲＥＣで報告されている

性能ですねこれに比べるとだいぶ低いと
いう結果になります
これは
ＴＲＥＣの場合は参加システムですねいろんなシステムが参加して
どのぐらいの性能が出てるかいろんな工夫がされています
特に大きいのはパラレルテキストですね同時期の同じ時期の
テキストを使って
質問拡張をしたりとかそういうテクニックが使われている
いうことで非常に性能が高い
ですので今回はこのそれの半分くらいですけどもまだまだ
検索の工夫の余地があって
いろいろ工夫をされると

もっと良い
検索結果が得られるんじゃないか
いうふうに考え


太めは以上ですね検索手法はまだまだ改善の余地がある
いうことがある
でこのテストコレクションは
近日中に第一版を公開したい
考えている
以上です
