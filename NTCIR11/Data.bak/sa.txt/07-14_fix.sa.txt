福岡大学のです
スペクトラルクラスタリングに基づくニューストピック適応言語モデルの構築
いうタイトルで
発表させていただきます



まずですね
タイトルにもありましたスペクトラルクラスタリングという

ものについて簡単に説明したいと思います後ほど
原理等についてはもう一度説明しますけれども
言葉の説明ということで最初に
ご説明します

スペクトラルクラスタリングっていうのはこのスペクトラルですけれども
いわゆる
スペクトラルスペクトル分解固有値分解
という意味でのスペクトル
ということですで
実際には対象データ間の類似度行列に対して固有ベクトルを用いて
クラスタリングをするという手法なんですが
元々はですねグラフ理論の最少カット問題というもので
適応されたものでして
かなり
取り扱いが容易であると簡潔な

簡潔性
ということから
近年こういった
すね文書分類であるとか画像認識ウェブマイニングＤＮＡ解析と色々な分野で用いられています
で関連手法としましては

非線形の前処理をした主成分分析であるとか
もしくは同じくカーネルＫミーンズ法
といったものと
ほぼ等価なものであるということが報告されています
また
LatentSemantic
インデキシング
にも関連性があるということで近年注目を浴びているものです
で
原理につきましては後ほど
また詳しく
ご説明します
でまず研究背景ですけれども
昨日とですね今日の発表と

大体被るというか重複するかと思うんですけども
我々の研究室で行っているのがですねこのニュース映像検索システムと
いうものを作ろうということで
現在開発してるところです
で
人手で行うのは大変索引を付けたいのだけれども人手で
それを行うのは困難であるところから
音声認識を利用して自動的に付与するということで多くの研究機関等で
行われていると思うんですけれども

この時に

ニュース映像を対象としているということから
そのニュース記事と全く同じものがウェブ上にもあるともしくは類似のものがウェブ上にもあるということで
ニューストピックに類似した記事をウェブから収集して
トピックに特化した言語モデルを構築するということでこの数年行ってきました
で
しかし実際にですねあの関連研究としてこういうウェブ上のテキストを利用するというもの
研究が数多く行われていまして先ほどの研究もありましたし昨日の
ご発表でもあったと思いますけれども

このようなものが関連研究としてあります
で実は昨年度のご発表ですね
音声言語シンポジウムだと思いますけれどもあ一昨年かな
同様のですねウェブ上のニュース記事を対象としてウェブから検索してきたものを用いるという
発表が既にあるんですけれども
基本的には同じようなアプローチ
いうことで
で

この後のセッションでもです
確か

ご発表があると思うんですけれども
ウェブ上のテキストを利用する場合には
大きく二つのパターンが
あるということで予めこういったテキスト集めて欲しいというふうに指定する場合と

認識結果から直接集めてきたいという場合とがあると
いうことで
我々のアプローチはこのこちらの方と同じと
いうことです
で簡単に処理の流れを説明しますけれども
最初にニュース音声を汎用言語モデルで認識します
で
認識結果から索引語候補を抽出して索引語を選び出し
ウェブ上から記事を
関連した記事類似記事を検索収集してきて
それを用いて言語モデルを更新するというもの
で
この処理が音声認識
最初の音声認識の時点では
誤認識等含まれているんですけどもうまく類似記事を集めてくることができれば

高精度の認識ができると誤認識が少なくなるということで
それをまた繰り返し何回もすることで
精度が上がるんではないかということでこの
これについては昨年のですね情報化
フォーラムで
発表したんですけれども
こういうやり方を
取っています
で
結論としましては
正直に言うと
うまく行く時はうまく行く
当たり前の結果ですねでうまく行くというのはつまり
ニュース記事がうまく集められて言語モデルでうまく
作れれば
どんどん更新
の度に精度が上がるんだけれども
もしここでウェブ記事ウェブ
ウェブ上から収集してきた記事がノイズがあったり違うトピックの記事だったりすると悪くなるという
当然の結果になるということ
で
問題点としてですね今お話しましたけれども
そもそもウェブ文書が適切なものがちゃんと集められているのかと
いうことそれから

適切でかつ充分な量の文章を収集できているかと
いう二つの問題が
問題と
点として考えられます
で

先ほどのお話しましたけれども
誤認識があると違うトピックを拾ってしまう可能性がありますし
そもそも
そのニューストピックそのニュースの内容ですね
例えばあの長期間にわたってよくニュースで報道されているようなものだと非常にたくさんの記事が存在する訳ですし

充分な量集めてくることができるんですが
一時的なニュースであったりもしくはローカルな記事だったりするニュースだったりすると

そもそも類似した記事自体が見つからないという問題が
ありました
で本研究では
この問題点を解決するために

ウェブ上から集めてきた

文書の中から
適切な文書だけを選択してそれを用いて
言語モデルを作ろうと
いうことです
でそれから二点目としては

昨年度の発表では
ニューストピックを予め切り出しておいて
全体で検索全体の認識結果から検索をするということを行ったんですが実際にはそのニュースの中では
例えば街頭インタビューが途中に挟まれていたりそれから
記者会見であったりとか
もしくは背景の雑音があったりとかで
発話毎に
かなり認識精度が異なるということが
ありますので
発話単位で処理をすると
いうことを考えました
で実際にはそのアナウンサーの音声は非常に明瞭な
発音になりますので

きれいに喋ってるとこは
非常に良い
途中で挟まれたそのインタビューであるとか
記者会見のようなものだと全く認識誤認識が多くなってしまうという
そういう問題があります
で
それからですね
これまで私達の提案した手法ではニュース記事であるということから
ニュースサイトを限定してですねニュースサイトから記事を集めてくるということを行っていたんですけれども


その場合
ローカルな記事であったり一時的な記事であったりすると充分な量集めることができないということで今回はニュースサイト以外を対象にして検索を行っています

で
ここで基本的な考え方なんですが
繰り返しになりますけれども
明瞭な発音で発声されたアナウンサーの音声
そういう
発話区間であれば
認識精度もかなり高い
でそういう認識結果から抜き出した索引語を用いた場合には

関連したもしくは類似した
多量の文書を収集することができると
こういう前提でやっていたんですけども
実際には
誤認識が多く含まれていることから
実際その誤認識が含まれているような
認識結果から索引語を抽出して検索しますと
微妙に異なるトピックであるとかもしくは全く関係ないトピックであるとかばらばらとですね色々な文書が収集されてしまうんじゃないかと
いうことです
で

こういう想定から
集めてきた記事をクラスタリングすることで
その
分布状況分布のを調べて

最も多く集まっている
部分と
いうものを用いたらどうだろうかというのが基本的な考え方になります

でそのクラスタリングに最初にご説明しましたスペクトルスペクトラルクラスタリングという利用するのですけれども

集めてきた文書に対して各文書間の類似度をまず最初に計算します
で
この時に

基本的にはその
単語をベクトル空間上での類似度計算するんですけれども
非常に
スパースな
でかつ高次元な空間になってしまうということから


そのまま
通常のクラスタリングを行いますと計算量の問題であったりとか精度の問題等で
若干

クラスタリングの
望ましいクラスタリングが得られないということで
今回このスペクトルクラスタリングを用いることにしました
で
特徴としましてこういった単語ベクトル空間での特徴空間ですね低次元化を行えることと
それから
特徴空間内での距離関係保存というふうに書いてますけども
近いものは
似ているものは似ている

低次元化特徴空間を低次元化した時に

文書間文書同士で似ているものは近くにと似ていないものは遠くにという
形で距離関係保存したままでこの次元の圧縮ができますので
非常にこういった文書
のクラスタリングとに向いていると
いうことで今回このスペクトラルクラスタリングに用いた

で簡単に原理を説明しますと最初にお話ししましたように元々はグラフの最少カット問題として
提案されていたものなんですけれども
例えばこういったグラフが
存在していてどこで
分割すると最も
最小のコストになるかということを考える
いう問題です
で
この時に
各ノードと

それが
繋がってるかどうかということで
ij間にWijといった重みを設定しておきます
で
更に
二つのクラス二つのグラフに分割するという場合には
考えますと
もしも
Aの方のグラフでグラフに含まれているならばqiを一とし
Bの方逆の方であれば-一とするというように
qiを置くということをしますと
この最少カット問題はこういった式を最小化する問題として定式化することができます
これは簡単にですねノードとノードと
差を取ってその間が繋がってるかどうかというので
全部足して足した時に最小になるようにということなんですが
これを展開していきますといわゆるこういった二次形式という形に置き換えることができまして

でさらにこれを

このままではですねこういった整数計画問題になりますので
そのまま解くことが難しいということで
連続値に緩和することによって

この式から
こういった方程式を得ることができます
で
結局ですねこの形に
変換し変形した時点で

いわゆる固有値問題というふうに置き換えることができますので
これの固有ベクトルが解となると
いうことになります
で先ほどの固有値問題の解く訳ですけれども

この場合最小固有値は〇となるという性質があることが分かっていますで〇というのは要するに固有ベクトルが全部一もしくは−一というのは直流成分になるわけなんですが
結局のところグラフ全体を表して分割じゃなくてグラフ全体を表しているようなものということで
一般的にはこの二番目以降の固有値に対応した固有ベクトルを利用すること
なります
で

さらに先ほどはグラフの分割の問題だったんですけれども一般のクラスタリングに応用する場合には
グラフの接続関係を表したWijというものを
ノード間の類似と
文書クラスタリングのものは文書間の類似度ですけどもに置き換えることで
同様の問題を解く
解くことでクラスタリングを行う事ができます
で本研究では
簡単簡単めに第二固有ベクトルですね二番目の固有値に対応した固有ベクトルだけを用いて近似的なクラスタリングを行うことにしました
でさらに

元々の
元々の
このクラスグラフの最少カット問題というのが
要するにこの
一番
二つのグラフの間で一番接続が少ない所と小さい所というのを求める問題になっていますので

クラスタリングを行う時の境界候補としては累積類似度の総和が小さい点を境界候補として
クラスタリングを行うということを行いましたこの後は
例で
ご説明しましょう
で実際に
スペクトラルクラスタリングやるとどういうことができるかということなんですが
こちらがですね縦横
が
文書集合なんですけれども

こういった文書が収集されたとして

各点ですねこの各点は黒い方が似ている文書だということを表しています
で対角線は当然自分同士ですよね真っ黒なんですけれども
こういったものを先ほどの式でスペクトルクラスタリングを行います
まして

得られた第二固有ベクトルの要素値をソーティングしますとこのようにですね
非常に
ギャップが存在するようなこういった結果が
得られる
というわけです
でこのソーティングした結果に基づいて元々のこの類似度行列を並べ替えてやりますとこのように
きれいにですね
似ているものは似ている所にと対角線上に集まるような形でクラスタリングされる
いうことで
で

結局この場合はですねもともと三つのトピックを混合してランダムに並べてあるんですけれども

クラスタリングの境界点としてはこういったとこですねこの集まった集合のこことここという形で求めたいので

各点における

類似度の総和と
いうものを求めた時に
小さくなるところというものが
境界点ということになります


で以上のような方法を用いまして
で
実際にウェブから収集した記事に対して

クラスタリングを行って
実験を行いました
で
認識エンジンとしては

Ｊｕｌｉｕｓですねを用いてますで汎用言語モデルには今回は
新聞記事から学習した語彙数二万の単語バイグラム
モデルを用いています
で

先ほどの
処理の流れでですね
ニューストピックへの適応言語モデル言語モデルの適応を行うという風にお話ししていたんですけれども
今回の
実験では
収集記事から作成したバイグラムモデルに対して汎用言語モデルの語彙を追加すると
いう形で
ニューストピック用言語モデル
特化した言語モデルを
作ってそれで実験を行いました
で音響モデルがＪｕｌｉｕｓ付属の音響モデルですけれども
評価尺度として
音声認識単語正解率と
索引語の候補となる名詞のみを対象とした再現率適合率
ので評価を行いました

で実験に用いたニューストピックなんですが
今回ですね用いたのはこの四つのトピック
なります
で

この四つのトピックを用いたとこれ
割と最近のものを用いるという用いたというのがあるんですけれども
一つこの理由の一つとしてはこれまでにですね
いくつかのトピックで
既に実験を行ってきていて
うまく行く時はうまく行くということが分かっている
ということで

比較的
一時的かつローカルな
あまり話題になってないような記事と
いうものでこの四つの記事をニュースを選んでいます
で平均
各トピックの平均放送時間は一分ちょいちょっとということ
で
この中の平均発話文数は
大体六文程度ということで
各発話
内には三十八単語数
平均三十八単語
含まれているんですが
中には小さなものや大きなものもあるということ
で索引語候補数
発話内の平均索引候補数としては
十一と
いうことになります
でご説明するの忘れてましたけれども

今回の実験では
トピック毎に
あらかじめトピックが切り分けられているものとして各トピック毎に実験を行っています
それからこの発話の切り出しに関しては

発話の切り出しに関してはですね
Ｊｕｌｉｕｓで用いられてるのと同様の処理で
パワーとですね零クロスレートで

切って
自動的に切り分けてる
ことをやってる
でそれから索引語候補に関しては今回対象としたのは
一般名詞固有名詞サ変名詞のみで
まず汎用言語モデルに
用いて各発話を認識した結果を
ご覧頂きたいと思います
トピック一から四までありましてそれぞれ大体六発話程度入ってるんですけれども
ご覧ですね
緑が
単語正解率で真ん中の赤いのが
適合率で黄色いのが再現率になります
ご覧頂いて分かる
お分かり頂けると思いますが
例えばこれですね
非常に
発話によっては非常に精度が高いものがあると
いうことです
で
ただ逆にですねこのように
中に含まれている発話によっては
ほとんど
認識されてないものと
いうものがあるということ
とにかくここはですね背景の雑音でかなり群衆がわあわあと言ってるようなところ
なんですけれども
認識
正解率は
四十五十％程度ですけれども
その中の名詞はほとんど間違えている
最後のこの六
トピックでの六番目のものに関しては
これはですねアナウンサーの
発話が終わった後で

記者会見があってかなりよどみ
言いよどみながら話しているようなもの
いうことで

で実際の例をご覧頂きたいと思いますけれども正解文の中から抜き出した名詞がこういうものだとなんですけれども認識結果としては
例えば発達とか影響ピーク北日本風というようなものは当たってるんですけれどもやはり誤認識で
全く関係なさそうなものが出ていると
このまま
これを用いて検索してしまうと当然関係ないようなトピックも集まってくるだろうということ
で先ほどの
スペクトラルクラスタリングを用いたクラスタリング結果なんですけれども
実際にはこういうような
結果になりました
上の図がですねソーティングというか並び替えを行った後の類似度行列になります
で
各点に対して累積類似度を求めて
クラスター
の際の
境界候補を出しているんですが
要するにこのひげが下の方に出ているものというのがクラスター境界の候補になります
で

でクラスタリングを行った結果トピック一に対しては

過半数は大体同じようなところに集まっていて最大のクラスタを形成している
で
三四というように

若干違うトピックを集めてしまったようなものというものは
違う離れた所にあるということ
でトピック三四に関してもほぼ同様なんですけれどもこのように
認識率が低かったものは外にはじき出されるようになっている

こちらは飛ばしますけども最終的に
認識結果をみますと
このような結果になりましたトピック一二三四とありまして
汎用言語モデルと
それからニューストピック今回提案手法
の結果が赤です
で
収集した記事全部で集めたものが黄色となっています


すね
赤い所という今回提案した手法でかなり改善が
じゃ
ほぼ同じか改善が見られている
いうことが分かり
以上まとめですけれども
今回は

ウェブ上の類似文書用いてニューストピック用言語モデル構築するという際に
スペクトラルクラスタリングを用いて
より類似した文書集合の上利用するという方法を提案しました
クラスタリング結果から
信頼性の低い音声認識か判断しそこは使わないというような
方法を取ります
今後の課題としてはこういうものが
以上で発表を終わります
